{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transformer Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoder-Decoder Architecture**\n",
    "\n",
    "It is used for **sequence-to-sequence** tasks like machine translation.\n",
    "\n",
    "This is the architecture proposed by the original transformer paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. in 2017.\n",
    "\n",
    "<img src=\"assets/encoder_decoder.png\" alt=\"Encoder-Decoder Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decoder-Only Architecture**\n",
    "\n",
    "It is used for **sequence generation** tasks like language modeling.\n",
    "\n",
    "This simpler architecture will be implemented in this notebook from scratch.\n",
    "\n",
    "<img src=\"assets/decoder_only.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scaled Dot-Product Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $Q \\in \\mathbb{R}^{m \\times d_k}$, $K \\in \\mathbb{R}^{n \\times d_k}$ y $V \\in \\mathbb{R}^{n \\times d_v}$ be the query, key and value matrices, respectively. The scaled dot-product attention is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\in \\mathbb{R}^{m \\times d_v}\n",
    "$$\n",
    "\n",
    "<img src=\"assets/self_attention.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddingSize: int, headSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.query = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.value = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(blockSize, blockSize)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        weights = q @ k.transpose(-2, -1) / (C ** 0.5)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        output = weights @ v\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $h$ be the number of heads. The multi-head attention is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\in \\mathbb{R}^{m \\times d_v}\n",
    "$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$ and $W_i^Q \\in \\mathbb{R}^{d_k \\times d_k}$, $W_i^K \\in \\mathbb{R}^{d_k \\times d_k}$, $W_i^V \\in \\mathbb{R}^{d_v \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_v}$ are the learnable parameters.\n",
    "\n",
    "<img src=\"assets/multi_head_attention.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, nHeads: int, embeddingSize: int, headSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([ScaledDotProductAttention(embeddingSize, headSize, blockSize, dropout) for _ in range(nHeads)])\n",
    "        self.projection = nn.Linear(embeddingSize, embeddingSize)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        output = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        output = self.projection(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Position-wise Feed-Forward Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position-wise feed-forward networks are defined as:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "where $W_1 \\in \\mathbb{R}^{d_{ff} \\times d_{model}}$, $b_1 \\in \\mathbb{R}^{d_{ff}}$, $W_2 \\in \\mathbb{R}^{d_{model} \\times d_{ff}}$ and $b_2 \\in \\mathbb{R}^{d_{model}}$ are the learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddingSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embeddingSize, 4 * embeddingSize),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embeddingSize, embeddingSize),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Layer Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layer normalization is defined as:\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sigma} + \\beta\n",
    "$$\n",
    "\n",
    "where $\\gamma \\in \\mathbb{R}^{d_{model}}$ and $\\beta \\in \\mathbb{R}^{d_{model}}$ are the learnable parameters, and $\\mu$ and $\\sigma$ are the mean and standard deviation of $x$, respectively.\n",
    "\n",
    "In the original paper, the layer normalization is applied after the residual connections, whereas in more modern implementations, it is applied before in what is known as **pre-norm formulation**.\n",
    "\n",
    "```python\n",
    "# Original paper\n",
    "x = LayerNorm(x + MultiHeadAttention(x))\n",
    "x = LayerNorm(x + FeedForward(x))\n",
    "\n",
    "# Pre-norm formulation\n",
    "x = x + MultiHeadAttention(LayerNorm(x))\n",
    "x = x + FeedForward(LayerNorm(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "\n",
    "    def __init__(self, nHeads: int, embeddingSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        headSize = embeddingSize // nHeads\n",
    "        self.multiHeadAttention = MultiHeadAttention(nHeads, embeddingSize, headSize, blockSize, dropout)\n",
    "        self.feedForward = FeedForward(embeddingSize, dropout)\n",
    "        self.layerNorm1 = nn.LayerNorm(embeddingSize)\n",
    "        self.layerNorm2 = nn.LayerNorm(embeddingSize)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.multiHeadAttention(self.layerNorm1(x))\n",
    "        x = x + self.feedForward(self.layerNorm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__ (self, vocabularySize: int, nLayers: int, nHeads: int, embeddingSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenEmbeddingTable = nn.Embedding(vocabularySize, embeddingSize)\n",
    "        self.positionEmbeddingTable = nn.Embedding(blockSize, embeddingSize)\n",
    "        self.layers = nn.Sequential(*[Layer(nHeads, embeddingSize, blockSize, dropout) for _ in range(nLayers)])\n",
    "        self.layerNorm = nn.LayerNorm(embeddingSize)\n",
    "        self.linearModelHead = nn.Linear(embeddingSize, vocabularySize)\n",
    "        self.blockSize = blockSize\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor, targets: torch.Tensor = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        B, T = idx.shape\n",
    "        tokenEmbeddings = self.tokenEmbeddingTable(idx)\n",
    "        positionEmbeddings = self.positionEmbeddingTable(torch.arange(T, device=device))\n",
    "        x = tokenEmbeddings + positionEmbeddings\n",
    "        x = self.layers(x)\n",
    "        x = self.layerNorm(x)\n",
    "        logits = self.linearModelHead(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.Tensor, nTokens: int) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(nTokens):\n",
    "                logits, loss = self(idx[:, -self.blockSize:])\n",
    "                logits = logits[:, -1, :]\n",
    "                probabilities = F.softmax(logits, dim=-1)\n",
    "                nextIdx = torch.multinomial(probabilities, num_samples=1)\n",
    "                idx = torch.cat([idx, nextIdx], dim=1)\n",
    "        self.train()\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 187096\n",
      "I\n",
      "\n",
      "Aquí me pongo a cantar\n",
      "al compás de la vigüela,\n",
      "que el hombre que lo desvela\n",
      "una pena estrordinaria,\n",
      "como la ave solitaria\n",
      "con el cantar se consuela.\n",
      "\n",
      "Pido a los santos del cielo\n",
      "que ayuden mi pensamiento:\n",
      "les pido en este momento\n",
      "que voy a cantar\n"
     ]
    }
   ],
   "source": [
    "dataDir = \"data\"\n",
    "file = os.path.join(dataDir, \"martin_fierro.txt\")\n",
    "with open(file, \"r\") as f:\n",
    "    text = f.read()\n",
    "print(f\"Text length: {len(text)}\")\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 72\n",
      "\n",
      " !\"'(),-.:;?ABCDEFGHIJLMNOPQRSTUVXYZabcdefghijklmnopqrstuvxyz¡¿Ñáéíñóúü\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sorted(set(text))\n",
    "vocabularySize = len(vocabulary)\n",
    "print(f\"Vocabulary size: {vocabularySize}\")\n",
    "print(\"\".join(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 51, 55, 1, 44, 41, 54, 49, 37, 50, 51, 55, 1, 55, 41, 37, 50, 1, 57, 50, 45, 40, 51, 55]\n",
      "Los hermanos sean unidos\n"
     ]
    }
   ],
   "source": [
    "stringToToken = {ch: i for i, ch in enumerate(vocabulary)}\n",
    "tokenToString = {i: ch for i, ch in enumerate(vocabulary)}\n",
    "encode = lambda string: [stringToToken[char] for char in string]\n",
    "decode = lambda tokens: \"\".join([tokenToString[token] for token in tokens])\n",
    "print(encode(\"Los hermanos sean unidos\"))\n",
    "print(decode(encode(\"Los hermanos sean unidos\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.79M parameters\n"
     ]
    }
   ],
   "source": [
    "blockSize = 256\n",
    "embeddingSize = 384\n",
    "nHeads = 6\n",
    "nLayers = 6\n",
    "dropout = 0.2\n",
    "\n",
    "model = GPTLanguageModel(vocabularySize, nLayers, nHeads, embeddingSize, blockSize, dropout).to(device)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsDir = \"models\"\n",
    "model.load_state_dict(torch.load(os.path.join(modelsDir, \"martin_fierro.pt\"), weights_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los hermanos sean unidos\n",
      "les mal pederme perseguía.\n",
      "\n",
      "Al consarme al esprendao,\n",
      "siempre dónde hacer el día\n",
      "aquiera el asunto en cuchería\n",
      "en el cielo el chillo el alcalo\n",
      "como un salvajir.\n",
      "\n",
      "¡Qué rito, él saliar la herma\n",
      "el juego, allí mis curisia,\n",
      "y esas tal verían esancas,\n",
      "con cada eBaba aprendido,\n",
      "que a su mío la puere\n",
      "tanto mi cosarnarme.\n",
      "\n",
      "\n",
      "Despace recobarle el pulpero\n",
      "me añarondo por eserancia,\n",
      "y a ver a salienta de froz\n",
      "con pa las yuros al verano\n",
      "de esa jeperanza el jueza.\n",
      "\n",
      "Dentrando otro cuando,\n",
      "nunca el falta la tierra\n",
      "habían y, algdita y en apuesanta;\n",
      "con una ada lo gana\n",
      "allíamos nada entece.\n",
      "\n",
      "Me hizo províamos al repanta\n",
      "he gente otro rigerar\n",
      "y la arrimar la prestencianda\n",
      "y escuán escapo ade todas,\n",
      "que aleguna de he ablao\n",
      "a dejar la albe fina.\n",
      "\n",
      "Yo tal vecía hubera\n",
      "aguantao ponga el bruto\n",
      "aunque me entrarra en allí;\n",
      "con les trapato él grito\n",
      "y le da acar el esperante\n",
      "y al fin, el cadarle,a una a pera,\n",
      "apués decir esa asombrao,\n",
      "\n",
      "siempre hecho al Cruz.....\n",
      "Les debía al otro\n",
      "allá entre les copaviana.\n",
      "\n",
      "Aunque seata ocasión\n",
      "de sus lado esperanza;\n",
      "yo sé lo lengua,\n",
      "aunque se agua estaba el sol,\n",
      "yo si de yolo espondicana\n",
      "empezó el la maneja ensaca.\n",
      "\n",
      "Y ya me alevanté al punta\n",
      "en con la fontementa un rato,\n",
      "hiciendo en lade boca\n",
      "sin empre comendar el barraca,\n",
      "siempre el aquella trapa a el soloma\n",
      "tiene un inmenso se alvarme.\n",
      "\n",
      "Y seguiendo el indio a un del desalito\n",
      "como el pelicho astanta!\n",
      "¿Fien que a encuentar\n",
      "sufar alta el atropellar;\n",
      "todo el había conocía\n",
      "la el phan de sentidapiarlo,\n",
      "y en indericada de flor\n",
      "como una carre a él asú.\n",
      "\n",
      "Y con le guste al lao es bara\",\n",
      "cai cuando empezó a solar;\n",
      "ya no éolvi se los atenar\n",
      "y el sentido y a un perdo esar\n",
      "tóndo delije los neerdan.\n",
      "\n",
      "Yo es yo, en conociendo\n",
      "ponzco andante cuando que tiene;\n",
      "para esperar de nada,\n",
      "donde aquel mundo que yo falta,\n",
      "y el diablo del rueno\n",
      "\n",
      "lo tal apunte al verseguita\n",
      "al cruerto del otro.\n",
      "\n",
      "Ansí me daba una un perda\n",
      "hacía algún autoque después;\n",
      "mas darse alguno\n",
      "andaba con adeabantando.\n",
      "\n",
      "Tenía el gaucho vivo\n",
      "por me dejabació.\n",
      "y no le era barravo\n",
      "éllo al estaba el pampero,\n",
      "y a uno pedía ni dejar\n",
      "ha plo dormiar lo astaba\n",
      "\n",
      "pero sin andar su escuasión.\n",
      "\n",
      "No tormento el pasarán,\n",
      "y en cuanto no motiga;\n",
      "hace el suelo del campo se algué,\n",
      "con los gesos tiempo orro.\n",
      "\n",
      "\n",
      "Me largué el carazón Saliz,\n",
      "un corazón de espeluz,\n",
      "y aunque estaba emprezao el prometido,\n",
      "y a en almarla en edeste muy botía.\n",
      "\n",
      "Allí vergante conocices\n",
      "y estas supleces bener\n",
      "cimpre voy al vecino,\n",
      "unque venía a aorridarme,\n",
      "venga que a trainar,\n",
      "pues naides ha achir la mataria\n",
      "lo air he sin cusagria.\n",
      "\n",
      "\n",
      "Cuando este cuando imita\n",
      "me aprue toda el potro\n",
      "ellía el pasade un velate\n",
      "y, al chaballo en el cuerelo\n",
      "sinóanperlo como una el cante;\n",
      "y fí, y, a a les acordea\n",
      "a complarao a trabajar.\n",
      "\n",
      "\n",
      "Decían acaba a algorraba,\n",
      "se una saleta y puro andanza,\n",
      "le llevaba lo un cercera\n",
      "y, al fin de las orgas\n",
      "cuando allí espianzao.\n",
      "\n",
      "Decían ella cero el pastella\n",
      "dende chiquito es truto;\n",
      "me dijo: \"el dormilito\n",
      "\"un carda más años.\"\n",
      "\n",
      "\"Le )pués yo los miras,\n",
      "\"áste lo pero la ha hermala,\n",
      "\"mas mundo y aunque nada esperanza,\n",
      "\"te, dejó en su cantida\n",
      "\"como el curó el tes.\n",
      "\n",
      "\"Cuuato que hambre lebo,\n",
      "\"siempo mi ortestario;\n",
      "\"ya nos se ha el diriteso\n",
      "\"lo saltanto y todos,\n",
      "\"le eché....... pero.... ¡¡ho! Chijo!\n",
      "Cra!.... denganta \")as\"!\n",
      "\"agua on lo hacea agana,\n",
      "\n",
      "pa sin esta hume sertao\n",
      "y fue con la pulta,\n",
      "\n",
      "pero no le nastació\n",
      "me atan grato una bia\n",
      "y me hacía no lo apobrecita.\n",
      "\n",
      "\n",
      "Pero donde donperme, con plocta,\n",
      "era metió si yo me dije:\n",
      "en el todo en no adoy\n",
      "como si gana andanían;\n",
      "andunta apuna hecía de manear\n",
      "ni guarro de gritos.\n",
      "\n",
      "A más proto el esa yun tor\n",
      "y alguna aproa en su corro;\n",
      "tal vez naides seatura;\n",
      "jamás he quedado la prima,\n",
      "lo hijoso el sustenté\n",
      "dende chiquitos carrerlos,\n",
      "y los enguan arriguente\n",
      ", si algún frestarones\n",
      "que andaba en alnenciao.\n",
      "\n",
      "\n",
      "Con el juyto los inorados,\n",
      "aunque el hombre algún infierno,\n",
      "sin saber alegracia al emblao\n",
      "la escapa el cierto al lucha\n",
      "y echéné mandaome de habla.\n",
      "¡Ah pade la borra\n",
      "de una sacapaza de mallar.\n",
      "\n",
      "\n",
      "¡Ai un perde\n",
      "Eque el los mesmo que Delicos\n",
      "me doy al maldito el motuer!\n",
      "Había de su nacido\n",
      "no salía a uno casarripeta\n",
      "en cuando el cuero se atreva,\n",
      "y ya me mandaba lo his!\n",
      "\n",
      "pero él el diablo lo asigura\n",
      "de algún penso damao él,\n",
      "y era traca hacerlo arreta\n",
      "cantando no haga el chinao.\n",
      "\n",
      "Tenía el hombre inde canto\n",
      "hasta que habían afeliguao\n",
      "y ladrárse y dí ampundo,\n",
      "metiden aquel cerveción\n",
      "pata encerraban sus días\n",
      "delató una disgracias.\n",
      "\n",
      "\n",
      "Y aquéllos a pretandos\n",
      "los mirones son consuelos,\n",
      "estaban el malo uno\n",
      "y quedó enque no esto penarse,\n",
      "y si camientas aquel trato,\n",
      "me acabó a hombre Hretdar\n",
      "de dónde aquel que dentro.\n",
      "\n",
      "\n",
      "Me sencéó a el ñudo, que andaba,\n",
      "es guarraba a el suelo;\n",
      "con más mulitaba\n",
      "y lo mal Del verao al punto,\n",
      "\n",
      "con lo habla de buste pinao\n",
      "y fíto en aquélla henchao.\n",
      "Pero desgraciao el patro\n",
      "al mortar con las puntas;\n",
      "\"o soné, dejamé aquel gato!\n",
      "¡Aijuna!... y ya que seata guascaca\n",
      "\"Nunca até paco acabó,\n",
      "sile despacarar me reistonaba:\n",
      "siempre me decía conocí\n",
      "que no lo se campéa anterar.\n",
      "\n",
      "Cuanto pércia al mi bronacia\n",
      "ni falta un pillo lenga el bresol\n",
      "el cimpo el dijo, y el gana\n",
      "la eché el no le de adona\n",
      "sina....... dondice a anijuna.\n",
      "\n",
      "Se hacía fronte a era.\n",
      "\n",
      "\n",
      "Ningún dejao allí\n",
      "como fila y pena;\n",
      "haciendo a la socar,\n",
      "mise ha de ser padecer,\n",
      "tenía a venir amos gente,\n",
      "era todo me amenatar\n",
      "fuerma donde aquel indio perdo.\n",
      "\n",
      "Siempre es el juez alguna\n",
      "aquello lo al pobre un baje,\n",
      "hicie que allí provejas\n",
      "lo llevarme a saliencia,\n",
      "y dejando con la herpeliga,\n",
      "el pierdo en aquella puerda\n",
      "había en el alma alro.\n",
      "\n",
      "En la pura que duré\n",
      "lo hice el pecharlo dito,\n",
      "el juzgao el indije:\n",
      "Cuando haregantaba la anca\n",
      "y hice el colore y esperaz,\n",
      "y a punta vergante,\n",
      "y ansí le puse escuida\n",
      "es te a querezar el jefes.\n",
      "\n",
      "Todo un junto pagarle\n",
      "la ajunta en sacaro;\n",
      "y hubiera por el sumpo ado\n",
      "hacerlos que uno aiguandar\n",
      "bilo, mas que es mi plersarme,\n",
      "quedan no dejaba a oryellas\n",
      "se llevaró con gritos.\n",
      "\n",
      "\n",
      "Y con la cruzar el desgraciao\n",
      "pero como momenda mi cuerdo;\n",
      "mas yo, me llomaren mudo\n",
      "aunque sue sabe más añol;\n",
      "porque su no congenta pertido\n",
      "por estar de mi venenga.\n",
      "\n",
      "Munto pa echo es mi penes,\n",
      "me los dijo el decir,\n",
      "y el prujimo que andarme,\n",
      "cantaba sin mete,\n",
      "páro por los que encae,\n",
      "ansí los mesmos socosos,\n",
      "un calito del Cristo.\n",
      "\n",
      "\n",
      "Es una maldicada el habraja\n",
      "un vaciado un decir,\n",
      "pues ocuando medio al Bellina\n",
      "pasanldo se le que tiene,\n",
      "el que uno toldo enfalete\n",
      "y el que sabe lo baja\n",
      "en solparendo allí Cución.\n",
      "\n",
      "Los tongos cojos\n",
      "pero y los recotos,\n",
      "con el reto a aflojar\n",
      "que naides, le dije: un tener\n",
      "por hayas dejas ponejas.\n",
      "\n",
      "Se sata bolar de Osalvarú\n",
      "el que recució es hombre apreCende;\n",
      "yo no es causa tierra pronte\n",
      "se perdió a medio modo.\n",
      "\n",
      "Es como el hay jugado,\n",
      "por esperanza el repelea:\n",
      "ya dejó no está modo\n",
      "lo que de un hijo escuidado,\n",
      "y he aceriado a estabrar\n",
      "que el primero su arroje.\n",
      "\n",
      "En el trance mí no resera\n",
      "en adorme algunacias dona:\n",
      "es gracias, hamas y las nerdas,\n",
      "con pasamente a el malmar\n",
      "me echénmé a mojos resucidas\n",
      "quedó al mis pedre jueras.\n",
      "\n",
      "Se encontrará el cantón\n",
      "por el azunto sacía,\n",
      "y en el evotro la vida\n",
      "y alcíó rede grano\n",
      "el dormir de había niquienda.\n",
      "\n",
      "Es todas que aguardando\n",
      "cuando a es guata tenta;\n",
      "pa su pa escar tingan\n",
      "y no habían ser ansí,\n",
      "¡dichoso al juera arriar.\n",
      "\n",
      "El que dentré ese presencia\n",
      "jamás que tó ranchaña,\n",
      "con mías la cuerdía\n",
      "muchas un cesisonadiuá.\n",
      "\n",
      "Pero indio a un moderido,\n",
      "empré ese al destino,\n",
      "hay el cumidao a Omadre\n",
      "entraba de pedir naides codras\n",
      "hacía a promir la verdía.\n",
      "\n",
      "Decían andaban a heridadao\n",
      "a arreglarle feg allía\n",
      "y la aaguana estacada,\n",
      "reczó un brutagón;\n",
      "tenía que andaba lucía\n",
      "y negraba carnera.\n",
      "\n",
      "\n",
      "Ton áhi puntosió., mataniendo\n",
      "entre trabajarse ús;\n",
      "pues sino es a pretar\n",
      "el todo si por el cucho,\n",
      "jueré si que quería,\n",
      "la vinga muy lada\n",
      "el dégado con el puerro.\n",
      "\n",
      "No salvan de amonandar\n",
      "hay que tantos malonían\n",
      "y salgarnos de ande aquel indía\n",
      "el que se gitaban muy andan\n",
      "imadre, prendía\"a, y y, escuchacar\n",
      "\"Vos prendite cuidao\".\"\n",
      "\n",
      "\n",
      "Pero al eché la aurte más\n",
      "-pero es te aza el principel;\n",
      "y cuanto al todo recule,\n",
      "por cuando se oca enco\n",
      "lue la engran vesa otro el perdido,\n",
      "y allí a esos campás enciando\n",
      "y todo aquella vecir camina.\n",
      "\n",
      "\n",
      "¡Y sonto me quedaré espacie\n",
      "no cuanto escomer!\n",
      "y si vansía el persencian\n",
      "cuando había dao escar!\n",
      "y no por me erarro a llobar\n",
      "como atrá en mi bendito.\n",
      "\n",
      "Si me pasaron de ande mí\n",
      "aquéllo por esponer\n",
      "al juntar en el puntal\n",
      "haciendo la oración\n",
      "y les daré como un rodo\n",
      "\n",
      "que tenían de las mujeres\n",
      "y perde mi encasión\n",
      "que se encuentro nadada\n",
      "que esperó ande mi frontera.\n",
      "\n",
      "Con la prenta muy brimera\n",
      "y emplican los resatos,\n",
      "frento son con la joguera,\n",
      "me amarran la medio dona\n",
      "en fin sacarme aprontida\n",
      "con la orache de mis días.\n",
      "\n",
      "\n",
      "Miento el mundo con cruería\n",
      "para mi comi antescuida,\n",
      "en cuanto me encuentra\n",
      "se aprimero me atropro:\n",
      "nadarán los dejaros\n",
      "de alzar este presencia.\n",
      "\n",
      "El note tanto bayones\n",
      "me tiró en de mi manente;\n",
      "es hombre necerdado al reglande\n",
      "y hacer es que entiende,\n",
      "y aunque es mi madre penade,\n",
      "porque alguna pue al lanza\n",
      "a su lo azo lo me dio.\n",
      "\n",
      "\n",
      "Era le dararle el avermento\n",
      "el pediré en la modena;\n",
      "es guardan lo pasade,\n",
      "aunque lo soy le descono,\n",
      "pues no solitar al nerariar\n",
      "como el payeló brutiar.\n",
      "\n",
      "¡Cuándo! verias a un palo,\n",
      "un punfalo tan sobrar!\n",
      "Se con el gaucho la Dios,\n",
      "al había un mío tormido,\n",
      "esperando el indio es gauasca.\n",
      "\n",
      "Allí en esa campita\n",
      "el que es miedo venir\n",
      "hae de alegrar buena come;\n",
      "que yañas, no que dejó\n",
      "aunque está año del chiquito\n",
      "de toda madre trabajo.\n",
      "\n",
      "\n",
      "Cuando escas, desgracias\n",
      "vano el chuno arracó;\n",
      "sin son alvaca a escansar\n",
      "cantando la reclumanda\n",
      "y al india en con trabajién\n",
      "peo y el indio a versigüento:\n",
      "yo he venga esperando,\n",
      "y ponerme después que habían enversar\n",
      "pero saquella no altcancia.\n",
      "\n",
      "\n",
      "Una flecha el china avesta\n",
      "en el milengo y y respal;\n",
      "lo larga here de sayentar\n",
      "el asunto bola carnel;\n",
      "a toda el corazón\n",
      "perse encante tanto horno,\n",
      "y andaban con aquellas inten\n",
      "sinó el vere cerso.\n",
      "\n",
      "\n",
      "No soy regar el anglo\n",
      "sin que le hable uno atande;\n",
      "sí otra es que donde\n",
      "y hombre aumir fina,\n",
      "pues alenando sierte\n",
      "to más del apararidor\n",
      "y me salvara en el cantho.\n",
      "\n",
      "En cuanto el en estintima\n",
      "enciaré a mis malentas;\n",
      "reculando \n"
     ]
    }
   ],
   "source": [
    "context = \"Los hermanos sean unidos\"\n",
    "context = torch.tensor(encode(context), dtype=torch.long).unsqueeze(0).to(device)\n",
    "generatedText = decode(model.generate(context, nTokens=10000)[0].tolist())\n",
    "print(generatedText)\n",
    "outputDir = \"outputs\"\n",
    "with open(os.path.join(outputDir, \"martin_fierro.txt\"), \"w\") as f:\n",
    "    f.write(generatedText)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
