{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transformer Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoder-Decoder Architecture**\n",
    "\n",
    "It is used for **sequence-to-sequence** tasks like machine translation.\n",
    "\n",
    "This is the architecture proposed by the original transformer paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. in 2017.\n",
    "\n",
    "<img src=\"assets/encoder_decoder.png\" alt=\"Encoder-Decoder Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decoder-Only Architecture**\n",
    "\n",
    "It is used for **sequence generation** tasks like language modeling.\n",
    "\n",
    "This simpler architecture will be implemented in this notebook from scratch.\n",
    "\n",
    "<img src=\"assets/decoder_only.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scaled Dot-Product Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $Q \\in \\mathbb{R}^{m \\times d_k}$, $K \\in \\mathbb{R}^{n \\times d_k}$ y $V \\in \\mathbb{R}^{n \\times d_v}$ be the query, key and value matrices, respectively. The scaled dot-product attention is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\in \\mathbb{R}^{m \\times d_v}\n",
    "$$\n",
    "\n",
    "<img src=\"assets/self_attention.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddingSize: int, headSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.query = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.value = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(blockSize, blockSize)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        weights = q @ k.transpose(-2, -1) / (C ** 0.5)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        output = weights @ v\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $h$ be the number of heads. The multi-head attention is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\in \\mathbb{R}^{m \\times d_v}\n",
    "$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$ and $W_i^Q \\in \\mathbb{R}^{d_k \\times d_k}$, $W_i^K \\in \\mathbb{R}^{d_k \\times d_k}$, $W_i^V \\in \\mathbb{R}^{d_v \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_v}$ are the learnable parameters.\n",
    "\n",
    "<img src=\"assets/multi_head_attention.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, nHeads: int, embeddingSize: int, headSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([ScaledDotProductAttention(embeddingSize, headSize, blockSize, dropout) for _ in range(nHeads)])\n",
    "        self.projection = nn.Linear(embeddingSize, embeddingSize)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        output = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        output = self.projection(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Position-wise Feed-Forward Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position-wise feed-forward networks are defined as:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "where $W_1 \\in \\mathbb{R}^{d_{ff} \\times d_{model}}$, $b_1 \\in \\mathbb{R}^{d_{ff}}$, $W_2 \\in \\mathbb{R}^{d_{model} \\times d_{ff}}$ and $b_2 \\in \\mathbb{R}^{d_{model}}$ are the learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddingSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embeddingSize, 4 * embeddingSize),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embeddingSize, embeddingSize),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Layer Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layer normalization is defined as:\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sigma} + \\beta\n",
    "$$\n",
    "\n",
    "where $\\gamma \\in \\mathbb{R}^{d_{model}}$ and $\\beta \\in \\mathbb{R}^{d_{model}}$ are the learnable parameters, and $\\mu$ and $\\sigma$ are the mean and standard deviation of $x$, respectively.\n",
    "\n",
    "In the original paper, the layer normalization is applied after the residual connections, whereas in more modern implementations, it is applied before in what is known as **pre-norm formulation**.\n",
    "\n",
    "```python\n",
    "# Original paper\n",
    "x = LayerNorm(x + MultiHeadAttention(x))\n",
    "x = LayerNorm(x + FeedForward(x))\n",
    "\n",
    "# Pre-norm formulation\n",
    "x = x + MultiHeadAttention(LayerNorm(x))\n",
    "x = x + FeedForward(LayerNorm(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "\n",
    "    def __init__(self, nHeads: int, embeddingSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        headSize = embeddingSize // nHeads\n",
    "        self.multiHeadAttention = MultiHeadAttention(nHeads, embeddingSize, headSize, blockSize, dropout)\n",
    "        self.feedForward = FeedForward(embeddingSize, dropout)\n",
    "        self.layerNorm1 = nn.LayerNorm(embeddingSize)\n",
    "        self.layerNorm2 = nn.LayerNorm(embeddingSize)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.multiHeadAttention(self.layerNorm1(x))\n",
    "        x = x + self.feedForward(self.layerNorm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__ (self, vocabularySize: int, nLayers: int, nHeads: int, embeddingSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenEmbeddingTable = nn.Embedding(vocabularySize, embeddingSize)\n",
    "        self.positionEmbeddingTable = nn.Embedding(blockSize, embeddingSize)\n",
    "        self.layers = nn.Sequential(*[Layer(nHeads, embeddingSize, blockSize, dropout) for _ in range(nLayers)])\n",
    "        self.layerNorm = nn.LayerNorm(embeddingSize)\n",
    "        self.linearModelHead = nn.Linear(embeddingSize, vocabularySize)\n",
    "        self.blockSize = blockSize\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor, targets: torch.Tensor = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        B, T = idx.shape\n",
    "        tokenEmbeddings = self.tokenEmbeddingTable(idx)\n",
    "        positionEmbeddings = self.positionEmbeddingTable(torch.arange(T, device=device))\n",
    "        x = tokenEmbeddings + positionEmbeddings\n",
    "        x = self.layers(x)\n",
    "        x = self.layerNorm(x)\n",
    "        logits = self.linearModelHead(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.Tensor, nTokens: int) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(nTokens):\n",
    "                logits, loss = self(idx[:, -self.blockSize:])\n",
    "                logits = logits[:, -1, :]\n",
    "                probabilities = F.softmax(logits, dim=-1)\n",
    "                nextIdx = torch.multinomial(probabilities, num_samples=1)\n",
    "                idx = torch.cat([idx, nextIdx], dim=1)\n",
    "        self.train()\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 187095\n",
      "I\n",
      "\n",
      "Aquí me pongo a cantar\n",
      "al compás de la vigüela,\n",
      "que el hombre que lo desvela\n",
      "una pena estrordinaria,\n",
      "como la ave solitaria\n",
      "con el cantar se consuela.\n",
      "\n",
      "Pido a los santos del cielo\n",
      "que ayuden mi pensamiento:\n",
      "les pido en este momento\n",
      "que voy a cantar\n"
     ]
    }
   ],
   "source": [
    "dataDir = \"data\"\n",
    "file = os.path.join(dataDir, \"martin_fierro.txt\")\n",
    "with open(file, \"r\") as f:\n",
    "    text = f.read()\n",
    "print(f\"Text length: {len(text)}\")\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 72\n",
      "\n",
      " !\"'(),-.:;?ABCDEFGHIJLMNOPQRSTUVXYZabcdefghijklmnopqrstuvxyz¡¿Ñáéíñóúü\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sorted(set(text))\n",
    "vocabularySize = len(vocabulary)\n",
    "print(f\"Vocabulary size: {vocabularySize}\")\n",
    "print(\"\".join(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 51, 55, 1, 44, 41, 54, 49, 37, 50, 51, 55, 1, 55, 41, 37, 50, 1, 57, 50, 45, 40, 51, 55]\n",
      "Los hermanos sean unidos\n"
     ]
    }
   ],
   "source": [
    "stringToToken = {ch: i for i, ch in enumerate(vocabulary)}\n",
    "tokenToString = {i: ch for i, ch in enumerate(vocabulary)}\n",
    "encode = lambda string: [stringToToken[char] for char in string]\n",
    "decode = lambda tokens: \"\".join([tokenToString[token] for token in tokens])\n",
    "print(encode(\"Los hermanos sean unidos\"))\n",
    "print(decode(encode(\"Los hermanos sean unidos\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.79M parameters\n"
     ]
    }
   ],
   "source": [
    "blockSize = 256\n",
    "embeddingSize = 384\n",
    "nHeads = 6\n",
    "nLayers = 6\n",
    "dropout = 0.2\n",
    "\n",
    "model = GPTLanguageModel(vocabularySize, nLayers, nHeads, embeddingSize, blockSize, dropout).to(device)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsDir = \"models\"\n",
    "model.load_state_dict(torch.load(os.path.join(modelsDir, \"martin_fierro.pt\"), weights_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los hermanos sean unidos;\n",
      "en mi hijos muy bolicho\n",
      "que yo nos lo daré\n",
      "mientras dos que andan....\n",
      "no el diablo no hay rasco.\n",
      "\n",
      "En mi pegazo de mi madre\n",
      "En salvajía ni tanto pie;\n",
      "tan cruzal oveja limero\n",
      "y silencio por de jamás.\n",
      "\n",
      "Que anda es milos pasabas\n",
      "otanto me lio me atrevo;\n",
      "y en el mesmo tomo si guarro\n",
      "porque el se cordero\n",
      "el mesmo que tan de ma,\n",
      "aregote de servierte\n",
      "y tanta que los inorancian.\n",
      "\n",
      "Viergo al todos las preciones\n",
      "para darle a su delito\n",
      "del llegar cautivo dosao;\n",
      "entre el jefo, y empero,\n",
      "nunca si fer bavario,\n",
      "uno matir y de yde la virgüela.\n",
      "\n",
      "Al verse jir de los pasas\n",
      "como sanarido certigo,\n",
      "sólo ha dicho es un horno\n",
      "más cerdido de Martín,\n",
      "de llaban los dos empesantos\n",
      "y se lloran y otra.\n",
      "\n",
      "Yo allí llegan tan día\n",
      "dirigitas de las raciones;\n",
      "todas años los ojos\n",
      "ningún un rancho dar;\n",
      "visiera mojadonar,\n",
      "y la hinia infeliz de arriba\n",
      "sin que no salenza\n",
      "dende consejicia para.\n",
      "\n",
      "Ese el tromendo del horror\n",
      "que compas aguarar;\n",
      "alguno, no solvir,\n",
      "en la carazón;\n",
      "no habla por vastanto,\n",
      "se le daba dentrato,\n",
      "a los presos tiromentos\n",
      "bajo los inferos de ramar.\n",
      "\n",
      "Aquéllos agerroson servijo,\n",
      "si no es dos remos años,\n",
      "nos hay rigararidos de Dios;\n",
      "que nunca lo que está están\n",
      "sinó un dáa otra dentro de chando\n",
      "cuando es la amada po.\n",
      "\n",
      "El todo la comura,\n",
      "el clarero de mano,\n",
      "como más amorido;\n",
      "los indios soy me traigo\n",
      "que no nos dellón menos.\n",
      "\n",
      "No tenga uno de abajar;\n",
      "el desabón los precesos\n",
      "y como precausante;\n",
      "y dando esos yos razones\n",
      "en barrigo, el cordero,\n",
      "hasta tomuradarido,\n",
      "eh cruzao que el malor.\n",
      "\n",
      "Una más güena del laz\n",
      "como una cargué;\n",
      "ésta era ofinillo galpe,\n",
      "la milica del su arrigo,\n",
      "y amigo, hasta al pago,\n",
      "éotro si los han milos!\n",
      "\n",
      "sin tanto al lazo infelo\n",
      "y sin turar esa conseja\n",
      "y los dirén y las pajes\n",
      "como o viviejas,\n",
      "de con reque no esebátra,\n",
      "por naides ha de ser causa,\n",
      "pues y ha topa que la escapa.\n",
      "\n",
      "Me diréen al hombre trabas\n",
      "como si un diablo;\n",
      "vino de lo quiebra es el decía,\n",
      "busca recobiao,\n",
      "al verate de se vir,\n",
      "y a ver ese a ruhombre;\n",
      "si lo amigo haciendo,\n",
      "en aquella momenta.\n",
      "\n",
      "Mas sabio, por es día\n",
      "en el desierpo del fortero\n",
      "naides se le queje;\n",
      "yo hay ganas que estaba\n",
      "en la infliz que está conselo\n",
      "no que perrime a medio\n",
      "el más inora a más.\n",
      "\n",
      "Y en su ilonncia ni y dura\n",
      "hoy siempo, hasta nueva,\n",
      "y aél la cosa esombra\n",
      "las ofrecían las oheras,\n",
      "sin dentradas, la invasión,\n",
      "o porque las asomas\n",
      "se di por que le dio.\n",
      "\n",
      "Si porque en al mistra\n",
      "redzan cruzado en el cariño;\n",
      "sin volunto en un pajo\n",
      "sufrimido en eso y cerdo;\n",
      "el mesmo es muy pronto\n",
      "en otro del tirarlo darito.\n",
      "\n",
      "Y sé sobre es pienso\n",
      "a naides me penar;Traiba de mi cuargero;\n",
      "el que no he de despino\n",
      "tramos perdido y el día,\n",
      "al llazo, a sompurarido,\n",
      "en esa trepella,\n",
      "en aquella vez mi pupara.\n",
      "\n",
      "En mil indio, y madrigo,\n",
      "tarde levaba de aña,\n",
      "hasta que lo emperrao,\n",
      "para todos sus vinos\n",
      "a están los salvajos,\n",
      "sin los mesmos que lo imora\n",
      "y se corazón al brimo.\n",
      "\n",
      "Porqué tambiérse saber,\n",
      "rago y los duros, los llantos\n",
      "como lindan suches echanchancos\n",
      "con el poms el arreglo,\n",
      "pues indio el cuelo del pa)\n",
      "hace no celencia los dos,\n",
      "y alguna no hasta negro.\n",
      "\n",
      "La gringa con angencias,\n",
      "a para desnudo,\n",
      "y donde se este enao,\n",
      "y ve en al carrero o señor,\n",
      "pa tiro a en un troazo\n",
      "que se duró de ests muy frir.\n",
      "\n",
      "Y tanto como un gringo\n",
      "que se cuida comierto:\n",
      "no perdigo, a chiquiera en servamo,\n",
      "a siempre de nto no dejaba\n",
      "lo aguardarlo la enterra.\n",
      "\n",
      "¡Y qué como se hizo un bravo,\n",
      "sin corazóo, cojo amaradó!\n",
      "Iera más pararido;\n",
      "mas no ha dio más avillo\n",
      "como que es deberajo!\n",
      "\n",
      "Del decir los dos retoresos\n",
      "se los más mirabos recoos;\n",
      "yo tanto ganó al paranto;\n",
      "él mandan de que era mandan,\n",
      "no trato digo al servicio,\n",
      "y soy resesarion y la vieja.\n",
      "\n",
      "Cuando en un terribo\n",
      "que crias de lo al cruzal;\n",
      "el nlogro ligrimente al lamano\n",
      "vientra naides comente;\n",
      "en su nico las no los engenten\n",
      "con el en, el silbor,\n",
      "para como que andan encantan.\n",
      "\n",
      "¡Vajarnos tropillando\n",
      "sina sabe razón,\n",
      "de uno los ausentinas,\n",
      "dentraban un males,\n",
      "y a guascas.\n",
      "\n",
      "la virgüelas no se atrotan,\n",
      "hago gaucho manamigo.\n",
      "Para que sabrigan\n",
      "los ratiros de barrno;\n",
      "\n",
      "sin mesmos, hacíamos ver\n",
      "y mesmio sacantojar.\n",
      "Primero, miramos lombrar.\n",
      "no dirite ha de enfermo,\n",
      "de comisa a mi jaflo\n",
      "cuando lo que conocía.\n",
      "\n",
      "Quien el campo de arrimir,\n",
      "vivieron y gaucho,\n",
      "que no tiene otro quieno,\n",
      "de alegra en su amamas\n",
      "lo dolor cautivano.\n",
      "\n",
      "Nos daraciando aquel muerto,\n",
      "llego a tratar em lata\n",
      "y al proncho ande lena;\n",
      "tan cafelo\n",
      "el orillo en embasante,\n",
      "o tiro comendiaba\n",
      "que venir la tierra.\n",
      "\n",
      "Todo sigura el fano\n",
      "el cruzarido primero;\n",
      "en el omentro animal\n",
      "se me duró con el brigor,\n",
      "y tayo en aquel momento\n",
      "primero que yo despentido\n",
      "eras de mis suyo quecha.\n",
      "\n",
      "Nacien la manua un olHama\n",
      "con el tribo que se muerto:\n",
      "el gaucho,, el cárrio, tor,\n",
      "mas, en las uno rigor\n",
      "ningunra en sa'a aflicia.\n",
      "\n",
      "Marcha remesanto lo atrde,\n",
      "hace se lo hace el destreza;\n",
      "ve cuando en seña seguida,\n",
      "y siempre todo es es yo;\n",
      "no ha mayor, ni ni moserio\n",
      "dando güéiandoló en las sacas.\n",
      "\n",
      "Al viejo como sin trabas\n",
      "como hasta criél y hancho;\n",
      "saga relatá se me ator,\n",
      "refiero me correro favo:\n",
      "en que no pade morir\n",
      "se camisa no hay vista,\n",
      "en mi cruzao de barujo,\n",
      "anque no daba grabiando.\n",
      "\n",
      "¡Hi empeñado, él de entré\n",
      "su ojogao el terro eseno!\n",
      "No ha de casiado serviero\n",
      "para noche quise añor\n",
      "sabe yo ni me razón,\n",
      "más mi jefe y lo que amigo,\n",
      "\n",
      "pero siempre al hombre de dí:\n",
      "\n",
      "si no me empure el cordero\n",
      "servicio con padece.\n",
      "Duelmente cantos\n",
      "sin noche de engronteridos,\n",
      "con es el barrovaridonador;\n",
      "les dieran nuevas, como el salijo;\n",
      "las manos les promas son han dudas.\n",
      "\n",
      "Yo he visto con tan madre\n",
      "ni los ojos carre el destino;\n",
      "cuando en el mesmo en calma,\n",
      "lloran al pajar con la broma\n",
      "y ha empezao si los rigolas.\n",
      "\n",
      "XXXII\n",
      "\n",
      "Mientran el cnuevo por quien ladro,\n",
      "hasta que el más infierro\n",
      "al ditiariol sentir;\n",
      "tanto lo pudo regular\n",
      "en la autoridá.\n",
      "\n",
      "Me dicha las oracionos,\n",
      "sus milicas y tenía;\n",
      "naté en medio y la piedra\n",
      "sin me darme y las quiera\n",
      "el güefierno los manios\n",
      "y que viveren mi tarrdas.\n",
      "\n",
      "Yo oy, si otro iban los blazos\n",
      "como sinocio el lanzanza;\n",
      "la manoca naca a pa Dios\n",
      "no tengan de cuanto,\n",
      "ni se la para o de bastanto.\n",
      "\n",
      "A sé en los tropos milones\n",
      "barazotaos de amarrta;\n",
      "en la india a madere morar.\n",
      "Lo es un bandito la muerto,\n",
      "y siempre en el potro echar\n",
      "jamos esta arriblar,\n",
      "y buscando-ca.\n",
      "\n",
      "áhi no más págrimosamos,\n",
      "y siempre ni seguido,\n",
      "ni ha...to ni mátor fermo,\n",
      "¡ni vatisia no se ha perdidá!\n",
      "\n",
      "la visticia que no veo\n",
      "¡AniOtencia que tardela!\n",
      "\n",
      "¡jamás has ta otra las resueldas\n",
      "es es cuera de hamos salva!\n",
      "\n",
      "Aunque no es oye lengao\n",
      "al lo virimento en mi hacho!\n",
      "Yo cuando no he venido\n",
      "tenía canso aflojado;\n",
      "dende entre animenturro\n",
      "aunque está medio un sabio.\n",
      "\n",
      "Para conoce al día estancia\n",
      "del empro comendé domor;\n",
      "alcáin los darones\n",
      "como por dos pintino;\n",
      "cuando uno el tiro\n",
      "en aquella ogerra de entreta,\n",
      "y hemos del corazón,\n",
      "son de animales\n",
      "como unos diebos lantinos\n",
      "\n",
      "Cuando el jefe me inferro\n",
      "dé los rayidos pasaba;\n",
      "sin me diron indios yo\n",
      "a mía ni triste revos.\n",
      "Y al vercios nombre camiento.\n",
      "\n",
      "Yo todos cantor bien primor\n",
      "ninguno en mi mativo;\n",
      "siempre con todos mi barmidos,\n",
      "soy si plazo que el máerdo\n",
      "que yo más bien los mirones.\n",
      "\n",
      "Yo sacua que me encontro\n",
      "la miedo del corrón,\n",
      "y resueldá de entre más\n",
      "¡naides le as agua;\n",
      "la que es dela del máNORE\n",
      "\n",
      "Ino alión si compasiona\n",
      "quisa tan livida,\n",
      "en vive eciao del rigor\n",
      "como la empara a plea.\n",
      "Tienda cincelo y sejos\n",
      "el recho del horado;\n",
      "y el tiempo el hombre de valiario\n",
      "no ha que un babara infiao,\n",
      "\n",
      "es ser le va un mermorro\n",
      "de todos la mesmoria.\n",
      "\n",
      "Que me persentaba compoverente;\n",
      "que traica habla prender,\n",
      "y me sangurió en un destino\n",
      "en una mora en en la ciego,\n",
      "ni primero qué a la oca.\n",
      "\n",
      "El mzoro que la han de cuida,\n",
      "én uno la conociende;\n",
      "Y ninguno se me áarrovir,\n",
      "la frontera un gritor,\n",
      "el presoniado mis horror;\n",
      "en el hombre torrorioso\n",
      "le pasando jura y flor.\n",
      "\n",
      "Dentró la vinta muy vaz,\n",
      "yo hago andanza y lo repina;\n",
      "siempre faltanzó la haria\n",
      "le diogüega que es día.\n",
      "\n",
      "Tran señan chisanos\n",
      "en la falta y manoja\n",
      "cumpre y con matrero,\n",
      "siempre de encerro,\n",
      "aunque la virgüela criñala,\n",
      "sin que al fin delao,\n",
      "como le daba ajéil.\n",
      "\n",
      "Se conocí al la tranciza\n",
      "como un recoberrío;\n",
      "cristiano, bien mi cuando,\n",
      "y no hay podo causario,\n",
      "nosí matar que hacea rencerdido\n",
      "con tamigo de tenvo.\n",
      "\n",
      "II\n",
      "\n",
      "Manda planas que estuvir\n",
      "sin tanto la cielo,\n",
      "bien, como que yo he dido,\n",
      "pero sinculo hago al día,\n",
      "pero ninguno le han rancaso\n",
      "las recordas y los mirones.\n",
      "\n",
      "Y cadao, con los dios,\n",
      "todos semponan lo almane\n",
      "al juez mi perdido Dios\n",
      "de uno una olvida yo;\n",
      "ha lanza otro jaribundo\n",
      "aunque no lo úeniitenmente\n",
      "quise le diablo un carra.\n",
      "\n",
      "Nosotros, con que esplician,\n",
      "toda razón, en uno salienta,\n",
      "no se que les dieban un brar\n",
      "en camafianzones y no gana;\n",
      "uno llegoan ochan darlo ni bromas\n",
      "fieran el desierro de viejo.\n",
      "\n",
      "IX\n",
      "\n",
      "Uno empe, como a mi ba,\n",
      "y aunque conocí la provida,\n",
      "ninguno bastó a hizo caratón:\n",
      "\"ese en un troazo meterrao\n",
      "aquéllo na grite ristravo;\n",
      "\"siempre el hombre guario\n",
      "\"un nombre debe aquír.\"\n",
      "Tu saber vizvo por var,\n",
      "y o no me acercar:\n",
      "también que no podía\n",
      "se descuidar consuelo,\n",
      "como el senario de antencia:\n",
      "siendo el cruzarle asiles\n",
      "que al mundo ade lo de pelerro.\n",
      "\n",
      "Esi me vino cometó\n",
      "si lo vida me tenía;\n",
      "el regundo que trate siguro\n",
      "tiro se me digo ajusa;\n",
      "se ven a mis altas\n",
      "y de ha de Dios he debertido\n",
      "\"no se afierró en carrera.\"\n",
      "\n",
      "\"He defender dos tono ambuivo\n",
      "a engar del infeliz:\n",
      "el avir tá me dirío,\n",
      "el gaucho era mi horribo,\n",
      "y esto ni lo hacienda.\n",
      "\n",
      "Lo mesmo atropé el porro,\n",
      "hasta en el palo de canto;\n",
      "le da ñas, cuerdo de un hombre\n",
      "el daño de te pos que gana\n",
      "que era dentra del mundo\n",
      "en carge ris a sus varaciones.\n",
      "\n",
      "Al varón de uno cariño,\n",
      "siempre es salvaje, cuando;\n",
      "era amigo con su conocelo,\n",
      "se inorancia no andancia,\n",
      "ni dan como que envenendencia\n",
      "y no aprendan dispas.\n",
      "\n",
      "El hojo vejamos la perda,\n",
      "que es sabe acosivar;\n",
      "no hay invade desperar\n",
      "pues se albarle en un lrato\n",
      "es lo que estárba del yerro\n",
      "no hastro que nacia.\n",
      "\n",
      "Hacetir le gitarres de sus hijos\n",
      "tanto las trasqao las carrenas\n",
      "que naides animale,\n",
      "viviene sin tenir hajas,\n",
      "pero a gaucho de mi inor.\n",
      "\n",
      "Soy inorancia fuego\n",
      "ha de tranto y reglo;\n",
      "a un peligro llegao,\n",
      "en el señrro tan gaucho,\n",
      "hasta que sin uno, han haigo.\n",
      "ponga el pasado el contano\n",
      "donde aquéllo tan barazo.\n",
      "\n",
      "Porque el inme\n"
     ]
    }
   ],
   "source": [
    "context = \"Los hermanos sean unidos\"\n",
    "context = torch.tensor(encode(context), dtype=torch.long).unsqueeze(0).to(device)\n",
    "generatedText = decode(model.generate(context, nTokens=10000)[0].tolist())\n",
    "print(generatedText)\n",
    "outputDir = \"outputs\"\n",
    "with open(os.path.join(outputDir, \"martin_fierro.txt\"), \"w\") as f:\n",
    "    f.write(generatedText)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
