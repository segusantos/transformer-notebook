{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqS26uut1_7N"
   },
   "source": [
    "# **Transformer Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVeQuGBF1_7T"
   },
   "source": [
    "### **Encoder-Decoder Architecture**\n",
    "\n",
    "It is used for **sequence-to-sequence** tasks like machine translation.\n",
    "\n",
    "This is the architecture proposed by the original transformer paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. in 2017.\n",
    "\n",
    "<img src=\"assets/encoder_decoder.png\" alt=\"Encoder-Decoder Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RE1K3Dex-h6Z"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APfB9jre1_7V"
   },
   "source": [
    "### **Decoder-Only Architecture**\n",
    "\n",
    "It is used for **sequence generation** tasks like language modeling.\n",
    "\n",
    "This simpler architecture will be implemented in this notebook from scratch.\n",
    "\n",
    "<img src=\"assets/decoder_only.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1749658844461,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "41bejM8V1_7W",
    "outputId": "dcab5d71-a00c-47d4-a108-2c5a2ef2c6eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not Path(\"transformer-notebook\").exists():\n",
    "        !git clone https://github.com/segusantos/transformer-notebook.git\n",
    "    %cd transformer-notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BxQZcCR1_7a"
   },
   "source": [
    "## **Scaled Dot-Product Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gY3WuR6c1_7b"
   },
   "source": [
    "Let $Q \\in \\mathbb{R}^{m \\times d_k}$, $K \\in \\mathbb{R}^{n \\times d_k}$ y $V \\in \\mathbb{R}^{n \\times d_v}$ be the query, key and value matrices, respectively. The scaled dot-product attention is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\in \\mathbb{R}^{m \\times d_v}\n",
    "$$\n",
    "\n",
    "<img src=\"assets/self_attention.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mSfCf-011_7c"
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 sequenceLength: int,\n",
    "                 embeddingSize: int,\n",
    "                 headSize: int,\n",
    "                 dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.query = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.value = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(sequenceLength, sequenceLength)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batchSize, sequenceLength, embeddingSize = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        weights = q @ k.transpose(-2, -1) / (embeddingSize ** 0.5)\n",
    "        weights = weights.masked_fill(self.tril[:sequenceLength, :sequenceLength] == 0, float(\"-inf\"))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        x = weights @ v\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0SNKag41_7d"
   },
   "source": [
    "## **Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erjXpdQe1_7e"
   },
   "source": [
    "Let $h$ be the number of heads. The multi-head attention is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\in \\mathbb{R}^{m \\times d_v}\n",
    "$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$ and $W_i^Q \\in \\mathbb{R}^{d_k \\times d_k}$, $W_i^K \\in \\mathbb{R}^{d_k \\times d_k}$, $W_i^V \\in \\mathbb{R}^{d_v \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_v}$ are the learnable parameters.\n",
    "\n",
    "<img src=\"assets/multi_head_attention.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DmUUm3Db1_7f"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 nHeads: int,\n",
    "                 sequenceLength: int,\n",
    "                 embeddingSize: int,\n",
    "                 dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([ScaledDotProductAttention(sequenceLength,\n",
    "                                                              embeddingSize,\n",
    "                                                              embeddingSize // nHeads,\n",
    "                                                              dropout) for _ in range(nHeads)])\n",
    "        self.projection = nn.Linear(embeddingSize, embeddingSize)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.projection(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECfdczW81_7g"
   },
   "source": [
    "## **Position-wise Feed-Forward Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1GfRIuN1_7g"
   },
   "source": [
    "The position-wise feed-forward networks are defined as:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "where $W_1 \\in \\mathbb{R}^{d_{ff} \\times d_{model}}$, $b_1 \\in \\mathbb{R}^{d_{ff}}$, $W_2 \\in \\mathbb{R}^{d_{model} \\times d_{ff}}$ and $b_2 \\in \\mathbb{R}^{d_{model}}$ are the learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qBpO3Eq21_7h"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embeddingSize: int,\n",
    "                 dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.feedForward = nn.Sequential(\n",
    "            nn.Linear(embeddingSize, 4 * embeddingSize),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embeddingSize, embeddingSize),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.feedForward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Sfu0Cvp1_7h"
   },
   "source": [
    "## **Layer Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0LWpqI71_7h"
   },
   "source": [
    "The layer normalization is defined as:\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sigma} + \\beta\n",
    "$$\n",
    "\n",
    "where $\\gamma \\in \\mathbb{R}^{d_{model}}$ and $\\beta \\in \\mathbb{R}^{d_{model}}$ are the learnable parameters, and $\\mu$ and $\\sigma$ are the mean and standard deviation of $x$, respectively.\n",
    "\n",
    "In the original paper, the layer normalization is applied after the residual connections, whereas in more modern implementations, it is applied before in what is known as **pre-norm formulation**.\n",
    "\n",
    "```python\n",
    "# Original paper\n",
    "x = LayerNorm(x + MultiHeadAttention(x))\n",
    "x = LayerNorm(x + FeedForward(x))\n",
    "\n",
    "# Pre-norm formulation\n",
    "x = x + MultiHeadAttention(LayerNorm(x))\n",
    "x = x + FeedForward(LayerNorm(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lereN3iL1_7i"
   },
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 nHeads: int,\n",
    "                 sequenceLength: int,\n",
    "                 embeddingSize: int,\n",
    "                 dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.multiHeadAttention = MultiHeadAttention(nHeads,\n",
    "                                                     sequenceLength,\n",
    "                                                     embeddingSize,\n",
    "                                                     dropout)\n",
    "        self.feedForward = FeedForward(embeddingSize, dropout)\n",
    "        self.layerNorm1 = nn.LayerNorm(embeddingSize)\n",
    "        self.layerNorm2 = nn.LayerNorm(embeddingSize)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.multiHeadAttention(self.layerNorm1(x))\n",
    "        x = x + self.feedForward(self.layerNorm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ke_AmotY1_7i"
   },
   "source": [
    "## **Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "R-xFgrgj1_7i"
   },
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__ (self,\n",
    "                  vocabularySize: int,\n",
    "                  nLayers: int,\n",
    "                  nHeads: int,\n",
    "                  sequenceLength:int,\n",
    "                  embeddingSize: int,\n",
    "                  dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenEmbeddingTable = nn.Embedding(vocabularySize, embeddingSize)\n",
    "        self.positionEmbeddingTable = nn.Embedding(sequenceLength, embeddingSize)\n",
    "        self.layers = nn.Sequential(*[Layer(nHeads,\n",
    "                                            sequenceLength,\n",
    "                                            embeddingSize,\n",
    "                                            dropout) for _ in range(nLayers)])\n",
    "        self.layerNorm = nn.LayerNorm(embeddingSize)\n",
    "        self.linearModelHead = nn.Linear(embeddingSize, vocabularySize)\n",
    "        self.sequenceLength = sequenceLength\n",
    "        self.apply(self.initWeights)\n",
    "\n",
    "    def initWeights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, targets: torch.Tensor = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        batchSize, sequenceLength = x.shape\n",
    "        tokenEmbeddings = self.tokenEmbeddingTable(x)\n",
    "        positionEmbeddings = self.positionEmbeddingTable(torch.arange(sequenceLength, device=device))\n",
    "        x = tokenEmbeddings + positionEmbeddings\n",
    "        x = self.layers(x)\n",
    "        x = self.layerNorm(x)\n",
    "        logits = self.linearModelHead(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batchSize, sequenceLength, embeddingSize = logits.shape\n",
    "            logits = logits.view(batchSize * sequenceLength, embeddingSize)\n",
    "            targets = targets.view(batchSize * sequenceLength)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x: torch.Tensor, nTokens: int) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(nTokens):\n",
    "                logits, loss = self(x[:, -self.sequenceLength:])\n",
    "                probabilities = F.softmax(logits[:, -1, :], dim=-1)\n",
    "                nextToken = torch.multinomial(probabilities, num_samples=1)\n",
    "                x = torch.cat([x, nextToken], dim=1)\n",
    "        self.train()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExfoF3cx1_7j"
   },
   "source": [
    "## **Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 89,
     "status": "ok",
     "timestamp": 1749658845179,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "oEK1NJv41_7j",
    "outputId": "fbbff5ef-5f06-4f6d-8e48-d2d422cb91d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = \"martin_fierro\" # \"shakespeare\"\n",
    "with open((Path(\"data\") / dataset).with_suffix(\".txt\"), \"r\") as f:\n",
    "    text = f.read()\n",
    "print(f\"Text length: {len(text)}\")\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1749658845186,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "Ut9lqYSO1_7j",
    "outputId": "26b0d998-d4d7-4a51-c277-e08eb32eefcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sorted(set(text))\n",
    "vocabularySize = len(vocabulary)\n",
    "print(f\"Vocabulary size: {vocabularySize}\")\n",
    "print(\"\".join(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1749658845192,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "kLvqV9Jr1_7k",
    "outputId": "cdeecf7a-6800-4b26-c7dc-5088984bd483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 53, 57, 1, 46, 43, 56, 51, 39, 52, 53, 57, 1, 57, 43, 39, 52, 1, 59, 52, 47, 42, 53, 57]\n",
      "Los hermanos sean unidos\n"
     ]
    }
   ],
   "source": [
    "stringToToken = {ch: i for i, ch in enumerate(vocabulary)}\n",
    "tokenToString = {i: ch for i, ch in enumerate(vocabulary)}\n",
    "encode = lambda string: [stringToToken[char] for char in string]\n",
    "decode = lambda tokens: \"\".join([tokenToString[token] for token in tokens])\n",
    "print(encode(\"Los hermanos sean unidos\"))\n",
    "print(decode(encode(\"Los hermanos sean unidos\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1749658845198,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "grysOjv-1_7k",
    "outputId": "50ab0090-7977-4e9c-a771-cd20eec6074c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([1115394])\n",
      "Data type: torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Data type: {data.dtype}\")\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1749658845227,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "pZjPFtGo1_7l",
    "outputId": "77fb5cb4-a26e-4607-9478-851e6007e166"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([892315])\n",
      "Validation data shape: torch.Size([223079])\n"
     ]
    }
   ],
   "source": [
    "trainValSplit = 0.8\n",
    "trainSize = int(len(data) * trainValSplit)\n",
    "trainData = data[:trainSize]\n",
    "valData = data[trainSize:]\n",
    "print(f\"Train data shape: {trainData.shape}\")\n",
    "print(f\"Validation data shape: {valData.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1749658845228,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "QWuJtH4g1_7l",
    "outputId": "a86d4714-747f-4d20-bac6-7204340694c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context batch shape: torch.Size([4, 8])\n",
      "tensor([[47, 57, 10,  1, 39, 52, 42,  1],\n",
      "        [59, 56,  1, 46, 43, 39, 56, 58],\n",
      "        [32, 46, 39, 58,  1, 39, 50, 61],\n",
      "        [26, 53, 58, 46, 47, 52, 45,  1]], device='cuda:0')\n",
      "Target batch shape: torch.Size([4, 8])\n",
      "tensor([[57, 10,  1, 39, 52, 42,  1, 50],\n",
      "        [56,  1, 46, 43, 39, 56, 58, 57],\n",
      "        [46, 39, 58,  1, 39, 50, 61, 39],\n",
      "        [53, 58, 46, 47, 52, 45,  1, 40]], device='cuda:0')\n",
      "Context: [47] -> Target: 57\n",
      "Context: [47, 57] -> Target: 10\n",
      "Context: [47, 57, 10] -> Target: 1\n",
      "Context: [47, 57, 10, 1] -> Target: 39\n",
      "Context: [47, 57, 10, 1, 39] -> Target: 52\n",
      "Context: [47, 57, 10, 1, 39, 52] -> Target: 42\n",
      "Context: [47, 57, 10, 1, 39, 52, 42] -> Target: 1\n",
      "Context: [47, 57, 10, 1, 39, 52, 42, 1] -> Target: 50\n",
      "Context: [59] -> Target: 56\n",
      "Context: [59, 56] -> Target: 1\n",
      "Context: [59, 56, 1] -> Target: 46\n",
      "Context: [59, 56, 1, 46] -> Target: 43\n",
      "Context: [59, 56, 1, 46, 43] -> Target: 39\n",
      "Context: [59, 56, 1, 46, 43, 39] -> Target: 56\n",
      "Context: [59, 56, 1, 46, 43, 39, 56] -> Target: 58\n",
      "Context: [59, 56, 1, 46, 43, 39, 56, 58] -> Target: 57\n",
      "Context: [32] -> Target: 46\n",
      "Context: [32, 46] -> Target: 39\n",
      "Context: [32, 46, 39] -> Target: 58\n",
      "Context: [32, 46, 39, 58] -> Target: 1\n",
      "Context: [32, 46, 39, 58, 1] -> Target: 39\n",
      "Context: [32, 46, 39, 58, 1, 39] -> Target: 50\n",
      "Context: [32, 46, 39, 58, 1, 39, 50] -> Target: 61\n",
      "Context: [32, 46, 39, 58, 1, 39, 50, 61] -> Target: 39\n",
      "Context: [26] -> Target: 53\n",
      "Context: [26, 53] -> Target: 58\n",
      "Context: [26, 53, 58] -> Target: 46\n",
      "Context: [26, 53, 58, 46] -> Target: 47\n",
      "Context: [26, 53, 58, 46, 47] -> Target: 52\n",
      "Context: [26, 53, 58, 46, 47, 52] -> Target: 45\n",
      "Context: [26, 53, 58, 46, 47, 52, 45] -> Target: 1\n",
      "Context: [26, 53, 58, 46, 47, 52, 45, 1] -> Target: 40\n"
     ]
    }
   ],
   "source": [
    "def getBatch(data: torch.Tensor,\n",
    "             batchSize: int,\n",
    "             sequenceLength: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    ix = torch.randint(0, data.size(0) - sequenceLength, (batchSize,))\n",
    "    x = torch.stack([data[i:i+sequenceLength] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+sequenceLength+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "batchSize = 4\n",
    "sequenceLength = 8\n",
    "xBatch, yBatch = getBatch(trainData, batchSize, sequenceLength)\n",
    "print(f\"Context batch shape: {xBatch.shape}\")\n",
    "print(xBatch)\n",
    "print(\"Target batch shape:\", yBatch.shape)\n",
    "print(yBatch)\n",
    "for batch in range(batchSize):\n",
    "    for token in range(sequenceLength):\n",
    "        context = xBatch[batch, :token+1].tolist()\n",
    "        target = yBatch[batch, token].item()\n",
    "        print(f\"Context: {context} -> Target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 175,
     "status": "ok",
     "timestamp": 1749658845400,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "AivElGvA1_7m",
    "outputId": "203624f4-bed4-4638-aca1-4f1915f29a45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.79M parameters\n"
     ]
    }
   ],
   "source": [
    "nLayers = 6\n",
    "nHeads = 6\n",
    "sequenceLength = 256\n",
    "embeddingSize = 384\n",
    "dropout = 0.2\n",
    "model = GPTLanguageModel(vocabularySize,\n",
    "                         nLayers,\n",
    "                         nHeads,\n",
    "                         sequenceLength,\n",
    "                         embeddingSize,\n",
    "                         dropout).to(device)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 727882,
     "status": "ok",
     "timestamp": 1749659573282,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "IjEwb0Ni1_7m",
    "outputId": "18c51863-8994-4cd2-f242-d6b27b10184a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Train loss: 4.284997940063477, Val loss: 4.282075881958008\n",
      "Iter: 500, Train loss: 1.85334050655365, Val loss: 2.0040788650512695\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m     valLoss = estimateLoss(model, valData, evalIter, batchSize, sequenceLength)\n\u001b[32m     35\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainLoss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalLoss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m xBatch, yBatch = \u001b[43mgetBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequenceLength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m logits, loss = model(xBatch, yBatch)\n\u001b[32m     38\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mgetBatch\u001b[39m\u001b[34m(data, batchSize, sequenceLength)\u001b[39m\n\u001b[32m      5\u001b[39m x = torch.stack([data[i:i+sequenceLength] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[32m      6\u001b[39m y = torch.stack([data[i+\u001b[32m1\u001b[39m:i+sequenceLength+\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m x, y = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, y.to(device)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimateLoss(model: nn.Module,\n",
    "                 data: torch.Tensor,\n",
    "                 evalIter: int,\n",
    "                 batchSize: int,\n",
    "                 sequenceLength: int) -> float:\n",
    "    model.eval()\n",
    "    losses = torch.zeros(evalIter)\n",
    "    for i in range(evalIter):\n",
    "        xBatch, yBatch = getBatch(data, batchSize, sequenceLength)\n",
    "        logits, loss = model(xBatch, yBatch)\n",
    "        losses[i] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean().item()\n",
    "\n",
    "\n",
    "# Load trained model\n",
    "# model.load_state_dict(torch.load((Path(\"models\") / dataset).with_suffix(\".pt\"), weights_only=False))\n",
    "\n",
    "# Train model from scratch\n",
    "batchSize = 64\n",
    "learningRate = 3e-4\n",
    "maxIter = 800\n",
    "evalInterval = 100\n",
    "evalIter = 100\n",
    "optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
    "for iter in range(maxIter + 1):\n",
    "    if iter % evalInterval == 0 or iter == maxIter - 1:\n",
    "        trainLoss = estimateLoss(model,\n",
    "                                 trainData,\n",
    "                                 evalIter,\n",
    "                                 batchSize,\n",
    "                                 sequenceLength)\n",
    "        valLoss = estimateLoss(model, valData, evalIter, batchSize, sequenceLength)\n",
    "        print(f\"Iter: {iter}, Train loss: {trainLoss}, Val loss: {valLoss}\")\n",
    "    xBatch, yBatch = getBatch(trainData, batchSize, sequenceLength)\n",
    "    logits, loss = model(xBatch, yBatch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "torch.save(model.state_dict(), (Path(\"models\") / dataset).with_suffix(\".pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 139674,
     "status": "ok",
     "timestamp": 1749659712958,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "sFq7hte51_7n",
    "outputId": "e0153772-10f7-4f6c-b46e-a417535af7e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los hermanos sean unidos.\n",
      "\n",
      "Los astimos las juridos\n",
      "que enel roron sustumbures;\n",
      "y en caminias en otan risa,\n",
      "debe aqueles pampas mi foro,\n",
      "cuando, da el encaque en supal\n",
      "eran de boliadas lo sol.\n",
      "\n",
      "Cuanto el desapulto\n",
      "nos cretestaban las campas\n",
      "viendo a al indio y levanto;\n",
      "pero a un hombre tenía\n",
      "como al tenerro campañó\n",
      "si lo puse lo ec\"\n",
      "levaran letó el trastoún yito.\n",
      "yo ya me hale alma suelo viles\n",
      "con las los yos, copletos\n",
      "y sin perros y hablaridos\n",
      "y indao las infiecias.\n",
      "\n",
      "Con laste cuitó con las tílas\n",
      "habías é rastó los de día\n",
      "émpre una: venos ¡Para!\n",
      "que el trabios que había\n",
      "con el arrogallé seguillo!\n",
      "Y Na tía estaba me rancal\n",
      "era ganaba las canas arganas\n",
      "de un punto cantón\n",
      "es la vistas sen traillan\n",
      "de uno que al nochina\n",
      "en al indio a entre asiona.\n",
      "\n",
      "Una prece esa comentenda\n",
      "como la enperranzan\n",
      "pero pan en dar estrasquiandas\n",
      "pue.Ña..\n",
      "no hizos tras en las pare¿\n",
      "Y algunas de hombre al áen potro;\n",
      "me he dicé que afligarse el entreo\n",
      "en medio del cancelanto;\n",
      "\n",
      "que he dé por cuitabullao\n",
      "en monitaré estal y tragoVes\n",
      "de para alivia se alimina.\n",
      "\n",
      "A mús yo mi encono tras los salvan,\n",
      "porque con las cantunas.\n",
      "En nada ce andaba\n",
      "preguidando canto el vanto.\n",
      "\n",
      "YA perro el boligo en selito\n",
      "los, altancias al en repuso;\n",
      "por el indirillos és ayutos\n",
      "se dejar un canto el Figo\n",
      "y tenía como queda el malde.\n",
      "\n",
      "Y contido como gatos y copas\n",
      " tiempo en otoyos mentos;\n",
      "mise aprece los ños bichos,\n",
      "debos los toy atras al espleta:\n",
      "aunque era en defecida,\n",
      "se con hace noche didespués\n",
      "a precer el desierto\n",
      "que el pastaba en rarmarir.\n",
      "\n",
      "En los desieros, son fieras\n",
      "que yo alganas y soy lo asombre;\n",
      "los, que nunos ponían faligían,\n",
      "diciéndoléas candosé\n",
      "¡A Para cara pasarguada.\n",
      "\n",
      "Una quera la taganta\n",
      "cuando esos de eló milones,\n",
      "éste que eran resolias,\n",
      "otro los dejaos que los bedos;\n",
      "los inos tiranone, tal cuero\n",
      "loras reclumentas, en los os\n",
      "nunca, valeces con el sol.\n",
      "\n",
      "Y Dési una justicia\n",
      "poda lo andue da hacer;\n",
      "y canto en tierran la carrda\n",
      "las entl seguida de amigo,\n",
      "pero yeguante col asunto\n",
      "el desilo y entre muy grilo;\n",
      "pues cuando no hervarion\n",
      "que eran a la eniedariao.\n",
      "\n",
      "Nos al tormentor, a mancipal\n",
      "en la que me atemaban,\n",
      "eran unas terrastas atanes;\n",
      "nos ceaneces alián los carcanzón;\n",
      "los sangunoán más duros,\n",
      "sin nociciban su degoles.\n",
      "con las cosos calés en velas\n",
      "de los pobres nas hacen las.\n",
      "\n",
      "Nos hizos los que tenían alivian\n",
      "las pin-las atretando el hombre\n",
      "de que la parrene porreje el boto;\n",
      "de no esotra suerto\n",
      "con lueña de allíUdar.\n",
      "\n",
      "Supre a pera decarmir\n",
      "si a robolar puelante;\n",
      "la mententé al al fin lo atrove,\n",
      "uno tomo que le dio aclamante\n",
      "y por tempo fin el cordone\".\n",
      "\n",
      "\"UN pegua no un buche\n",
      "no ranca al por calque vieja;\n",
      "quue ande primera el campañal\n",
      "y su serto la condelate\n",
      "irba ligra la fiel causa.\n",
      "\n",
      "Viendo al verla, él l cantrer\n",
      "ylle al lanza de da la manos,\n",
      "como a cantó se levo\n",
      "a cadan la de guarte.\n",
      "\n",
      "¡Aí él qué un pobre duñao\n",
      "no conto razón un pajera\n",
      "sin ideja, como eran el asisona;\n",
      "le dan jamás de güelos!\n",
      "Con los pellamón\n",
      "son por ellas prudos misocas;\n",
      "-\"Vamos me hombres sodos,\n",
      "aunque el empezo y sobrar,\n",
      "en los hab) elegante decesiar\n",
      "como la cotra espelóan.\n",
      "\n",
      "Allí entan las de estonces,\n",
      "debernocé como bía,\n",
      "có ampurase hizo estaba,\n",
      "sin pretido que y en el donde\n",
      "entre que en las colbas\n",
      "vino si averar allí\n",
      "y al mí las destilas\n",
      "al vejarlos estaban pella.\n",
      "\n",
      "El mubre cualquilló\n",
      "verle que habe val allí\n",
      "él mucento que pa abugllo;\n",
      "en tanto punto por estrumento\n",
      "para a el frontes de Dios,\n",
      "hace todos como soy.\n",
      "\n",
      "Sin medio con las costinas\n",
      "mejoran que el zolvo lones\n",
      "donde un domento mango,\n",
      "como el boldeo y afla\n",
      "y diciendo que esta el brance\n",
      "de aquella la que con las cación.\n",
      "\n",
      "Mas veces se y madrendían\n",
      "los fren a tirastalicial\n",
      "porque abría: si a noche condiedo;\n",
      "ya no esoportro nos metíos\n",
      "a junta para de malateron.\n",
      "\n",
      "Una vez de sus miceros\n",
      "con De que contes que el oyo;\n",
      "pues que los campocos del noche\n",
      "sin ampecen no su padeces;\n",
      "ninguno al vez a arrimarse,\n",
      "ninguno, junta lo camientica\n",
      "que es fe buesca el gaucho.\n",
      "\n",
      "Canta me los bolas atelidos\n",
      "cuando agua muce maneja;\n",
      "hace y le entenconaba cada\n",
      "eran la cada de maneja;\n",
      "Iesto al compasión\n",
      "arle prudaba a una cielo\n",
      "y allíba barrimdado entito.\n",
      "\n",
      "Por con el cabucho muy vor\n",
      "de hay estaluzación deogado;\n",
      "y pero aguante, mi bustando\n",
      "y se levó al estaba de modo;\n",
      "y de él se quejaba el suelo\n",
      "ansí, los años sol vientes\n",
      "pero que las cualas benas.\n",
      "\n",
      "Es servan los candiciones\n",
      "pido que les hablaban compiendo:\n",
      "aflocando hasta el gaucho\n",
      "de gascas los conueles\n",
      "de nos han ya inviadores.\n",
      "\n",
      "Ya vente el tolmos venidos\n",
      "con las peneneciando metían;\n",
      "nacía otros han indiaos negraban\n",
      "entrese son aniarsas sinaos,\n",
      "de las, campas las comosenes\n",
      "algunos con santar.\n",
      "\n",
      "CIVII\n",
      "\n",
      "¡No si empezo el opellicón\n",
      "una despué a boca\n",
      "algunar de una vejan;\n",
      "vengan las prendabando\n",
      "más compasioa lorardan\n",
      "con otros se de hechinar;\n",
      "que es esa boliar en alca,\n",
      "cuando echaran algíando.\n",
      "\n",
      "Me puso que no has....tado hacerá\n",
      "sino más verán viviso,\n",
      "pues sin también Ar fronto,\n",
      "como ampañó a quel fortón\n",
      "que ni que podía muy ejo.\n",
      "\n",
      "Si erazón fin, ya te viejo\n",
      "debó con los que una taba,\n",
      "habe silar sobrán todas\n",
      "porque me piso y me listicoa.\n",
      "\n",
      "Dentdé nos fueros él\n",
      "debente aquel rovenarse;\n",
      "no hecho que vagan de todos,\n",
      "me tuvo un vive romón,\n",
      "no porque lo abrazao ninunicia\n",
      "sabe lo ante bondito;\n",
      "que lo caya que lo hizo\n",
      "y sin de modito bres,\n",
      "pero allí de pastelido.\n",
      "\n",
      "Desprece atroveta los incial\n",
      "ninguen que bajidarme alta;\n",
      "viendo dolegé dande\n",
      "de anda prece suelo\n",
      "debe aquel bolde\n",
      "lo indio suque revento.\n",
      "\n",
      "Y si alguero el lano\n",
      "me hallé en su pelón;\n",
      "éste como aguandaron\n",
      "inferon saciaba alas tanejas\n",
      "y cada el verto no en un vijo\n",
      "que como tener los petos.\n",
      "\n",
      "Llevó más sique ese un castilo\n",
      "tengó el porrón mal ladicia,\n",
      "envo si alvirlo alcanza\n",
      "con los mal carando;\n",
      "al ver gran úman tinoS\n",
      "y se largó el mal tí.\n",
      "\n",
      "El cuanto me puntón de guitas\n",
      "había agrañao a capazado.\n",
      "¡Ah, saliaba cansao\n",
      "cuando mi jandaba a ganaña!\n",
      "\n",
      "Güeviendo abando pasado\n",
      "esuban hacía al trpelle\n",
      "se despretao mi ovejará!\n",
      "¡Qué iba prires medio be\n",
      "que pasar esosa crubijar\n",
      "y los reclavos de larguéía\n",
      "y que las intesías cagiaos!\n",
      "\n",
      "Salvó el anocence adal;\n",
      "el mande todo descogó;\n",
      "las estrumento los infinios\n",
      "como vinos abareron\n",
      "con jamás y largadas.\n",
      "\n",
      "Ya esas regunas les ventaren\n",
      "apuaba mi presos de ros\n",
      "había en mi casión añana,\n",
      "para como a mi mi viejo\n",
      "que andaban a todiciendo\n",
      "pero y albecabó candiñado.\n",
      "\n",
      "Todo cuanto al plantera\n",
      "al fletamo un saco cajo;\n",
      "era como el punllo,\n",
      "pio lo dos mis deligo,\n",
      "yo con esa compo los lomes,\n",
      "y un hombigao uno enguno.\n",
      "\n",
      "Pero puede en un tempanzo\n",
      "de dijo aberance,\n",
      "la indiabra cante mi forteza\n",
      "me alcarra a el contielo\n",
      "con las ciendo, como que los tienes\n",
      "sin on tiras listas.\n",
      "\n",
      "N pues me ingunos\n",
      "con miedios en un vanido;\n",
      "¡los pobre que es inperro\n",
      "con los espelas y males;\n",
      "en muy fin como gen dejan\n",
      "había de que vivir trabajos.\n",
      "\n",
      "Y presentía a parende,\n",
      "y con los repanzaban;\n",
      "las vaces en hices curiados;\n",
      "debe en los cuando, asilantes,\n",
      "con el erancidpo aquél caballlo,\n",
      "al fán juntas son en cuento.\n",
      "\n",
      "En esa medas de cantos,\n",
      "que hubieos se indios son sintos,\n",
      "viene, esan unosos, mesesiabes\n",
      "duchas se......... sopercas y sinte.\n",
      "pero un infia el gaucho.\n",
      "con como el bropico\n",
      "con el salen de anincia.\n",
      "\n",
      "Dempañao a la cosa enco\n",
      "cuando después vin ¡Qué tempo en un pesó;\n",
      "no  si gusta has caballe\n",
      "a toras la la ovente:\n",
      "¡qué lo cA iba la vación\n",
      "para las netas! ventieras!\n",
      "\n",
      "Y diciéndose a tan tambre\n",
      "como el fiero cazón tanto,\n",
      "le arribé camillo canchón\n",
      "le daba al recavil.\n",
      "\n",
      "Aunque?Rí empre asigura\n",
      "sin las potros de siguiereso\n",
      "se tiempo el lao bolvaje;\n",
      "la manácia jamás dugo\n",
      "al verándoliga a heciendá\n",
      "entrrase como dos muyos\n",
      "y al señorido algaban cante.\n",
      "truta que esto habían agualidos\n",
      "suele colver las que venían\n",
      "allí amos veraos ballas\n",
      "rese la indúración\n",
      "\n",
      "viviéndomose como anija,\n",
      "que de aquella había de tener,\n",
      "como canto noche iba de cierto,\n",
      "sólo que eran el lao dondo\n",
      "ya cayó las juentes que intras.\n",
      "\n",
      "Nos peos deben a querate\n",
      "unandas a gollenas,\n",
      "ni alza salvente solían\n",
      "con otró lonos presonos\n",
      "que al vecí sen descuelos\n",
      "que nos tirances los pantos\n",
      "el que me inono contido.\n",
      "\n",
      "Mas puedeces\n",
      "volvieron saliciandos;\n",
      "con camaré apicarse trancia,\n",
      "al indio se hombre a migo,\n",
      "sin prence los dolos son;\n",
      "los.......... pero no anten pelas\n",
      "a las pareces res les,\n",
      "pero no hay rimente.\n",
      "\n",
      "Yo pues, como esoseraciones\n",
      "que a hervir esos cabellas\n",
      "y, me hizo año y con los pobles\n",
      "y diré afleja el manejo:\n",
      "y es ropuedo tigo con esante\n",
      "y el tiempo que al proversete\n",
      "él me que dar lo quejue.\n",
      "\n",
      "Lo el que taba con perece\n",
      "hicierba de cariades y viente;\n",
      "esto rocel que que iba\n",
      "no han esperarlo amao,\n",
      "la printao, pa avertarlo,\n",
      "sustró la las lanzas y que las\n",
      "más salvararon atellas.\n",
      "\n",
      "Deburanque las tenías,\n",
      "milentes, como el pazonendando\n",
      "hasta otros largo no golies;\n",
      "les gimpos ason penanzan\n",
      "que dino afliciones\n",
      "como unas canzas un paja.\n",
      "\n",
      "Porque andaba la lanca\n",
      "al perronado el lamano entonao;\n",
      "una que lenguenta en en undanto\n",
      "tempatado cetendidos minos,\n",
      "y si andaremos de mi ijoda\n",
      "aunque eran daque frirme.\n",
      "\n",
      "Prive todo de espereteras\n",
      "aunque las sueles mesetu)\n",
      "que allí de apadronel,\n",
      "lo mesmo fuilicio a fronivio\n",
      "a la pleta al cosilción.\n",
      "\n",
      "La rede qué cualquillo\n",
      "es un mucido tiene desesperau;\n",
      "el casino que era rebeda,\n",
      "cói se esaliar despieza\n",
      "lo dejarlo mesmaba a alcarda;\n",
      "y escapo el cualquillo serto\n",
      "lo que es tenía como brinca.\n",
      "\n",
      "¡Y se tran de aquellas,\n",
      "le que andandaba me avecer,\n",
      "y neguientréa el trancador\n",
      "y primero canpeado venir\n",
      "más levaramos y gJenían\n",
      "mientraslé a bruilla.\n",
      "\n",
      "Es Cadará no la guada\n",
      "él esperance él trace;\n",
      "le dije gardó, cada el edogo\n",
      "él que es corde esponaje;\n",
      "¡l tiempo no que el tino\n",
      "la donde que de corazón.\n",
      "\n",
      "Dices su no señor\n",
      "lo darán aquella de Dueña;\n",
      "a todo te esolinao\n",
      "la perda este jue a orromenido,\n",
      "a fáigando legra en la emplan dicendo;\n",
      "entre de da maneja,\n",
      "se podíano, yo, gembien allí\n",
      "bando allí de cuinaldes.\n",
      "\n",
      "Mas cora salía los das\n",
      "muce gañaao a paracencia,\n",
      "con las fiesdes y de cayón;\n",
      "allí ya ya tadula\n",
      "a ví de los deciades\n",
      "\"Artía las mujeras,\n",
      "\"Recán.\"\n",
      "le qué perendica\n",
      "\"pues naides cañida\n",
      "\"te una manda cepacerda:\n",
      "\"¿Ahe, perro el pelianzao\n",
      "le dio le que se aguarda\n",
      "\"le dejaré a pedre\n",
      "\"cuasa que él suy escupo.\"\n",
      "\n",
      "\"A mí me vara él la soba\n",
      "aunque dejarse algua\n"
     ]
    }
   ],
   "source": [
    "context = \"Los hermanos sean unidos\" # \"To be, or not to be\"\n",
    "context = torch.tensor(encode(context), dtype=torch.long).unsqueeze(0).to(device)\n",
    "generatedText = decode(model.generate(context, nTokens=10000)[0].tolist())\n",
    "print(generatedText)\n",
    "with open((Path(\"outputs\") / dataset).with_suffix(\".txt\"), \"w\") as f:\n",
    "    f.write(generatedText)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "18P5ELoTJruhjlZCO6gr8afIF5VeWWKT9",
     "timestamp": 1749610753907
    }
   ]
  },
  "kernelspec": {
   "display_name": "transformer-notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
