{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transformer Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoder-Decoder Architecture**\n",
    "\n",
    "It is used for **sequence-to-sequence** tasks like machine translation.\n",
    "\n",
    "This is the architecture proposed by the original transformer paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. in 2017.\n",
    "\n",
    "<img src=\"assets/encoder_decoder.png\" alt=\"Encoder-Decoder Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decoder-Only Architecture**\n",
    "\n",
    "It is used for **sequence generation** tasks like language modeling.\n",
    "\n",
    "This simpler architecture will be implemented in this notebook from scratch.\n",
    "\n",
    "<img src=\"assets/decoder_only.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scaled Dot-Product Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $Q \\in \\mathbb{R}^{m \\times d_k}$, $K \\in \\mathbb{R}^{n \\times d_k}$ y $V \\in \\mathbb{R}^{n \\times d_v}$ be the query, key and value matrices, respectively. The scaled dot-product attention is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\in \\mathbb{R}^{m \\times d_v}\n",
    "$$\n",
    "\n",
    "<img src=\"assets/self_attention.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddingSize: int, headSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.query = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.value = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(blockSize, blockSize)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        weights = q @ k.transpose(-2, -1) / (C ** 0.5)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        output = weights @ v\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $h$ be the number of heads. The multi-head attention is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\in \\mathbb{R}^{m \\times d_v}\n",
    "$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$ and $W_i^Q \\in \\mathbb{R}^{d_k \\times d_k}$, $W_i^K \\in \\mathbb{R}^{d_k \\times d_k}$, $W_i^V \\in \\mathbb{R}^{d_v \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_v}$ are the learnable parameters.\n",
    "\n",
    "<img src=\"assets/multi_head_attention.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, nHeads: int, embeddingSize: int, headSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([ScaledDotProductAttention(embeddingSize, headSize, blockSize, dropout) for _ in range(nHeads)])\n",
    "        self.projection = nn.Linear(embeddingSize, embeddingSize)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        output = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        output = self.projection(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Position-wise Feed-Forward Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position-wise feed-forward networks are defined as:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "where $W_1 \\in \\mathbb{R}^{d_{ff} \\times d_{model}}$, $b_1 \\in \\mathbb{R}^{d_{ff}}$, $W_2 \\in \\mathbb{R}^{d_{model} \\times d_{ff}}$ and $b_2 \\in \\mathbb{R}^{d_{model}}$ are the learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddingSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embeddingSize, 4 * embeddingSize),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embeddingSize, embeddingSize),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Layer Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layer normalization is defined as:\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sigma} + \\beta\n",
    "$$\n",
    "\n",
    "where $\\gamma \\in \\mathbb{R}^{d_{model}}$ and $\\beta \\in \\mathbb{R}^{d_{model}}$ are the learnable parameters, and $\\mu$ and $\\sigma$ are the mean and standard deviation of $x$, respectively.\n",
    "\n",
    "In the original paper, the layer normalization is applied after the residual connections, whereas in more modern implementations, it is applied before in what is known as **pre-norm formulation**.\n",
    "\n",
    "```python\n",
    "# Original paper\n",
    "x = LayerNorm(x + MultiHeadAttention(x))\n",
    "x = LayerNorm(x + FeedForward(x))\n",
    "\n",
    "# Pre-norm formulation\n",
    "x = x + MultiHeadAttention(LayerNorm(x))\n",
    "x = x + FeedForward(LayerNorm(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "\n",
    "    def __init__(self, nHeads: int, embeddingSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        headSize = embeddingSize // nHeads\n",
    "        self.multiHeadAttention = MultiHeadAttention(nHeads, embeddingSize, headSize, blockSize, dropout)\n",
    "        self.feedForward = FeedForward(embeddingSize, dropout)\n",
    "        self.layerNorm1 = nn.LayerNorm(embeddingSize)\n",
    "        self.layerNorm2 = nn.LayerNorm(embeddingSize)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.multiHeadAttention(self.layerNorm1(x))\n",
    "        x = x + self.feedForward(self.layerNorm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__ (self, vocabularySize: int, nLayers: int, nHeads: int, embeddingSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenEmbeddingTable = nn.Embedding(vocabularySize, embeddingSize)\n",
    "        self.positionEmbeddingTable = nn.Embedding(blockSize, embeddingSize)\n",
    "        self.layers = nn.Sequential(*[Layer(nHeads, embeddingSize, blockSize, dropout) for _ in range(nLayers)])\n",
    "        self.layerNorm = nn.LayerNorm(embeddingSize)\n",
    "        self.linearModelHead = nn.Linear(embeddingSize, vocabularySize)\n",
    "        self.blockSize = blockSize\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor, targets: torch.Tensor = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        B, T = idx.shape\n",
    "        tokenEmbeddings = self.tokenEmbeddingTable(idx)\n",
    "        positionEmbeddings = self.positionEmbeddingTable(torch.arange(T, device=device))\n",
    "        x = tokenEmbeddings + positionEmbeddings\n",
    "        x = self.layers(x)\n",
    "        x = self.layerNorm(x)\n",
    "        logits = self.linearModelHead(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.Tensor, nTokens: int) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(nTokens):\n",
    "                logits, loss = self(idx[:, -self.blockSize:])\n",
    "                logits = logits[:, -1, :]\n",
    "                probabilities = F.softmax(logits, dim=-1)\n",
    "                nextIdx = torch.multinomial(probabilities, num_samples=1)\n",
    "                idx = torch.cat([idx, nextIdx], dim=1)\n",
    "        self.train()\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDir = \"data\"\n",
    "file = os.path.join(dataDir, \"shakespeare.txt\")\n",
    "with open(file, \"r\") as f:\n",
    "    text = f.read()\n",
    "print(f\"Text length: {len(text)}\")\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sorted(set(text))\n",
    "vocabularySize = len(vocabulary)\n",
    "print(f\"Vocabulary size: {vocabularySize}\")\n",
    "print(\"\".join(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 53, 1, 40, 43, 1, 53, 56, 1, 52, 53, 58, 1, 58, 53, 1, 40, 43]\n",
      "To be or not to be\n"
     ]
    }
   ],
   "source": [
    "stringToToken = {ch: i for i, ch in enumerate(vocabulary)}\n",
    "tokenToString = {i: ch for i, ch in enumerate(vocabulary)}\n",
    "encode = lambda string: [stringToToken[char] for char in string]\n",
    "decode = lambda tokens: \"\".join([tokenToString[token] for token in tokens])\n",
    "print(encode(\"To be or not to be\"))\n",
    "print(decode(encode(\"To be or not to be\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([1115394])\n",
      "Data type: torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Data type: {data.dtype}\")\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([1003854])\n",
      "Validation data shape: torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "trainValSplit = 0.9\n",
    "trainSize = int(len(data) * trainValSplit)\n",
    "trainData = data[:trainSize]\n",
    "valData = data[trainSize:]\n",
    "print(f\"Train data shape: {trainData.shape}\")\n",
    "print(f\"Validation data shape: {valData.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context batch shape: torch.Size([4, 8])\n",
      "tensor([[57,  1, 46, 47, 57,  1, 50, 53],\n",
      "        [ 1, 58, 46, 43, 56, 43,  1, 41],\n",
      "        [17, 26, 15, 17, 10,  0, 32, 53],\n",
      "        [57, 58,  6,  1, 61, 47, 58, 46]], device='cuda:0')\n",
      "Target batch shape: torch.Size([4, 8])\n",
      "tensor([[ 1, 46, 47, 57,  1, 50, 53, 60],\n",
      "        [58, 46, 43, 56, 43,  1, 41, 39],\n",
      "        [26, 15, 17, 10,  0, 32, 53,  1],\n",
      "        [58,  6,  1, 61, 47, 58, 46,  0]], device='cuda:0')\n",
      "Context: [57] -> Target: 1\n",
      "Context: [57, 1] -> Target: 46\n",
      "Context: [57, 1, 46] -> Target: 47\n",
      "Context: [57, 1, 46, 47] -> Target: 57\n",
      "Context: [57, 1, 46, 47, 57] -> Target: 1\n",
      "Context: [57, 1, 46, 47, 57, 1] -> Target: 50\n",
      "Context: [57, 1, 46, 47, 57, 1, 50] -> Target: 53\n",
      "Context: [57, 1, 46, 47, 57, 1, 50, 53] -> Target: 60\n",
      "Context: [1] -> Target: 58\n",
      "Context: [1, 58] -> Target: 46\n",
      "Context: [1, 58, 46] -> Target: 43\n",
      "Context: [1, 58, 46, 43] -> Target: 56\n",
      "Context: [1, 58, 46, 43, 56] -> Target: 43\n",
      "Context: [1, 58, 46, 43, 56, 43] -> Target: 1\n",
      "Context: [1, 58, 46, 43, 56, 43, 1] -> Target: 41\n",
      "Context: [1, 58, 46, 43, 56, 43, 1, 41] -> Target: 39\n",
      "Context: [17] -> Target: 26\n",
      "Context: [17, 26] -> Target: 15\n",
      "Context: [17, 26, 15] -> Target: 17\n",
      "Context: [17, 26, 15, 17] -> Target: 10\n",
      "Context: [17, 26, 15, 17, 10] -> Target: 0\n",
      "Context: [17, 26, 15, 17, 10, 0] -> Target: 32\n",
      "Context: [17, 26, 15, 17, 10, 0, 32] -> Target: 53\n",
      "Context: [17, 26, 15, 17, 10, 0, 32, 53] -> Target: 1\n",
      "Context: [57] -> Target: 58\n",
      "Context: [57, 58] -> Target: 6\n",
      "Context: [57, 58, 6] -> Target: 1\n",
      "Context: [57, 58, 6, 1] -> Target: 61\n",
      "Context: [57, 58, 6, 1, 61] -> Target: 47\n",
      "Context: [57, 58, 6, 1, 61, 47] -> Target: 58\n",
      "Context: [57, 58, 6, 1, 61, 47, 58] -> Target: 46\n",
      "Context: [57, 58, 6, 1, 61, 47, 58, 46] -> Target: 0\n"
     ]
    }
   ],
   "source": [
    "def getBatch(data: torch.Tensor,\n",
    "             batchSize: int,\n",
    "             blockSize: int\n",
    "             ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    ix = torch.randint(0, data.size(0) - blockSize, (batchSize,))\n",
    "    x = torch.stack([data[i:i+blockSize] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+blockSize+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "batchSize = 4 # Sequences in parallel\n",
    "blockSize = 8 # Sequence length\n",
    "\n",
    "xBatch, yBatch = getBatch(trainData, batchSize, blockSize)\n",
    "\n",
    "print(f\"Context batch shape: {xBatch.shape}\")\n",
    "print(xBatch)\n",
    "print(\"Target batch shape:\", yBatch.shape)\n",
    "print(yBatch)\n",
    "for b in range(batchSize):\n",
    "    for t in range(blockSize):\n",
    "        context = xBatch[b, :t+1].tolist()\n",
    "        target = yBatch[b, t].item()\n",
    "        print(f\"Context: {context} -> Target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.79M parameters\n"
     ]
    }
   ],
   "source": [
    "blockSize = 256\n",
    "embeddingSize = 384\n",
    "nHeads = 6\n",
    "nLayers = 6\n",
    "dropout = 0.2\n",
    "\n",
    "model = GPTLanguageModel(vocabularySize, nLayers, nHeads, embeddingSize, blockSize, dropout).to(device)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Train loss: 4.22703742980957, Val loss: 4.225285053253174\n",
      "Iter: 500, Train loss: 1.8503810167312622, Val loss: 1.9786573648452759\n",
      "Iter: 1000, Train loss: 1.3868194818496704, Val loss: 1.6092053651809692\n",
      "Iter: 1500, Train loss: 1.2401565313339233, Val loss: 1.523661494255066\n",
      "Iter: 1999, Train loss: 1.1415157318115234, Val loss: 1.4956190586090088\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimateLoss(model: nn.Module,\n",
    "                 data: torch.Tensor,\n",
    "                 evalIter: int,\n",
    "                 batchSize: int,\n",
    "                 blockSize: int\n",
    "                ) -> float:\n",
    "    model.eval()\n",
    "    losses = torch.zeros(evalIter)\n",
    "    for i in range(evalIter):\n",
    "        xBatch, yBatch = getBatch(data, batchSize, blockSize)\n",
    "        logits, loss = model(xBatch, yBatch)\n",
    "        losses[i] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean().item()\n",
    "\n",
    "\n",
    "batchSize = 64\n",
    "learningRate = 3e-4\n",
    "maxIter = 2000\n",
    "evalInterval = 500\n",
    "evalIter = 200\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "for iter in range(maxIter):\n",
    "    if iter % evalInterval == 0 or iter == maxIter - 1:\n",
    "        trainLoss = estimateLoss(model, trainData, evalIter, batchSize, blockSize)\n",
    "        valLoss = estimateLoss(model, valData, evalIter, batchSize, blockSize)\n",
    "        print(f\"Iter: {iter}, Train loss: {trainLoss}, Val loss: {valLoss}\")\n",
    "    xBatch, yBatch = getBatch(trainData, batchSize, blockSize)\n",
    "    logits, loss = model(xBatch, yBatch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "modelsDir = \"models\"\n",
    "torch.save(model.state_dict(), os.path.join(modelsDir, \"shakespeare.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be bustled?\n",
      "'Go, get you we counto the service, bear himself,\n",
      "Would not do speak from your conclument to me?\n",
      "\n",
      "WARWICK:\n",
      "My good lord, O God is dead; look up ourselves.\n",
      "As my heart is this doubtal king--\n",
      "Who took owe the office to my crown?\n",
      "If I he no best import in my country:\n",
      "The foults of time of thy eyes shall fits thyself.\n",
      "Please your ladyship about your found?\n",
      "Upon the peace, call the weakeful of sight;\n",
      "Our dear 'twere dishonours: therefore, indeed\n",
      "A primosible blow'd blocked friends,\n",
      "Went rather no viney,' the heavens, kneel'd teeth\n",
      "Of the thousand being valiable blood. Thought mervest\n",
      "Hath been more than the league of thy head;\n",
      "For 't look more be ready 'baried,\n",
      "Where time from feast, and thou hast post the place?\n",
      "No, keep him again; so he found, dares!\n",
      "Hadst thou backnot thee? have pierce.\n",
      "\n",
      "Shepherd:\n",
      "Thou'rt thou the earth,\n",
      "And think of babals thy bones:\n",
      "I'll keep to bize me secur on my back to me.\n",
      "And what strike the babe to the death of their\n",
      "are fetch.\n",
      "\n",
      "Shepherd:\n",
      "Sir, I will like my fortune to confound thy strut for\n",
      "for my son friends: God hear prince, Henry, and still thou let for\n",
      "them to the son. Even that same cursed thou bid me:\n",
      "why, thou dost not like me, in that crack'st is stay,\n",
      "and more of thine air all appeace.\n",
      "\n",
      "Second Senator:\n",
      "Are you here?\n",
      "\n",
      "First Servant:\n",
      "Still your royal joy?\n",
      "\n",
      "CORIOLANUS:\n",
      "It is for them weldess,\n",
      "And well amost all trouble; if thou shouldst plead\n",
      "His wife, being the king enemy, whom I fly.\n",
      "\n",
      "Second Watchman:\n",
      "Dropp'd him arrest!\n",
      "\n",
      "Second Roman:\n",
      "Mere he that is should be kill up him:\n",
      "He fill me but a charge flock. He came\n",
      "Come me to her a cause of princes and said;\n",
      "And how do God's hearth! Ans lament of him:\n",
      "God and Nire; and let us say me this morning.\n",
      "Why must such away we are part gentlemen;\n",
      "What says't to place me, and bear him to serve\n",
      "At home awhile thou art encounter the throates,\n",
      "Which then thou art compe to the pleasur.\n",
      "\n",
      "SICINIUS:\n",
      "Refuse it:\n",
      "\n",
      "BRUTUS:\n",
      "I deserve not: they do pretest me be like than he\n",
      "Should die me as their twempest their letters,\n",
      "And turn amagination for her murderers:\n",
      "But cannot for the title;\n",
      "It shall die to seffects with you, and them have loved\n",
      "With had borne, so shall suck and oppove\n",
      "As I love as burneth with you.\n",
      "\n",
      "First Lord:\n",
      "Take him of to her leaving her:\n",
      "The tower he hath blushnef by your friends:\n",
      "It she is courteous Angelo,\n",
      "Were thou a dangerous thus bestard and untimeless,\n",
      "That the eagle of love his noble house,\n",
      "Shown mischief she in London of Salisbury,\n",
      "Ere we are, and there we find an amlife waborn\n",
      "Should be lung as with those visitation;\n",
      "And then she, we say the husband; thereof, aMaster\n",
      "Then was Master Stanley leave to say our homains;\n",
      "Make his samme rap curtains of his cub,\n",
      "And idd heart the prisoner of Vaugham?\n",
      "\n",
      "Second Withdraw, when now?\n",
      "\n",
      "CLARENCE:\n",
      "Passage say no more soul's fond kings is\n",
      "\n",
      "STANLEY:\n",
      "By white of Polixenes, though we were heard.\n",
      "\n",
      "HASTINGS:\n",
      "Each way with the orator of thy pace,\n",
      "And much of the fresh of name I laid on thy\n",
      "requirest I will find-fires our oyllips.\n",
      "\n",
      "CARISLE:\n",
      "It is his lady of my lordship's tongue,\n",
      "And Bap olive, as I not should not prove it.\n",
      "\n",
      "KING RICHARD II:\n",
      "An why, marriiage?\n",
      "\n",
      "BUCKINGHAM:\n",
      "An I, I am come to follower: saw thou, my lord;\n",
      "Withal I were thy deep, thou art to her.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Grave me, deserve my poor aid. I have losts day:\n",
      "It is no most way to do him.\n",
      "\n",
      "KING RICHARD III:\n",
      "I tell us it, I hopen thee.\n",
      "\n",
      "KING RICHARD III:\n",
      "Harry, a poor merry than he gall,\n",
      "Until they quarrel him to your love,\n",
      "Marrying to church preparate or man;\n",
      "First him like a lamourning suit on.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Away! why, my good lord and marshows, my wealthsoever's day?\n",
      "\n",
      "RICHARD:\n",
      "I would you hence the night burked at once.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Good virginal: as I therefore,\n",
      "Who look'd not; to that Salisbury\n",
      "Have no usedom; and behold me the Duke of YorksPrinve,\n",
      "And he would not be so much by this absolve,\n",
      "Were now stoodd-and that I repent, as away,\n",
      "A power of wretched counterpey,\n",
      "We fight our hearts, hath bind me anto little;\n",
      "Which toe and entreate in my vewlness flast\n",
      "Unto Bolingbroke. The breather behinds you all,\n",
      "For I from his sovereign's play eyes maw our-son,\n",
      "Is my inherity? how shall he will love-dead.\n",
      "Take me, here ibehors of your sons my mousths,\n",
      "That were you will leave out and himself.\n",
      "Threater, these larks on earth brief me see,\n",
      "Having at the plain. Direct keeps you in him?\n",
      "\n",
      "LORD ROSS:\n",
      "Poor Hastings: good men!\n",
      "\n",
      "CAPULET:\n",
      "\n",
      "MARCIUS:\n",
      "What a crown, with the city for thy window, sir?\n",
      "Take her keeps great me physics and blush on\n",
      "His Mercury.\n",
      "\n",
      "First Servingman:\n",
      "This my lord.\n",
      "\n",
      "ROMEO:\n",
      "Helch Warwick him she can swall. What, my good lords?\n",
      "\n",
      "MERCUTIO:\n",
      "I have a heart-should ever, who you receivent.\n",
      "\n",
      "ESCALUS:\n",
      "He has solen are services to it. What, ne'er not call and amongster in himself\n",
      "have hundred, so some fail on earth of his\n",
      "men of country the thousand many thus.\n",
      "\n",
      "ERSC OF:\n",
      "Fie, for I have gaze to show him an in your world\n",
      "throngs and Glaunt him is virtue, and like the law\n",
      "and that cut our bloody or flowship. But, who is the ocean charge?\n",
      "When bright shall serve the steel? Bringbroke Romeo\n",
      "on the render of the colours of that steel\n",
      "his apparent. Let him should again, as well essee him\n",
      "ready, dispatch, aback you.\n",
      "\n",
      "Shepherd:\n",
      "Ay, in day?\n",
      "\n",
      "AUTOLYCUS:\n",
      "He was his fellow, sir: he call, cut i'; for all the enemy, both.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Keeps flock me for them to our account this mind, but he\n",
      "that six the country's wife as then you known most, and\n",
      "not dischoose as fray and beuty as a lasts, sir: what\n",
      "hour home as you do: he had in his day uncle?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Cure me to know you not, we'll return me.\n",
      "\n",
      "ISABELLA:\n",
      "Go you; for I thank my majesty.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "You have slept me, whom looks you most call you mine.\n",
      "\n",
      "LUCENTIO:\n",
      "Will show you?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "You are loved?\n",
      "\n",
      "ISABELLA:\n",
      "'Tis a hand of me, sir;\n",
      "That I will keep.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Romeo that ever? O manster, and thou deserve,\n",
      "Your welcome into take your assem as you me;\n",
      "Your poor imports your pardon.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Your majesty may proceed upon my tempes back.\n",
      "Marry, Petmpey had the noble Ludhly be;\n",
      "Put up as Romeo's rudy; in that earth, but that have\n",
      "clumsh a retired a man 'thou.' Will't thou mayst swear,\n",
      "'Tis eye-ten life?\n",
      "\n",
      "Provost:\n",
      "Why command is here? what your leaves I find\n",
      "Your she speaks? you are could her married?\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Hence, must I, how came to arm. He well.\n",
      "\n",
      "DUCHESS:\n",
      "O, farewell, my lord.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Stand up upon him, Warwick, that Paul's thou,\n",
      "And dear me no more.\n",
      "\n",
      "DUKE OF YORK:\n",
      "I cannot put it. What say, you should come?\n",
      "\n",
      "EXETER:\n",
      "\n",
      "QUEEN MARGARET:\n",
      "I will tell you what you must do not\n",
      "The jest to be England's country.\n",
      "\n",
      "KING EDWARD IV:\n",
      "\n",
      "AGOT:\n",
      "Ay, a queen, 'tis but as we about wedd.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Oh, my lord! O wife, madam, marry, my noble moon;\n",
      "I must in my sorrow, I love my holy tongue,\n",
      "If I love it, cheerly doth love as die:\n",
      "I am thy aunthought I cannot be coming;\n",
      "Who it both thee leave, and early old fly\n",
      "I never account the seems of knee\n",
      "As I am Aphore with a day in Vienna,\n",
      "Than that all the king of men down speak.\n",
      "O God! why, O, if I will do be Richard!\n",
      "Tell me, he, Juliet, Tybalt, with thee! alack,sh foe!\n",
      "\n",
      "PARIS:\n",
      "Ay, Juliet, hence; let me lie you both the conquerh.\n",
      "\n",
      "GARET:\n",
      "I think not he watch, but if you make your make;\n",
      "But what, she is children or no fie?\n",
      "\n",
      "PERDITA:\n",
      "You many faith, I pray them at his brother\n",
      "Our chargeles than we holy food. That blessides them not\n",
      "Hath anger to divide it: the time cannot be,\n",
      "Not he no be deserced.\n",
      "\n",
      "PAULINA:\n",
      "No, no more.\n",
      "\n",
      "Volsce:\n",
      "Hundred, he-did such going;\n",
      "I cannot see what should hold it ill be so;\n",
      "So resolve do you?\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "In forgot at love, and see how once the queen,\n",
      "In more friendly close of him. What oft all this?\n",
      "I in your assistant the of his love,\n",
      "So much prely as your brother well part\n",
      "Still o'er the common more tended: dust you call it!\n",
      "In were honour against the sea, as was mine one of me?\n",
      "\n",
      "KING HENRY VI:\n",
      "\n",
      "BUSKY:\n",
      "The rest, I am too, thusand decle, and for his anier.\n",
      "\n",
      "BUSHY:\n",
      "Here's the greaters are these do the blood comback.\n",
      "\n",
      "YORK:\n",
      "You may have heard it.\n",
      "\n",
      "RICHARD:\n",
      "He had such is your mother's deaths,\n",
      "She was altered thy appeal blood,\n",
      "Knowledge by God, Tyrrel! she is your hants,\n",
      "And therewixt you, go and Triber your worship?\n",
      "Let us not love be, I cannot see;\n",
      "Your armys accing spiritacle,\n",
      "When many old will-penitencal. But it is a man?\n",
      "Sun your arms\n",
      "That bear me out caloud of our-place; a man on me:\n",
      "'Tis done for me within another beauty's gone,\n",
      "That she's senseless.\n",
      "I have a slander'd fruit-timenced\n",
      "Bale more than which is into, as if\n",
      "Myself we have heard to mound him.\n",
      "\n",
      "First Servant:\n",
      "Which, what neither was? shall I heard it be my hand,\n",
      "I am rash'd, that we made him backly and\n",
      "Doch making, and the helm of his own heart,\n",
      "When weeping was nothing know must I see\n",
      "The life of a busil throar to with that our redeems?\n",
      "My son should do receive again us to eagly.\n",
      "Speak not striumphant, if I unlong them love.\n",
      "Take him, why, thou cartaies me, and makess my truel,\n",
      "Is no maid, in fetch, of Margjuster\n",
      "Is current all war. unholyman flouts,\n",
      "Now you hence makes him from him?\n",
      "\n",
      "SICINIUS:\n",
      "He curied the princely and me to save\n",
      "Our unknown looks or from Rome on the king\n",
      "And the prince on their false twenty heirs.\n",
      "\n",
      "HERMIONE:\n",
      "Why, I talk in him: 'tis a father's hand, to other sea\n",
      "Selfsolve to be that liude I should be a world.\n",
      "\n",
      "CORIOLANUS:\n",
      "But he will of these close.\n",
      "\n",
      "AUTOLYCUS:\n",
      "When Overhorse?\n",
      "\n",
      "Officer:\n",
      "Hadst thou not seem'd me? let me be had. Bid you not hare sister\n",
      "A rag murder to live our bring:\n",
      "I shall be pity mine ear in Rome,\n",
      "To him beg'n our will speeder'd to unsull alour\n",
      "Battle. My men heard an old me at my help.\n",
      "So Please you well good, you found I pray you not:\n",
      "That our given the sun upon your higholy should be,\n",
      "Which I have left to no much fain must respect,\n",
      "I shall steal me him mocker'd him to keep.\n",
      "\n",
      "BUSHY:\n",
      "'Twas she's adversel to him, sir.\n",
      "\n",
      "BRAKENBURY:\n",
      "What, we must not be very drunken--\n",
      "\n",
      "KING RICHARD II:\n",
      "Come hither, my Gloucester, to the Christian Surly,\n",
      "And so thou wouldst be grief to do e\n"
     ]
    }
   ],
   "source": [
    "context = \"To be or not to be\"\n",
    "context = torch.tensor(encode(context), dtype=torch.long).unsqueeze(0).to(device)\n",
    "generatedText = decode(model.generate(context, nTokens=10000)[0].tolist())\n",
    "print(generatedText)\n",
    "outputDir = \"outputs\"\n",
    "with open(os.path.join(outputDir, \"shakespeare.txt\"), \"w\") as f:\n",
    "    f.write(generatedText)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
