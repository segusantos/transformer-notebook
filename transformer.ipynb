{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqS26uut1_7N"
   },
   "source": [
    "# **Transformer Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVeQuGBF1_7T"
   },
   "source": [
    "### **Encoder-Decoder Architecture**\n",
    "\n",
    "It is used for **sequence-to-sequence** tasks like machine translation.\n",
    "\n",
    "This is the architecture proposed by the original transformer paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. in 2017.\n",
    "\n",
    "<img src=\"assets/encoder_decoder.png\" alt=\"Encoder-Decoder Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RE1K3Dex-h6Z"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APfB9jre1_7V"
   },
   "source": [
    "### **Decoder-Only Architecture**\n",
    "\n",
    "It is used for **sequence generation** tasks like language modeling.\n",
    "\n",
    "This simpler architecture will be implemented in this notebook from scratch.\n",
    "\n",
    "<img src=\"assets/decoder_only.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1749658844461,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "41bejM8V1_7W",
    "outputId": "dcab5d71-a00c-47d4-a108-2c5a2ef2c6eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not Path(\"data\").exists():\n",
    "        !git clone https://github.com/segusantos/transformer-notebook.git\n",
    "        %cd transformer-notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BxQZcCR1_7a"
   },
   "source": [
    "## **Scaled Dot-Product Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gY3WuR6c1_7b"
   },
   "source": [
    "Let $Q \\in \\mathbb{R}^{m \\times d_k}$, $K \\in \\mathbb{R}^{n \\times d_k}$ y $V \\in \\mathbb{R}^{n \\times d_v}$ be the query, key and value matrices, respectively. The scaled dot-product attention is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\in \\mathbb{R}^{m \\times d_v}\n",
    "$$\n",
    "\n",
    "<img src=\"assets/self_attention.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "mSfCf-011_7c"
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 sequenceLength: int,\n",
    "                 embeddingSize: int,\n",
    "                 headSize: int,\n",
    "                 dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.query = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.value = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(sequenceLength, sequenceLength)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batchSize, sequenceLength, embeddingSize = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        weights = q @ k.transpose(-2, -1) / (embeddingSize ** 0.5)\n",
    "        weights = weights.masked_fill(self.tril[:sequenceLength, :sequenceLength] == 0, float(\"-inf\"))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        x = weights @ v\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0SNKag41_7d"
   },
   "source": [
    "## **Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erjXpdQe1_7e"
   },
   "source": [
    "Let $h$ be the number of heads. The multi-head attention is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\in \\mathbb{R}^{m \\times d_v}\n",
    "$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$ and $W_i^Q \\in \\mathbb{R}^{d_k \\times d_k}$, $W_i^K \\in \\mathbb{R}^{d_k \\times d_k}$, $W_i^V \\in \\mathbb{R}^{d_v \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_v}$ are the learnable parameters.\n",
    "\n",
    "<img src=\"assets/multi_head_attention.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "DmUUm3Db1_7f"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 nHeads: int,\n",
    "                 sequenceLength: int,\n",
    "                 embeddingSize: int,\n",
    "                 dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([ScaledDotProductAttention(sequenceLength,\n",
    "                                                              embeddingSize,\n",
    "                                                              embeddingSize // nHeads,\n",
    "                                                              dropout) for _ in range(nHeads)])\n",
    "        self.projection = nn.Linear(embeddingSize, embeddingSize)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.projection(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECfdczW81_7g"
   },
   "source": [
    "## **Position-wise Feed-Forward Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1GfRIuN1_7g"
   },
   "source": [
    "The position-wise feed-forward networks are defined as:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "where $W_1 \\in \\mathbb{R}^{d_{ff} \\times d_{model}}$, $b_1 \\in \\mathbb{R}^{d_{ff}}$, $W_2 \\in \\mathbb{R}^{d_{model} \\times d_{ff}}$ and $b_2 \\in \\mathbb{R}^{d_{model}}$ are the learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "qBpO3Eq21_7h"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embeddingSize: int,\n",
    "                 dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.feedForward = nn.Sequential(\n",
    "            nn.Linear(embeddingSize, 4 * embeddingSize),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embeddingSize, embeddingSize),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.feedForward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Sfu0Cvp1_7h"
   },
   "source": [
    "## **Layer Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0LWpqI71_7h"
   },
   "source": [
    "The layer normalization is defined as:\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sigma} + \\beta\n",
    "$$\n",
    "\n",
    "where $\\gamma \\in \\mathbb{R}^{d_{model}}$ and $\\beta \\in \\mathbb{R}^{d_{model}}$ are the learnable parameters, and $\\mu$ and $\\sigma$ are the mean and standard deviation of $x$, respectively.\n",
    "\n",
    "In the original paper, the layer normalization is applied after the residual connections, whereas in more modern implementations, it is applied before in what is known as **pre-norm formulation**.\n",
    "\n",
    "```python\n",
    "# Original paper\n",
    "x = LayerNorm(x + MultiHeadAttention(x))\n",
    "x = LayerNorm(x + FeedForward(x))\n",
    "\n",
    "# Pre-norm formulation\n",
    "x = x + MultiHeadAttention(LayerNorm(x))\n",
    "x = x + FeedForward(LayerNorm(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "lereN3iL1_7i"
   },
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 nHeads: int,\n",
    "                 sequenceLength: int,\n",
    "                 embeddingSize: int,\n",
    "                 dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.multiHeadAttention = MultiHeadAttention(nHeads,\n",
    "                                                     sequenceLength,\n",
    "                                                     embeddingSize,\n",
    "                                                     dropout)\n",
    "        self.feedForward = FeedForward(embeddingSize, dropout)\n",
    "        self.layerNorm1 = nn.LayerNorm(embeddingSize)\n",
    "        self.layerNorm2 = nn.LayerNorm(embeddingSize)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.multiHeadAttention(self.layerNorm1(x))\n",
    "        x = x + self.feedForward(self.layerNorm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ke_AmotY1_7i"
   },
   "source": [
    "## **Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "R-xFgrgj1_7i"
   },
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__ (self,\n",
    "                  vocabularySize: int,\n",
    "                  nLayers: int,\n",
    "                  nHeads: int,\n",
    "                  sequenceLength:int,\n",
    "                  embeddingSize: int,\n",
    "                  dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenEmbeddingTable = nn.Embedding(vocabularySize, embeddingSize)\n",
    "        self.positionEmbeddingTable = nn.Embedding(sequenceLength, embeddingSize)\n",
    "        self.layers = nn.Sequential(*[Layer(nHeads,\n",
    "                                            sequenceLength,\n",
    "                                            embeddingSize,\n",
    "                                            dropout) for _ in range(nLayers)])\n",
    "        self.layerNorm = nn.LayerNorm(embeddingSize)\n",
    "        self.linearModelHead = nn.Linear(embeddingSize, vocabularySize)\n",
    "        self.sequenceLength = sequenceLength\n",
    "        self.apply(self.initWeights)\n",
    "\n",
    "    def initWeights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, targets: torch.Tensor = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        batchSize, sequenceLength = x.shape\n",
    "        tokenEmbeddings = self.tokenEmbeddingTable(x)\n",
    "        positionEmbeddings = self.positionEmbeddingTable(torch.arange(sequenceLength, device=device))\n",
    "        x = tokenEmbeddings + positionEmbeddings\n",
    "        x = self.layers(x)\n",
    "        x = self.layerNorm(x)\n",
    "        logits = self.linearModelHead(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batchSize, sequenceLength, embeddingSize = logits.shape\n",
    "            logits = logits.view(batchSize * sequenceLength, embeddingSize)\n",
    "            targets = targets.view(batchSize * sequenceLength)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x: torch.Tensor, nTokens: int) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(nTokens):\n",
    "                logits, loss = self(x[:, -self.sequenceLength:])\n",
    "                probabilities = F.softmax(logits[:, -1, :], dim=-1)\n",
    "                nextToken = torch.multinomial(probabilities, num_samples=1)\n",
    "                x = torch.cat([x, nextToken], dim=1)\n",
    "        self.train()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExfoF3cx1_7j"
   },
   "source": [
    "## **Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 89,
     "status": "ok",
     "timestamp": 1749658845179,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "oEK1NJv41_7j",
    "outputId": "fbbff5ef-5f06-4f6d-8e48-d2d422cb91d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 187095\n",
      "I\n",
      "\n",
      "Aquí me pongo a cantar\n",
      "al compás de la vigüela,\n",
      "que el hombre que lo desvela\n",
      "una pena estrordinaria,\n",
      "como la ave solitaria\n",
      "con el cantar se consuela.\n",
      "\n",
      "Pido a los santos del cielo\n",
      "que ayuden mi pensamiento:\n",
      "les pido en este momento\n",
      "que voy a cantar\n"
     ]
    }
   ],
   "source": [
    "dataset = \"martin_fierro\" # \"shakespeare\"\n",
    "with open((Path(\"data\") / dataset).with_suffix(\".txt\"), \"r\") as f:\n",
    "    text = f.read()\n",
    "print(f\"Text length: {len(text)}\")\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1749658845186,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "Ut9lqYSO1_7j",
    "outputId": "26b0d998-d4d7-4a51-c277-e08eb32eefcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 72\n",
      "\n",
      " !\"'(),-.:;?ABCDEFGHIJLMNOPQRSTUVXYZabcdefghijklmnopqrstuvxyz¡¿Ñáéíñóúü\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sorted(set(text))\n",
    "vocabularySize = len(vocabulary)\n",
    "print(f\"Vocabulary size: {vocabularySize}\")\n",
    "print(\"\".join(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1749658845192,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "kLvqV9Jr1_7k",
    "outputId": "cdeecf7a-6800-4b26-c7dc-5088984bd483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 51, 55, 1, 44, 41, 54, 49, 37, 50, 51, 55, 1, 55, 41, 37, 50, 1, 57, 50, 45, 40, 51, 55]\n",
      "Los hermanos sean unidos\n"
     ]
    }
   ],
   "source": [
    "stringToToken = {ch: i for i, ch in enumerate(vocabulary)}\n",
    "tokenToString = {i: ch for i, ch in enumerate(vocabulary)}\n",
    "encode = lambda string: [stringToToken[char] for char in string]\n",
    "decode = lambda tokens: \"\".join([tokenToString[token] for token in tokens])\n",
    "print(encode(\"Los hermanos sean unidos\"))\n",
    "print(decode(encode(\"Los hermanos sean unidos\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1749658845198,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "grysOjv-1_7k",
    "outputId": "50ab0090-7977-4e9c-a771-cd20eec6074c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([187095])\n",
      "Data type: torch.int64\n",
      "tensor([21,  0,  0, 13, 53, 57, 67,  1, 49, 41,  1, 52, 51, 50, 43, 51,  1, 37,\n",
      "         1, 39, 37, 50, 56, 37, 54,  0, 37, 48,  1, 39, 51, 49, 52, 65, 55,  1,\n",
      "        40, 41,  1, 48, 37,  1, 58, 45, 43, 71, 41, 48, 37,  7,  0, 53, 57, 41,\n",
      "         1, 41, 48,  1, 44, 51, 49, 38, 54, 41,  1, 53, 57, 41,  1, 48, 51,  1,\n",
      "        40, 41, 55, 58, 41, 48, 37,  0, 57, 50, 37,  1, 52, 41, 50, 37,  1, 41,\n",
      "        55, 56, 54, 51, 54, 40, 45, 50, 37, 54])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Data type: {data.dtype}\")\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1749658845227,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "pZjPFtGo1_7l",
    "outputId": "77fb5cb4-a26e-4607-9478-851e6007e166"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([149676])\n",
      "Validation data shape: torch.Size([37419])\n"
     ]
    }
   ],
   "source": [
    "trainValSplit = 0.8\n",
    "trainSize = int(len(data) * trainValSplit)\n",
    "trainData = data[:trainSize]\n",
    "valData = data[trainSize:]\n",
    "print(f\"Train data shape: {trainData.shape}\")\n",
    "print(f\"Validation data shape: {valData.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1749658845228,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "QWuJtH4g1_7l",
    "outputId": "a86d4714-747f-4d20-bac6-7204340694c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context batch shape: torch.Size([4, 8])\n",
      "tensor([[57, 50, 51,  1, 55, 41,  1, 40],\n",
      "        [17, 55,  1, 40, 41,  1, 37, 48],\n",
      "        [55, 37, 54, 50, 51, 55, 51,  0],\n",
      "        [48, 53, 57, 45, 54, 45, 40, 51]], device='cuda:0')\n",
      "Target batch shape: torch.Size([4, 8])\n",
      "tensor([[50, 51,  1, 55, 41,  1, 40, 37],\n",
      "        [55,  1, 40, 41,  1, 37, 48, 49],\n",
      "        [37, 54, 50, 51, 55, 51,  0, 60],\n",
      "        [53, 57, 45, 54, 45, 40, 51,  9]], device='cuda:0')\n",
      "Context: [57] -> Target: 50\n",
      "Context: [57, 50] -> Target: 51\n",
      "Context: [57, 50, 51] -> Target: 1\n",
      "Context: [57, 50, 51, 1] -> Target: 55\n",
      "Context: [57, 50, 51, 1, 55] -> Target: 41\n",
      "Context: [57, 50, 51, 1, 55, 41] -> Target: 1\n",
      "Context: [57, 50, 51, 1, 55, 41, 1] -> Target: 40\n",
      "Context: [57, 50, 51, 1, 55, 41, 1, 40] -> Target: 37\n",
      "Context: [17] -> Target: 55\n",
      "Context: [17, 55] -> Target: 1\n",
      "Context: [17, 55, 1] -> Target: 40\n",
      "Context: [17, 55, 1, 40] -> Target: 41\n",
      "Context: [17, 55, 1, 40, 41] -> Target: 1\n",
      "Context: [17, 55, 1, 40, 41, 1] -> Target: 37\n",
      "Context: [17, 55, 1, 40, 41, 1, 37] -> Target: 48\n",
      "Context: [17, 55, 1, 40, 41, 1, 37, 48] -> Target: 49\n",
      "Context: [55] -> Target: 37\n",
      "Context: [55, 37] -> Target: 54\n",
      "Context: [55, 37, 54] -> Target: 50\n",
      "Context: [55, 37, 54, 50] -> Target: 51\n",
      "Context: [55, 37, 54, 50, 51] -> Target: 55\n",
      "Context: [55, 37, 54, 50, 51, 55] -> Target: 51\n",
      "Context: [55, 37, 54, 50, 51, 55, 51] -> Target: 0\n",
      "Context: [55, 37, 54, 50, 51, 55, 51, 0] -> Target: 60\n",
      "Context: [48] -> Target: 53\n",
      "Context: [48, 53] -> Target: 57\n",
      "Context: [48, 53, 57] -> Target: 45\n",
      "Context: [48, 53, 57, 45] -> Target: 54\n",
      "Context: [48, 53, 57, 45, 54] -> Target: 45\n",
      "Context: [48, 53, 57, 45, 54, 45] -> Target: 40\n",
      "Context: [48, 53, 57, 45, 54, 45, 40] -> Target: 51\n",
      "Context: [48, 53, 57, 45, 54, 45, 40, 51] -> Target: 9\n"
     ]
    }
   ],
   "source": [
    "def getBatch(data: torch.Tensor,\n",
    "             batchSize: int,\n",
    "             sequenceLength: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    ix = torch.randint(0, data.size(0) - sequenceLength, (batchSize,))\n",
    "    x = torch.stack([data[i:i+sequenceLength] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+sequenceLength+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "batchSize = 4\n",
    "sequenceLength = 8\n",
    "xBatch, yBatch = getBatch(trainData, batchSize, sequenceLength)\n",
    "print(f\"Context batch shape: {xBatch.shape}\")\n",
    "print(xBatch)\n",
    "print(\"Target batch shape:\", yBatch.shape)\n",
    "print(yBatch)\n",
    "for batch in range(batchSize):\n",
    "    for token in range(sequenceLength):\n",
    "        context = xBatch[batch, :token+1].tolist()\n",
    "        target = yBatch[batch, token].item()\n",
    "        print(f\"Context: {context} -> Target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 175,
     "status": "ok",
     "timestamp": 1749658845400,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "AivElGvA1_7m",
    "outputId": "203624f4-bed4-4638-aca1-4f1915f29a45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.79M parameters\n"
     ]
    }
   ],
   "source": [
    "nLayers = 6\n",
    "nHeads = 6\n",
    "sequenceLength = 256\n",
    "embeddingSize = 384\n",
    "dropout = 0.2\n",
    "model = GPTLanguageModel(vocabularySize,\n",
    "                         nLayers,\n",
    "                         nHeads,\n",
    "                         sequenceLength,\n",
    "                         embeddingSize,\n",
    "                         dropout).to(device)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 727882,
     "status": "ok",
     "timestamp": 1749659573282,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "IjEwb0Ni1_7m",
    "outputId": "18c51863-8994-4cd2-f242-d6b27b10184a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimateLoss(model: nn.Module,\n",
    "                 data: torch.Tensor,\n",
    "                 evalIter: int,\n",
    "                 batchSize: int,\n",
    "                 sequenceLength: int) -> float:\n",
    "    model.eval()\n",
    "    losses = torch.zeros(evalIter)\n",
    "    for i in range(evalIter):\n",
    "        xBatch, yBatch = getBatch(data, batchSize, sequenceLength)\n",
    "        logits, loss = model(xBatch, yBatch)\n",
    "        losses[i] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean().item()\n",
    "\n",
    "\n",
    "# Load trained model\n",
    "# model.load_state_dict(torch.load((Path(\"models\") / dataset).with_suffix(\".pt\"), weights_only=False))\n",
    "\n",
    "# Train model from scratch\n",
    "batchSize = 64\n",
    "learningRate = 3e-4\n",
    "maxIter = 800\n",
    "evalInterval = 100\n",
    "evalIter = 100\n",
    "optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
    "for iter in range(maxIter + 1):\n",
    "    if iter % evalInterval == 0 or iter == maxIter - 1:\n",
    "        trainLoss = estimateLoss(model,\n",
    "                                 trainData,\n",
    "                                 evalIter,\n",
    "                                 batchSize,\n",
    "                                 sequenceLength)\n",
    "        valLoss = estimateLoss(model, valData, evalIter, batchSize, sequenceLength)\n",
    "        print(f\"Iter: {iter}, Train loss: {trainLoss}, Val loss: {valLoss}\")\n",
    "    xBatch, yBatch = getBatch(trainData, batchSize, sequenceLength)\n",
    "    logits, loss = model(xBatch, yBatch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "torch.save(model.state_dict(), (Path(\"models\") / dataset).with_suffix(\".pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 139674,
     "status": "ok",
     "timestamp": 1749659712958,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "sFq7hte51_7n",
    "outputId": "e0153772-10f7-4f6c-b46e-a417535af7e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los hermanos sean unidos,\n",
      "la mangana al pobre lao,\n",
      "una mujera como el cielo..\n",
      "\n",
      "El gaucho pobre el indio\n",
      "pero desado desasiegida:\n",
      "cuando vido esamo sabeErra,\n",
      "al cuerto me volé el suerto\n",
      "con las espas juras y gallicas;\n",
      "al verlos crelos estrulos\n",
      "y los indios cabutos.\n",
      "\n",
      "La la mozo ban dar astigunas,\n",
      "no predo que rota el pingo\n",
      "y el peluque tel infin los valones;\n",
      "con hombre se asiguidando\n",
      "los cuerpos de hacer apiezados.\n",
      "\n",
      "Pido no, consuelanciando\n",
      "éramos si aguas!\n",
      "veamos Piarendo el cueyo\n",
      "lo espanté ya polierza.\n",
      "Pero, al fin es más gato\n",
      "ni a a de piara y tirasciano.\n",
      "\n",
      "Afliendo qué el manodo\n",
      "me lada en indiadestida:\n",
      "ande, echaba agranaos,\n",
      "aunque era es indios trabes,\n",
      "porque no estran las panas;\n",
      "de esas años, como yenan,\n",
      "sin perdonen dentras pomonentes\n",
      "como en en el lanzan\n",
      "como otroros en las penandan.\n",
      "\n",
      "A forvean las perresos\n",
      "me que como canteros el asanto;\n",
      "con en oporto con su asinancia\n",
      "y viven de milanzas a tan botan amáos\n",
      "aunque le atura tener lima.\n",
      "\n",
      "Es se dentran las camplanzas\n",
      "por sacariones en vientiendan,\n",
      "cuandpo elos casonojes,\n",
      "cuaviendo aguardando aquellas\n",
      "y echan roto seguidas,\n",
      "en lágras cadas\n",
      "de Ecel y viejos, las infiesas,\n",
      "como cadespupantos como a dura\n",
      "en espuego bartó el corrollo,\n",
      "vi con poco el montro periallo.\n",
      "\n",
      "Pero estaban yo paso\n",
      "porque andanza el canto el guapo,\n",
      "que no viviero como entonca\n",
      "porque es estas planzas los cieles\n",
      "que la tormendo el pico.\n",
      "\n",
      "Nos he cuido con camino\n",
      "diciéndolésme cual secuido;\n",
      "él que una desgraque no abota\n",
      "a la vararlo del de un blanche,\n",
      "dejado más decía\n",
      "que eramosarle alije.\n",
      "\n",
      "Y al es nos viejos muchas\n",
      "bando esperadar como estaba;\n",
      "como podía en aprecarse,\n",
      "falte a escapáciando,\n",
      "solían de al lanza ascaas\n",
      "ra dó de decaraza.\n",
      "\n",
      "Hagado ceralajas,\n",
      "cuando esos liejos y sobre,\n",
      "que por los guasiviones\n",
      "con los de indiedes,XIX\n",
      "\n",
      "Mas andas podeces amas,\n",
      "con el asasel divirioia\n",
      "y sin de el fueño que biene,\n",
      "no me vido de cierda,\n",
      "iba aunque de tabl en vieja.\n",
      "\n",
      "Se perle hizo de matar\n",
      "sin que una peno al inte;\n",
      "no la ocasión a vena\n",
      "de coy hicierda esclama\n",
      "pero se noche enterrariao\n",
      "que no los tenía larta\n",
      "de la tormento no yecía.\n",
      "\n",
      "Pues con perdar: eso del resto\n",
      "al versento cualquieran viva\n",
      "cuando esos o esolos,troios\n",
      "y mico llovertos comi el añor;\n",
      "le dise rancó el brigo\n",
      "y en mi dicuelos de ayuso.\n",
      "\n",
      "Es el ven tontos en seña;\n",
      "como voz desante Con,\n",
      "su se alta inque juerza sereña,\n",
      "con las enteros consellas,\n",
      "era duros alcanzas de halados,\n",
      "y ha al menocen su tiempo\n",
      "en persió como juego.\n",
      "\n",
      "En el pertino no si me inicone\n",
      "allí que es lengá el goyo\n",
      "y andiada a legraciada,\n",
      "cuando esa eso ayo\n",
      "anda si aflojeMe yo, levé\n",
      "más lo gen dicicelo y me aguante.\n",
      "\n",
      "El perca y con esdicia\n",
      "que su incó con lo encuevo;\n",
      "y allí precantó de amuchado\n",
      "pero a hacen el mesmo entero;\n",
      "el como yo mo son largado,\n",
      "en que que nos espareces,\n",
      "cuando no sé cora en los manes.\n",
      "\n",
      "A ven más con eso\n",
      "tan Dios asimbe los que ansí;\n",
      "bresos desconocidos el base;\n",
      "con y los alivaren al quere\n",
      "al mídan de asuelo.\n",
      "\n",
      "Jamás dos pareces espuellos\n",
      "a jura silicio dijundo;\n",
      "era salgarmabr de cuidaa;\n",
      "me halla negre de la maniadua\n",
      "espara eso un Jundo uno un terrón\n",
      "miedpo de qué con legüevo.\n",
      "\n",
      "Cantaba ando, ía elas güelas,\n",
      "pero al paso me atráciomos,\n",
      "de de muy preso dento,\n",
      "no los que los pritiros se sombre\n",
      "el pinque todo saquel señale.\n",
      "\n",
      "Suele duro y conven un voz\n",
      "agraOpuaos con cuentando;\n",
      "para que los cristones\n",
      "cuando esas botan aflijes,\n",
      "al indio es lanzal vertan;\n",
      "no portían an amidan\n",
      "aunque la habre el cutindo.\n",
      "\n",
      "XDespués que eso que una miscaa,\n",
      "es despobre esaseguas\n",
      "que esos pino dos remujen\n",
      "es que adrejar allí;\n",
      "la señor la con un goLlpe\n",
      "do estao que la vanda.\n",
      "\n",
      "Sino dan como la una\n",
      "que le desdanté que un terraga;\n",
      "y allí suelo lan gena\n",
      "sinó con mi cañalde,\n",
      "dicendo alca fonto el gán ese mando;\n",
      "y ya ya, ansí canto ampo\n",
      "nos debuían a costra el condel.\n",
      "\n",
      "Se vino me abran las esmanas\n",
      "con las el pelino resa\n",
      "los que es potritas, el picho........ ¡chice viras,\n",
      "para una mora San güelas\n",
      "con y pierde el tiro de en sol.\n",
      "\n",
      "Y pobre me inompre la baju\n",
      "para en mi mostra de visto:\n",
      "es pronte aquél del preso;\n",
      "sin cosas gamen\n",
      "se hacen de gal en el cuento\n",
      "en cuanque el auno es canta\n",
      "era mi cabrave y allí.\n",
      "\n",
      "Sine el cargé y mado canto,\n",
      "vino una carancia déeto,\n",
      "que pues eso es ganez,\n",
      "el cal tiempo la mesmo iguala;\n",
      "lo me arregué fin de pasca\n",
      "y entre el pelpiarlo inieto.\n",
      "\n",
      "Rece mi carrerce comen poos,\n",
      "como por en encontrarme dieza,\n",
      "maza,, con los pudos toldos\n",
      "con los cuidos Cras antao;\n",
      "formando el yo,\n",
      "me ponía al acariadorlo,\n",
      "que aquel mandan en fea.\n",
      "\n",
      "Me el voz campo de de queño\n",
      "y en tener la mujunto;\n",
      "y de ponce el gobre grango\n",
      "como y languna de mi cosa:\n",
      "la lo pelicaban que a un patró\n",
      "le lanza de oclao.\n",
      "Y ¡VIIXI\n",
      "\n",
      "Hasta al noces y venía\n",
      "a la puebla el día gauchacer!\n",
      "Es la caldicía\n",
      "la ñada esa lo algada,\n",
      "logré la sanguna virda\n",
      "y canarce de las peluas!\n",
      "\n",
      "-\"Hacentestá el pun bricho\n",
      "prefendarlo a costas que las pies\n",
      "que yo al maldita la pesara;\n",
      "él buel manoce retodario\n",
      "para no soy allías viertao.\"\n",
      "\n",
      "üel campañargao como ganel,\n",
      "vuenta lo entence el pulpe,\n",
      "era esto pelque la ciha\n",
      "lo seguia es alas que el incho,\n",
      "porquiero retonen,\n",
      "no los amparan caradas\n",
      "al pobre los dos vividos\n",
      "como hecidos perros\n",
      "cuando nos me ategraciaos,\n",
      "con una vios me añuda,\n",
      "sin dejanes el paso\n",
      "có un canto livión\n",
      "dol(ando rocegía.\n",
      "\n",
      "Se on este humbre en Tuero\n",
      "pero lo habían hacía y regana;\n",
      "con lestas, empuS leguías\n",
      "cantaban benas apualas\n",
      "cuando me día el desolvaje,\n",
      "deban que estaba que connsuevanto.\n",
      "\n",
      "Estuve cuanto lo escrepandí\n",
      "unas acarecen erates:\n",
      "el pelé vermos y se bratandá\n",
      "van por los toldiceros\n",
      "\"me un cuevo el rolzo.\"\n",
      "\n",
      "XIXI\n",
      "\n",
      "Las boneresos de revescuiós,\n",
      "si ponía toda almar;\n",
      "nos matao de puestar\n",
      "y ya penaron, sido comesejo;\n",
      "seguidó la voz andare,\n",
      "porque las saliaban milir,\n",
      "ni noche otro presonances\n",
      "cuando con la costadanzan.\n",
      "\n",
      "Cal vez a junete el coraz\n",
      "el desechino en las gallas\n",
      "que salga como gaucho de caño;\n",
      "al páña lo dan seguida\n",
      "cuando arazó das añón.\n",
      "\n",
      "Allá los boldando de ayúse malzas,\n",
      "en las cosas de las copes,\n",
      "como cerdan y tan voz,\n",
      "con los acosas despiones,\n",
      "ran¡paranandos mediezas.\n",
      "\n",
      "Sos abamos juros y hizos,\n",
      "en susos volvidos, pales;\n",
      "que les pueben deseó\n",
      "enan cla reguada,\n",
      "pues los peléan cometa,\n",
      "pero eso pocuesos y modos\n",
      "y debe nuventos se domban,\n",
      "y en  corar la cala.\n",
      "\n",
      "Esas cayó\n",
      "le miro jinas avir,\n",
      "cantando terrías en incia\n",
      "diju\"que ablarlo sarir,\n",
      "para la era ecinciar\n",
      "por las vinos declames:\n",
      "la vida el juzcio elejo\n",
      "y allí se le huno le diañó.\n",
      "\n",
      "Sólamé el carrerol del ruido:\n",
      "ninguna es resento año,\n",
      "contra le lanza ninguno\n",
      "es se acolñado a comira;\n",
      "y es me andan dilas\n",
      "lo los coPenen el que mano.\n",
      "\n",
      "Quitos con el lo cordo\n",
      "para on caya ganos en flos.\n",
      "Licimos lado a fontes,\n",
      "pero han añitarlos, patadones\n",
      "como una muy negrancia\n",
      "aunque es solvan medio.\n",
      "\n",
      "Ayo más me odeoían\n",
      "para cualquiera este las pelas\n",
      "entre que abllaman el dé yo,\n",
      "para un pengo que hascado\n",
      "asta no se te apocasióas\n",
      "taa sustra preceres desancias\n",
      "que las es porría soy anciando.\n",
      "\n",
      "Pero al juez, se añor\n",
      "lo hizo lo de desalté;\n",
      "más pante el su frio no chiquillo;\n",
      "dejé que esa arrriba la cuerto\n",
      "bándoló de carte un volvión,\n",
      "y dende como que llevente\n",
      "como cuando más boldore.\n",
      "\n",
      "El pindio el fin morigo\n",
      "me llevamo que en el pacio,\n",
      "cuando fronte al fin,\n",
      "y corto me la india,\n",
      "el con estino que lo oyes\n",
      "los de vamás camento.\n",
      "\n",
      "Mas trestas llatos del pellato\n",
      "como canté el cantome;\n",
      "más tiros las ventes\n",
      "voy mi calidá dentré,\n",
      "le dirá que es saber,\n",
      "no son por los mejos hijos.\n",
      "\n",
      "Y miedo atrme fin sin penseras\n",
      "sin desponechas de vivan:\n",
      "sino, chaidido a San andi?\n",
      "\n",
      "Y visino les cuentos comisas\n",
      "que él que que aflivir las cones\n",
      "que dujarles de enterrio siquere.\n",
      "\n",
      "Cante es eso los solvan,\n",
      "solían deja ray, el desiejo,\n",
      "fijo de lanza una genas;\n",
      "allí es h carde un reveniones\n",
      "y solos del ciques que no sargaban,\n",
      "cuando al fin divientos\n",
      "el rediaba de entienentro\n",
      "porque en pasarle y lanza.\n",
      "\n",
      "Ferro de allí del perajo;\n",
      "los lazoneción sin paguala,\n",
      "mas pa los decir que hacitando a vivida;\n",
      "me dan que desarlas.\n",
      "\n",
      "No son pocos; alenes el cuidos\n",
      "no hechos como asinos,\n",
      "soy eran salvajar tor,\n",
      "viendo al del latero\n",
      "lo que man de consuelda lones;\n",
      "en un canto asegualá\n",
      "hiciendo los delichillos.\n",
      "\n",
      "Para en una de mis inores\n",
      "como el solté en las juerzas,\n",
      "mas no conso hizo afir para,\n",
      "no en un balanciado vuelo,\n",
      "con los cuidaos cadas;\n",
      "a los cerdesnan amayos,\n",
      "ninguna salgunia mociadado;\n",
      "pero son seguían otros cualquierenos.\n",
      "\n",
      "Las precasas herecas resomos\n",
      "al ponenemos cado loseras,\n",
      "cuando el pobre de descuidos,\n",
      "y ansiCunos los lentos manejidos\n",
      "alzando que esa empiera alguna.\n",
      "\n",
      "Nunca vineronoces\n",
      "al por de solvarán,\n",
      "nos han otros de dentrarón,\n",
      "ningunos él su volvaróVir,\n",
      "pero en que esa inora tierra\n",
      "la que esa desiquina por comer.\n",
      "\n",
      "Y refe, de salir saltor\n",
      "como tenía arrios, ben aliciones,\n",
      "vitándolés que eron,\n",
      "y fortos, ningunos bierron\n",
      "y se aeguarran como ruelo;\n",
      "en mil largao mis corderias\n",
      "y corten su intón de animal.\n",
      "\n",
      "Tuve no pues de halar\n",
      "hasta al de racer el desguena;\n",
      "éste se mandí al de un desierto\n",
      "cuando ¡Pediés! a la celadición\n",
      "el de alcasaridos de eradoras,\n",
      "como hizo en perrían a pradeca\n",
      "el que el juez del sundo\n",
      "y con güen punto úlvarlo.\n",
      "\n",
      "Que estraro te vecenció\n",
      "empezandonabá los dices,\n",
      "pues esocape es anuma\n",
      "cuando esa lo vida\n",
      "lo en reque la ama enco\n",
      "y a lo crastaba al cuerto;\n",
      "dejé con la gente\n",
      "cuando y la cargaté.\n",
      "\n",
      "El printo, coraque\n",
      "sigue sabec\n",
      "cuando leva de la juera,\n",
      "el que deserigo de liquiera,\n",
      "y y yo me entiedé aquel;\n",
      "es la muya calza de venda,\n",
      "le dijo que estaba el juez,\n",
      "para no la leto apre¡a!.\n",
      "\n",
      "Me levé a al senticiano\n",
      "de un bote de están proyudo,\n",
      "y amigo, este l hoy mano,\n",
      " es chabe era reseglumo\n",
      "porque sin de indio del tógo\n",
      "que es castalde antel chento.\n",
      "\n",
      "Yo amos al almando\n",
      "como aullo de punto;\n",
      "al versé y mos cerazón,\n",
      "ni los tiembras nos covías\n",
      "de una mujercia\n",
      "que me encuida a ladigarían\n",
      "\n",
      "que entro en el aprimasió\n",
      "y el que éstaba carrraciada.\n",
      "\n",
      "Lespués remesos sucedos\n",
      "de adaron tenían hacer en picado,\n",
      "entras mis nunca nociendidos\n",
      "con lovenos dicendones,\n",
      "los pudijes, como un vendo\n",
      "como lanciendo, alba miliao.\n",
      "\n",
      "Y le arrrume a rerijo\n",
      "que rese cogue el cuempo\n",
      "porque el indo esprimaró\n",
      "que la la palara a busca;\n",
      "no hace gana carrse al punta,\n",
      "sin que a sabore aljaría;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = \"Los hermanos sean unidos\" # \"To be, or not to be\"\n",
    "context = torch.tensor(encode(context), dtype=torch.long).unsqueeze(0).to(device)\n",
    "generatedText = decode(model.generate(context, nTokens=10000)[0].tolist())\n",
    "print(generatedText)\n",
    "with open((Path(\"outputs\") / dataset).with_suffix(\".txt\"), \"w\") as f:\n",
    "    f.write(generatedText)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "18P5ELoTJruhjlZCO6gr8afIF5VeWWKT9",
     "timestamp": 1749610753907
    }
   ]
  },
  "kernelspec": {
   "display_name": "transformer-notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
