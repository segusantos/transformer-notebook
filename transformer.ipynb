{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqS26uut1_7N"
   },
   "source": [
    "# **Transformer Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVeQuGBF1_7T"
   },
   "source": [
    "### **Encoder-Decoder Architecture**\n",
    "\n",
    "It is used for **sequence-to-sequence** tasks like machine translation.\n",
    "\n",
    "This is the architecture proposed by the original transformer paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. in 2017.\n",
    "\n",
    "<img src=\"assets/encoder_decoder.png\" alt=\"Encoder-Decoder Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RE1K3Dex-h6Z"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APfB9jre1_7V"
   },
   "source": [
    "### **Decoder-Only Architecture**\n",
    "\n",
    "It is used for **sequence generation** tasks like language modeling.\n",
    "\n",
    "This simpler architecture will be implemented in this notebook from scratch.\n",
    "\n",
    "<img src=\"assets/decoder_only.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1749658844461,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "41bejM8V1_7W",
    "outputId": "dcab5d71-a00c-47d4-a108-2c5a2ef2c6eb"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not Path(\"data\").exists():\n",
    "        !git clone https://github.com/segusantos/transformer-notebook.git\n",
    "        %cd transformer-notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BxQZcCR1_7a"
   },
   "source": [
    "## **Scaled Dot-Product Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gY3WuR6c1_7b"
   },
   "source": [
    "Let $Q \\in \\mathbb{R}^{m \\times d_k}$, $K \\in \\mathbb{R}^{n \\times d_k}$ y $V \\in \\mathbb{R}^{n \\times d_v}$ be the query, key and value matrices, respectively. The scaled dot-product attention is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\in \\mathbb{R}^{m \\times d_v}\n",
    "$$\n",
    "\n",
    "<img src=\"assets/self_attention.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mSfCf-011_7c"
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 sequenceLength: int,\n",
    "                 embeddingSize: int,\n",
    "                 headSize: int,\n",
    "                 dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.query = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.value = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(sequenceLength, sequenceLength)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batchSize, sequenceLength, embeddingSize = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        weights = q @ k.transpose(-2, -1) / (embeddingSize ** 0.5)\n",
    "        weights = weights.masked_fill(self.tril[:sequenceLength, :sequenceLength] == 0, float(\"-inf\"))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        x = weights @ v\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0SNKag41_7d"
   },
   "source": [
    "## **Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erjXpdQe1_7e"
   },
   "source": [
    "Let $h$ be the number of heads. The multi-head attention is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\in \\mathbb{R}^{m \\times d_v}\n",
    "$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$ and $W_i^Q \\in \\mathbb{R}^{d_k \\times d_k}$, $W_i^K \\in \\mathbb{R}^{d_k \\times d_k}$, $W_i^V \\in \\mathbb{R}^{d_v \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_v}$ are the learnable parameters.\n",
    "\n",
    "<img src=\"assets/multi_head_attention.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DmUUm3Db1_7f"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 nHeads: int,\n",
    "                 sequenceLength: int,\n",
    "                 embeddingSize: int,\n",
    "                 dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([ScaledDotProductAttention(sequenceLength,\n",
    "                                                              embeddingSize,\n",
    "                                                              embeddingSize // nHeads,\n",
    "                                                              dropout) for _ in range(nHeads)])\n",
    "        self.projection = nn.Linear(embeddingSize, embeddingSize)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.projection(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECfdczW81_7g"
   },
   "source": [
    "## **Position-wise Feed-Forward Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1GfRIuN1_7g"
   },
   "source": [
    "The position-wise feed-forward networks are defined as:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "where $W_1 \\in \\mathbb{R}^{d_{ff} \\times d_{model}}$, $b_1 \\in \\mathbb{R}^{d_{ff}}$, $W_2 \\in \\mathbb{R}^{d_{model} \\times d_{ff}}$ and $b_2 \\in \\mathbb{R}^{d_{model}}$ are the learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qBpO3Eq21_7h"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embeddingSize: int,\n",
    "                 dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.feedForward = nn.Sequential(\n",
    "            nn.Linear(embeddingSize, 4 * embeddingSize),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embeddingSize, embeddingSize),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.feedForward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Sfu0Cvp1_7h"
   },
   "source": [
    "## **Layer Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0LWpqI71_7h"
   },
   "source": [
    "The layer normalization is defined as:\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sigma} + \\beta\n",
    "$$\n",
    "\n",
    "where $\\gamma \\in \\mathbb{R}^{d_{model}}$ and $\\beta \\in \\mathbb{R}^{d_{model}}$ are the learnable parameters, and $\\mu$ and $\\sigma$ are the mean and standard deviation of $x$, respectively.\n",
    "\n",
    "In the original paper, the layer normalization is applied after the residual connections, whereas in more modern implementations, it is applied before in what is known as **pre-norm formulation**.\n",
    "\n",
    "```python\n",
    "# Original paper\n",
    "x = LayerNorm(x + MultiHeadAttention(x))\n",
    "x = LayerNorm(x + FeedForward(x))\n",
    "\n",
    "# Pre-norm formulation\n",
    "x = x + MultiHeadAttention(LayerNorm(x))\n",
    "x = x + FeedForward(LayerNorm(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lereN3iL1_7i"
   },
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 nHeads: int,\n",
    "                 sequenceLength: int,\n",
    "                 embeddingSize: int,\n",
    "                 dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.multiHeadAttention = MultiHeadAttention(nHeads,\n",
    "                                                     sequenceLength,\n",
    "                                                     embeddingSize,\n",
    "                                                     dropout)\n",
    "        self.feedForward = FeedForward(embeddingSize, dropout)\n",
    "        self.layerNorm1 = nn.LayerNorm(embeddingSize)\n",
    "        self.layerNorm2 = nn.LayerNorm(embeddingSize)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.multiHeadAttention(self.layerNorm1(x))\n",
    "        x = x + self.feedForward(self.layerNorm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ke_AmotY1_7i"
   },
   "source": [
    "## **Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "R-xFgrgj1_7i"
   },
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__ (self,\n",
    "                  vocabularySize: int,\n",
    "                  nLayers: int,\n",
    "                  nHeads: int,\n",
    "                  sequenceLength:int,\n",
    "                  embeddingSize: int,\n",
    "                  dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenEmbeddingTable = nn.Embedding(vocabularySize, embeddingSize)\n",
    "        self.positionEmbeddingTable = nn.Embedding(sequenceLength, embeddingSize)\n",
    "        self.layers = nn.Sequential(*[Layer(nHeads,\n",
    "                                            sequenceLength,\n",
    "                                            embeddingSize,\n",
    "                                            dropout) for _ in range(nLayers)])\n",
    "        self.layerNorm = nn.LayerNorm(embeddingSize)\n",
    "        self.linearModelHead = nn.Linear(embeddingSize, vocabularySize)\n",
    "        self.sequenceLength = sequenceLength\n",
    "        self.apply(self.initWeights)\n",
    "\n",
    "    def initWeights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, targets: torch.Tensor = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        batchSize, sequenceLength = x.shape\n",
    "        tokenEmbeddings = self.tokenEmbeddingTable(x)\n",
    "        positionEmbeddings = self.positionEmbeddingTable(torch.arange(sequenceLength, device=device))\n",
    "        x = tokenEmbeddings + positionEmbeddings\n",
    "        x = self.layers(x)\n",
    "        x = self.layerNorm(x)\n",
    "        logits = self.linearModelHead(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batchSize, sequenceLength, embeddingSize = logits.shape\n",
    "            logits = logits.view(batchSize * sequenceLength, embeddingSize)\n",
    "            targets = targets.view(batchSize * sequenceLength)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x: torch.Tensor, nTokens: int) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(nTokens):\n",
    "                logits, loss = self(x[:, -self.sequenceLength:])\n",
    "                probabilities = F.softmax(logits[:, -1, :], dim=-1)\n",
    "                nextToken = torch.multinomial(probabilities, num_samples=1)\n",
    "                x = torch.cat([x, nextToken], dim=1)\n",
    "        self.train()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExfoF3cx1_7j"
   },
   "source": [
    "## **Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 89,
     "status": "ok",
     "timestamp": 1749658845179,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "oEK1NJv41_7j",
    "outputId": "fbbff5ef-5f06-4f6d-8e48-d2d422cb91d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 187095\n",
      "I\n",
      "\n",
      "Aquí me pongo a cantar\n",
      "al compás de la vigüela,\n",
      "que el hombre que lo desvela\n",
      "una pena estrordinaria,\n",
      "como la ave solitaria\n",
      "con el cantar se consuela.\n",
      "\n",
      "Pido a los santos del cielo\n",
      "que ayuden mi pensamiento:\n",
      "les pido en este momento\n",
      "que voy a cantar\n"
     ]
    }
   ],
   "source": [
    "dataset = \"martin_fierro\" # \"shakespeare\"\n",
    "with open((Path(\"data\") / dataset).with_suffix(\".txt\"), \"r\") as f:\n",
    "    text = f.read()\n",
    "print(f\"Text length: {len(text)}\")\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1749658845186,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "Ut9lqYSO1_7j",
    "outputId": "26b0d998-d4d7-4a51-c277-e08eb32eefcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 72\n",
      "\n",
      " !\"'(),-.:;?ABCDEFGHIJLMNOPQRSTUVXYZabcdefghijklmnopqrstuvxyz¡¿Ñáéíñóúü\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sorted(set(text))\n",
    "vocabularySize = len(vocabulary)\n",
    "print(f\"Vocabulary size: {vocabularySize}\")\n",
    "print(\"\".join(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1749658845192,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "kLvqV9Jr1_7k",
    "outputId": "cdeecf7a-6800-4b26-c7dc-5088984bd483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 51, 55, 1, 44, 41, 54, 49, 37, 50, 51, 55, 1, 55, 41, 37, 50, 1, 57, 50, 45, 40, 51, 55]\n",
      "Los hermanos sean unidos\n"
     ]
    }
   ],
   "source": [
    "stringToToken = {ch: i for i, ch in enumerate(vocabulary)}\n",
    "tokenToString = {i: ch for i, ch in enumerate(vocabulary)}\n",
    "encode = lambda string: [stringToToken[char] for char in string]\n",
    "decode = lambda tokens: \"\".join([tokenToString[token] for token in tokens])\n",
    "print(encode(\"Los hermanos sean unidos\"))\n",
    "print(decode(encode(\"Los hermanos sean unidos\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1749658845198,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "grysOjv-1_7k",
    "outputId": "50ab0090-7977-4e9c-a771-cd20eec6074c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([187095])\n",
      "Data type: torch.int64\n",
      "tensor([21,  0,  0, 13, 53, 57, 67,  1, 49, 41,  1, 52, 51, 50, 43, 51,  1, 37,\n",
      "         1, 39, 37, 50, 56, 37, 54,  0, 37, 48,  1, 39, 51, 49, 52, 65, 55,  1,\n",
      "        40, 41,  1, 48, 37,  1, 58, 45, 43, 71, 41, 48, 37,  7,  0, 53, 57, 41,\n",
      "         1, 41, 48,  1, 44, 51, 49, 38, 54, 41,  1, 53, 57, 41,  1, 48, 51,  1,\n",
      "        40, 41, 55, 58, 41, 48, 37,  0, 57, 50, 37,  1, 52, 41, 50, 37,  1, 41,\n",
      "        55, 56, 54, 51, 54, 40, 45, 50, 37, 54])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Data type: {data.dtype}\")\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1749658845227,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "pZjPFtGo1_7l",
    "outputId": "77fb5cb4-a26e-4607-9478-851e6007e166"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([149676])\n",
      "Validation data shape: torch.Size([37419])\n"
     ]
    }
   ],
   "source": [
    "trainValSplit = 0.8\n",
    "trainSize = int(len(data) * trainValSplit)\n",
    "trainData = data[:trainSize]\n",
    "valData = data[trainSize:]\n",
    "print(f\"Train data shape: {trainData.shape}\")\n",
    "print(f\"Validation data shape: {valData.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1749658845228,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "QWuJtH4g1_7l",
    "outputId": "a86d4714-747f-4d20-bac6-7204340694c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context batch shape: torch.Size([4, 8])\n",
      "tensor([[57, 50, 51,  1, 55, 41,  1, 40],\n",
      "        [17, 55,  1, 40, 41,  1, 37, 48],\n",
      "        [55, 37, 54, 50, 51, 55, 51,  0],\n",
      "        [48, 53, 57, 45, 54, 45, 40, 51]], device='cuda:0')\n",
      "Target batch shape: torch.Size([4, 8])\n",
      "tensor([[50, 51,  1, 55, 41,  1, 40, 37],\n",
      "        [55,  1, 40, 41,  1, 37, 48, 49],\n",
      "        [37, 54, 50, 51, 55, 51,  0, 60],\n",
      "        [53, 57, 45, 54, 45, 40, 51,  9]], device='cuda:0')\n",
      "Context: [57] -> Target: 50\n",
      "Context: [57, 50] -> Target: 51\n",
      "Context: [57, 50, 51] -> Target: 1\n",
      "Context: [57, 50, 51, 1] -> Target: 55\n",
      "Context: [57, 50, 51, 1, 55] -> Target: 41\n",
      "Context: [57, 50, 51, 1, 55, 41] -> Target: 1\n",
      "Context: [57, 50, 51, 1, 55, 41, 1] -> Target: 40\n",
      "Context: [57, 50, 51, 1, 55, 41, 1, 40] -> Target: 37\n",
      "Context: [17] -> Target: 55\n",
      "Context: [17, 55] -> Target: 1\n",
      "Context: [17, 55, 1] -> Target: 40\n",
      "Context: [17, 55, 1, 40] -> Target: 41\n",
      "Context: [17, 55, 1, 40, 41] -> Target: 1\n",
      "Context: [17, 55, 1, 40, 41, 1] -> Target: 37\n",
      "Context: [17, 55, 1, 40, 41, 1, 37] -> Target: 48\n",
      "Context: [17, 55, 1, 40, 41, 1, 37, 48] -> Target: 49\n",
      "Context: [55] -> Target: 37\n",
      "Context: [55, 37] -> Target: 54\n",
      "Context: [55, 37, 54] -> Target: 50\n",
      "Context: [55, 37, 54, 50] -> Target: 51\n",
      "Context: [55, 37, 54, 50, 51] -> Target: 55\n",
      "Context: [55, 37, 54, 50, 51, 55] -> Target: 51\n",
      "Context: [55, 37, 54, 50, 51, 55, 51] -> Target: 0\n",
      "Context: [55, 37, 54, 50, 51, 55, 51, 0] -> Target: 60\n",
      "Context: [48] -> Target: 53\n",
      "Context: [48, 53] -> Target: 57\n",
      "Context: [48, 53, 57] -> Target: 45\n",
      "Context: [48, 53, 57, 45] -> Target: 54\n",
      "Context: [48, 53, 57, 45, 54] -> Target: 45\n",
      "Context: [48, 53, 57, 45, 54, 45] -> Target: 40\n",
      "Context: [48, 53, 57, 45, 54, 45, 40] -> Target: 51\n",
      "Context: [48, 53, 57, 45, 54, 45, 40, 51] -> Target: 9\n"
     ]
    }
   ],
   "source": [
    "def getBatch(data: torch.Tensor,\n",
    "             batchSize: int,\n",
    "             sequenceLength: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    ix = torch.randint(0, data.size(0) - sequenceLength, (batchSize,))\n",
    "    x = torch.stack([data[i:i+sequenceLength] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+sequenceLength+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "batchSize = 4\n",
    "sequenceLength = 8\n",
    "xBatch, yBatch = getBatch(trainData, batchSize, sequenceLength)\n",
    "print(f\"Context batch shape: {xBatch.shape}\")\n",
    "print(xBatch)\n",
    "print(\"Target batch shape:\", yBatch.shape)\n",
    "print(yBatch)\n",
    "for batch in range(batchSize):\n",
    "    for token in range(sequenceLength):\n",
    "        context = xBatch[batch, :token+1].tolist()\n",
    "        target = yBatch[batch, token].item()\n",
    "        print(f\"Context: {context} -> Target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 175,
     "status": "ok",
     "timestamp": 1749658845400,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "AivElGvA1_7m",
    "outputId": "203624f4-bed4-4638-aca1-4f1915f29a45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.79M parameters\n"
     ]
    }
   ],
   "source": [
    "nLayers = 6\n",
    "nHeads = 6\n",
    "sequenceLength = 256\n",
    "embeddingSize = 384\n",
    "dropout = 0.2\n",
    "model = GPTLanguageModel(vocabularySize,\n",
    "                         nLayers,\n",
    "                         nHeads,\n",
    "                         sequenceLength,\n",
    "                         embeddingSize,\n",
    "                         dropout).to(device)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 727882,
     "status": "ok",
     "timestamp": 1749659573282,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "IjEwb0Ni1_7m",
    "outputId": "18c51863-8994-4cd2-f242-d6b27b10184a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Train loss: 4.339381217956543, Val loss: 4.339980602264404\n",
      "Iter: 100, Train loss: 2.330831527709961, Val loss: 2.3314294815063477\n",
      "Iter: 200, Train loss: 2.2554426193237305, Val loss: 2.2624142169952393\n",
      "Iter: 300, Train loss: 2.149970769882202, Val loss: 2.164577007293701\n",
      "Iter: 400, Train loss: 2.0018880367279053, Val loss: 2.0282297134399414\n",
      "Iter: 500, Train loss: 1.7985153198242188, Val loss: 1.8598233461380005\n",
      "Iter: 600, Train loss: 1.6028972864151, Val loss: 1.7175953388214111\n",
      "Iter: 700, Train loss: 1.4128634929656982, Val loss: 1.623814582824707\n",
      "Iter: 799, Train loss: 1.2280348539352417, Val loss: 1.607738971710205\n",
      "Iter: 800, Train loss: 1.2292596101760864, Val loss: 1.60943603515625\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimateLoss(model: nn.Module,\n",
    "                 data: torch.Tensor,\n",
    "                 evalIter: int,\n",
    "                 batchSize: int,\n",
    "                 sequenceLength: int) -> float:\n",
    "    model.eval()\n",
    "    losses = torch.zeros(evalIter)\n",
    "    for i in range(evalIter):\n",
    "        xBatch, yBatch = getBatch(data, batchSize, sequenceLength)\n",
    "        logits, loss = model(xBatch, yBatch)\n",
    "        losses[i] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean().item()\n",
    "\n",
    "\n",
    "# Load trained model\n",
    "# model.load_state_dict(torch.load((Path(\"models\") / dataset).with_suffix(\".pt\"), weights_only=False))\n",
    "\n",
    "# Train model from scratch\n",
    "batchSize = 64\n",
    "learningRate = 3e-4\n",
    "maxIter = 800\n",
    "evalInterval = 100\n",
    "evalIter = 100\n",
    "optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
    "for iter in range(maxIter + 1):\n",
    "    if iter % evalInterval == 0 or iter == maxIter - 1:\n",
    "        trainLoss = estimateLoss(model,\n",
    "                                 trainData,\n",
    "                                 evalIter,\n",
    "                                 batchSize,\n",
    "                                 sequenceLength)\n",
    "        valLoss = estimateLoss(model, valData, evalIter, batchSize, sequenceLength)\n",
    "        print(f\"Iter: {iter}, Train loss: {trainLoss}, Val loss: {valLoss}\")\n",
    "    xBatch, yBatch = getBatch(trainData, batchSize, sequenceLength)\n",
    "    logits, loss = model(xBatch, yBatch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "torch.save(model.state_dict(), (Path(\"models\") / dataset).with_suffix(\".pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 139674,
     "status": "ok",
     "timestamp": 1749659712958,
     "user": {
      "displayName": "Segundo Santos Torrado",
      "userId": "15292425414534339924"
     },
     "user_tz": 180
    },
    "id": "sFq7hte51_7n",
    "outputId": "e0153772-10f7-4f6c-b46e-a417535af7e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los hermanos sean unidos\n",
      "mas que se tura a almao,\n",
      "y al hermemos mis inodas\n",
      "porque varse a carre espara;\n",
      "naides da del pisado pesca\n",
      "empezó a livida nos de todo.\n",
      "\n",
      "\"Soy dio aquel decirguelo\n",
      "en tal pingo crumillo;\n",
      "a la causa en la pobre\n",
      "y era un calito día:, \"se atreva,\n",
      "\"vola toda \"escoque Cauz.\"\n",
      "\n",
      "\"A OY vos dejos,\n",
      "\"porque le asisió, a un rator\n",
      "\"jue no verÑotao se culerchudo\",\n",
      "\"y al verár trepego los decí\",\n",
      "\"vo\", de vueho a de jauera,\"\n",
      "\"lle calige desto veneno\".\"\n",
      "\n",
      "\"recyes y lo inora modoné.\"\n",
      "Le repíó ver vieno\n",
      "\"donde te el estonte.\"\n",
      "\"El que no toy iente esea\n",
      "\" refroldita\".\"Se Ien \"lasta\n",
      "\"yo es casta el hocuchá!\n",
      "\"las bravos silhas\"\n",
      "\"se llevarés de ley me ajé\".\n",
      "\n",
      "\"Descueles que no hé es cuanto\n",
      "\"hasta de dos cair en su malenza,\n",
      "\"yo debedor las pllatas\n",
      "\"tierTe se resaorien Di vimplo!\"\n",
      "\n",
      "\"yo cuérta el solario\n",
      "\"no tal vez a a la feta;\n",
      "\"en contrandiése dije\"\n",
      "\"te venao por de de asienta;\"e je\"y \"y aa la vist\".\".\n",
      "no en le quitese en el latje\".\n",
      "\n",
      "Se se dece concontes que ha veces\n",
      "sin das con el lendito esto;\n",
      "altro camino me hace horagao,\n",
      "lo dije: \"es un echerdidio,\n",
      "\"por todo no que se ha\n",
      "\"cuando berre britar.\"\n",
      "\n",
      "Ese eso es lo mero que en un peligro\n",
      "le asigua emperzó a fuera;\n",
      "mas no he recoletá que me duda,\n",
      "\"Voto dende la deciria\n",
      "\"y no se deja nueña verdá\n",
      "\"y de te hente endarseñarse.\"\n",
      "\n",
      "\"A vacamos por desa\n",
      "aunque en esesas y bellajas;\"ves descaos del cencinales\n",
      "con hasta que a solir paranza;\n",
      "y de esta gran las munas\n",
      "le pobre al trever con de palica.\"\n",
      "\n",
      "\"Es es que es lama comenda,\n",
      "a tuve hacer que en esta un monta;\n",
      "es es griso a la lana pilata\n",
      "lauy con de ajueras reblos\n",
      "y le arrmanca en sundanza;\n",
      "porque se los quedan ampanarlo\n",
      "caiga el pobre es remeso.\n",
      "\n",
      "En es defrecerba y en cuero,\n",
      "aunque ruega el briguno y toda,\n",
      "por en poninguna con el dale,\n",
      "como más le sin rediebe,\n",
      "a unque sueve se acompesir,\n",
      "ni que persegue anda estaña\n",
      "aunque compedilas tene.es noprendido.\n",
      "\n",
      "La indiio a sin silimaentone\n",
      "su que era a pobreza ola;\n",
      "tal indio le ha de refan,\n",
      "cuando no tenía,\n",
      "me remodié el que asta:\n",
      "es barrata en la desace.\n",
      "\n",
      "Estábamos están, créomí\n",
      "a las desaciones a de tías,\n",
      "cempezó sola de un humano\n",
      "allí serecía compidado\n",
      "todiva asiverar rujeras,\n",
      "a ya la casta en tan carda\n",
      "me pa sólos, Dios las cueros.\n",
      "\n",
      "Me les pasan los bienel furos\n",
      "como cantan de destacera;\n",
      "las inditos se venalaos\n",
      "parechen y a salidá\n",
      "ya dispor a de sabe sale,\n",
      "y al puede le de desiej a vino\n",
      "la cerco a lao de de hacende este.\n",
      "\n",
      "Ael viejo inetro de flece,\n",
      "como fine desneñaciendo;\n",
      "con es lo que esta manecha.an encon las guaras\n",
      "y salía un cero los aquellasJueras\n",
      "que da pronta cuanto el del jía,\n",
      "sin pasa enJuertas y medá,\n",
      "\n",
      "ya salguno se desieron\n",
      "vando y asu alcasión.\n",
      "Y con el pocho oy dequedó\n",
      "desacao hacendo avisión;\n",
      "que en algho estaban cañino,\n",
      "con las hiciquieros palechen\n",
      "y sin mis salvajes a meno;\n",
      "como enseñaron ande la campa.\n",
      "\n",
      "Juez despájarlo a los\n",
      "me que es dempecundo el bueneo;\n",
      "una tiene cuel comendo\n",
      "como que a inoro de un ampalo,\n",
      "pues en sena cabas en espuela,\n",
      "sin mal indio\n",
      "con la que cuebello con minendá.\n",
      "\n",
      "Vús puede entre que este el cargo\n",
      "ni compietenque vivene lado,\n",
      "porque eran algún que tienene de yo,\n",
      "se diuezo apro al horraribo\n",
      "con junda da en ponza grasca.\n",
      "Este crezciba los mejos,\n",
      "yo hacendo de más volvido.\n",
      "\n",
      "No recetró a pompuna fronerao,\n",
      "y decía me bailaba amarra,\n",
      "el suerto ponco soden su pojas;\n",
      "prece el larguz de que el mundo\n",
      "era loras den el sente.do causa an,\n",
      "\n",
      "turba rojao ci a no tán cuento,\n",
      "sante yo y haciéndolé el golpito\n",
      "como que al paso de consuelo.\n",
      "\n",
      "Lo a tormiente el considido\n",
      "y pues les come que sufriere;\n",
      "la viene los careyos\n",
      "estas capos al infinites\n",
      "nos naides no pecho antenda,\n",
      "que no son piedea paranza\n",
      "con a una la tueña.\n",
      "\n",
      "Saqueda se dispernos carnos\n",
      "cuantos con algá oles llían,\n",
      "con las trociciales\n",
      "que por se arrsegllas,\n",
      "como hasta con se arreguada;\n",
      "y un señol pena el modeo\n",
      "al presto que a medio en un secondino,\n",
      "que no quiera de su niabre\n",
      "la junto que a su arrimbo,\n",
      "cuando los añunos en mundos,\n",
      "lo indio es que ibuera lanza\n",
      "ca falte en el gros de laonición.\n",
      "\n",
      "Mas y se aflenguí nun yiciones\n",
      "con vaneces aquel toldo\n",
      "mas inora coloriente\n",
      "pero la acuplina de pasario,\n",
      "para los dos que adurar,\n",
      "y el chan gaucho lo ha vente.\n",
      "\n",
      "A me halllaré muy pinego.y con es peligros\n",
      "y auda me más de cuoro.\n",
      "es es al plumbo de fanto\n",
      "yen que lo me dejé seb;\n",
      "un de pinfiero que yo es queje,\n",
      "y es me soyo murtas;\n",
      "bueno que al mal forte,\n",
      "y poco al descosa prefía.\n",
      "\n",
      "¡Ah vieja!, tamposies yo,\n",
      "me después oyobazo, y a conocíé,\n",
      "\"duecíatas al bauchería\n",
      "me aché, a parde a pleté.\"\n",
      "Las indio que brebao\n",
      "divieron ni dentre que es buierra;\n",
      "yo no primero y la mataña\n",
      "que no falta en un lazadu sizo:\n",
      "le grata que es ha muerta y una cardo\n",
      "que le soledá Cue volva.\n",
      "\n",
      "Jamás del intioe las pulpas\n",
      "tenero ese se poblane;\n",
      "hacían; los traces se alteza,\n",
      "no estaba jan el cuadento;\n",
      "y cuando a componando ajeno,\n",
      "ni un carco con retoga,\n",
      "empiéndomé a lo aterno.\n",
      "\n",
      "De atropobé gamocién\n",
      "cono es que naceció\n",
      "en el tanta treza,\n",
      "porque alguna muy fiera,\n",
      "para esta que males\n",
      "era ascoso es y lo decía,\n",
      "entre más \"Ñueflecía\"Que ¡Crista\n",
      "\"ansas ampuless aparados!\"\n",
      "\n",
      "¿Cuándo en los daba los cocían\n",
      "que andeaba alzo solbriúdo;\n",
      "\"y sabe puncarle olvén,\n",
      "\"pues me confelejaba,\n",
      "\"Dico de Di no aquel decían\n",
      "\"lo vi a suerte al cuerdo\n",
      "\"es que me quisea muerto\n",
      "\"y debiera pasonermo.\"\n",
      "\n",
      "Por precueste suele\n",
      "desale que alguna muy descafija,\n",
      "pues estábar en el cuero\n",
      "de este alvició de flado,\n",
      "y aquella que pasta me guarrí\n",
      "y tenida era peliquiera mi madre\".\n",
      "\n",
      "\"De amoro de Cesconsu perro\n",
      "de descuridás de dicinar;que es hombre taner\n",
      "pecues estara de pasa arrra\n",
      "y a punta morida;\n",
      "de en modeo del dejade\n",
      "andueá se en estuño cuando,\n",
      "un tampo a vez, con ya es un malito\n",
      "en el carte el palo\n",
      "mo que el que los ojorar.\"\n",
      "\n",
      "\"Z\n",
      "Dejperza el tiempo que yo vena\n",
      "\"porque redaría al suerte\";\n",
      "cuando se sea venía apiardo\n",
      "estaba cuanto adél llevo,\n",
      "DejonóÑamos y ganargato\".\n",
      "\n",
      "Vuelve otro y hasta vezo\n",
      "y el perdío nueve la para:\n",
      "\"Ara esta un mombrano\"\n",
      "que se decía la hampa a prevar,\n",
      "\"y soy me le aperé a contrelllo,\n",
      "\"dije dejo: ¿puse aullante\n",
      "\"como que causa de gabla.\"\n",
      "\n",
      "\"El que a dentre muclato\n",
      "\"vos lo mesmos años contreso;\n",
      "\"y más añor\n",
      "que donque era corro,\n",
      "\"calguno de duro que habén frono\".\"\n",
      "\n",
      "\"Era un gaucho, que lo ocasiones\n",
      "\"porque no había un danje\",\n",
      "\"y aguchares me hechechetá\",\n",
      "\"y para no mediareno:\n",
      "\"a dejeando \"la deciquina\n",
      "\"lle alegre a cuestá!\"\n",
      "\n",
      "\"rece, es rehopéjo más de yoS;\"por al placeco\n",
      "\"que que debir causante\n",
      "\"tal vez en otra horbotro muerto,\n",
      "\"yo me salide de matre\n",
      "\"le dice \"el frogillo.\"\n",
      "De el meCorte  de esermeno\n",
      "\"no soy cá de que desebe.\", que yo me cuento,\n",
      "\n",
      "\"No dejé por la fallpeta\n",
      "\"y se ansé ¡ao a trevis ansí!\"\n",
      "Y siempre el juezs me encuento\n",
      "es a treballo en al friento\";\n",
      "\"le dije.\": -le larguél mundo\n",
      "Y hazo cuando dole herágro.\"\n",
      "\n",
      "\"Lle decía te estoro estaba:\"Has,\n",
      "\"con se destás comedáces,\",\n",
      "\"y áhe de receliesta la adencia\n",
      "\"esta que a está el tierro\".\"\n",
      "\n",
      "Pás voy cuán señorArá,\n",
      "él que que es perseguido:\n",
      "\"habrán en olicio que en su saben\n",
      "\"Un trace soquería;\n",
      "\"que estu corra como los\n",
      "te volvivió a a contra De fresos.\"\n",
      "\n",
      "\"Esta yo es azo de entarero,\n",
      "es harácul de dos laecía;\n",
      "trepues de que están sufrido\n",
      "al colasas no oltitanes;\n",
      "as le disendicia dendence\n",
      "prontos pamientos,\n",
      "lo echan cuando y lanza\n",
      "a de aquel Brudo.\"\n",
      "\n",
      "\"el dice tuve no vasido\n",
      "\"cuando en está me acuarr:\n",
      "no en la visano.\"\n",
      "\n",
      "\"El que era cayor\n",
      "a sufriera lo amiterra:\"Negra de intretencias,\n",
      "y alte yo que no vendé\n",
      "si enora que no toco\n",
      "\"infuina es hacer ánta queretá\".\"\n",
      "\n",
      "\"A naides le mesmanias\n",
      "ale mes senmito que un prató;como en la ferrLa y mesmo,\n",
      "y aurre más que apreséta como,\n",
      "\"porque les dejá de aquel hombre\n",
      "\"lo dejo que en la estreva\".\"-dijo a de legrar\".\n",
      "\n",
      "\"yo nos clamentió que decercazo,\n",
      "\"de tratata al vuez muyodo;\n",
      "\n",
      "la hizo pasa el calel,\n",
      "y e\"a todo en esa sol;\n",
      "despuéz de una eón,\n",
      "que en el valaz se agua.\"\n",
      "\n",
      "\"Tuve a traquel hizora,\"es le da\",\n",
      "y aunque venía ocontro\n",
      "\"ni tengao a boliao,\n",
      "\"con estarles los añones\n",
      "\"y no se heredido que me afliz,\n",
      "\"que ya pronte devoy\n",
      "\"el que se muerte\n",
      "\"en es deseñar que en mi cal.\"\n",
      "\n",
      "\"Dejo cuida yo dizo me cumpla\n",
      "\"por si dormán que de co,\n",
      "\"no puede en eserte mudo,\n",
      "\"y que es dicer chentrados\n",
      "\"todo a perricazuna va vesteces.\"\n",
      "\"Von a ocantescás que males,\n",
      "\"es complato a )\"yo ha que redée descucheca\";\"Seá le dudesechace,\n",
      "salió es causasivente tudasido;\n",
      "\"es déle la amadrenzaron,\n",
      "\"en \"que todo enseo a oclarno\".\"\n",
      "\n",
      "\"el negraté con la ñudita\".\"\n",
      "Despeté caletes,A y alta\n",
      "\"más me valé no de liz Dios,\n",
      "\"pues en un milino que yo\n",
      "\"a me un pampo y me quebrantao.\"\n",
      "\"No le dabe a asacar\n",
      "\"le cardó empezó a un tumbo dijo,\n",
      "\"no que lo sabile en un bierrno\",\n",
      "Y en ansí mata Penitescuarse,\n",
      "la que secre a despuente\n",
      "si no hace que es estaa curerde.\"\n",
      "\n",
      "\"Eec protenen la causaa\n",
      "a veces se no gallar,\n",
      "con uno que desátra una piego,\n",
      "son cuanto en que se encueles,\n",
      "con esas la alimane\n",
      "que el reclejo y con cal dolo,\n",
      "muchas el que en esollía.\"\n",
      "\n",
      "\"Pero otro que nada deja,\"cuando del descuento;\n",
      "soy lo quedao un empisonao\n",
      "ansí salvaré en de hucidao,\n",
      "puess ino más que poce escusta\n",
      "que esos en esta pelas.\", con los mantas\n",
      ": si deján que resegura,\n",
      "porque nuna hay güeya esta le defo\n",
      "\n",
      "y ya \"vermácuándolá de aves.\"\n",
      "\n",
      "\"Más me doléas dos me que que era;\"res, no hay en que decentra\n",
      "\"te jatas que mier na escucentra\",\n",
      "\"y entre nuevo siguro,\n",
      "\"y defe el casino del Sanor.\"\n",
      "\n",
      "Yo que en haje de un amol\n",
      "que sino del cuene asparar;\n",
      "ni dece estar al compente\n",
      "y faleja, comojor al bago\n",
      "y nos digo pueda sea.\"\n",
      "\n",
      "El precimes que yo al viuda\n",
      "a sin dejento más de almas;\n",
      "yo ansí hallía lanza con traba,\n",
      "la crista espeló el malejo,\n",
      "les cohacendo el botienque lo caucha:\n",
      "el pilato de escopelzare;\n",
      "y le dese hacen al vivide\n",
      "y sino tener es ansí;\n",
      "que ansí sabe hablaba irragada.\"\n",
      "\n",
      "\"Uno tregutaábamos,\n",
      "en verlán lrecotidao;\n",
      "y estabao le escupilaba\n",
      "y se alertában todo;\n",
      "se le deblaba arriente,Jugan y lllagar\n",
      "áhi cuanto que es sueño en pegorro,\n",
      "como que se arreda estabe\n",
      "el treye más seño de tue,\n",
      "dentre y esa animago.\n",
      "\n",
      "Motra mi pinvo,\n",
      "erra aquel mesmo me cacargó:\n",
      "su gullpe boligro al camaño\n",
      "si pa que el modo desti¿pa vueño a mejor.\n",
      "\n",
      "Bampa echacho de mostres;\n",
      "trayendo aunque los pasitenes\n",
      "en lo hablan como a cuadror\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = \"Los hermanos sean unidos\" # \"To be, or not to be\"\n",
    "context = torch.tensor(encode(context), dtype=torch.long).unsqueeze(0).to(device)\n",
    "generatedText = decode(model.generate(context, nTokens=10000)[0].tolist())\n",
    "print(generatedText)\n",
    "with open((Path(\"outputs\") / dataset).with_suffix(\".txt\"), \"w\") as f:\n",
    "    f.write(generatedText)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "18P5ELoTJruhjlZCO6gr8afIF5VeWWKT9",
     "timestamp": 1749610753907
    }
   ]
  },
  "kernelspec": {
   "display_name": "transformer-notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
