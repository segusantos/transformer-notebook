{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transformer Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoder-Decoder Architecture**\n",
    "\n",
    "It is used for **sequence-to-sequence** tasks like machine translation.\n",
    "\n",
    "This is the architecture proposed by the original transformer paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. in 2017.\n",
    "\n",
    "<img src=\"assets/encoder_decoder.png\" alt=\"Encoder-Decoder Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decoder-Only Architecture**\n",
    "\n",
    "It is used for **sequence generation** tasks like language modeling.\n",
    "\n",
    "This simpler architecture will be implemented in this notebook from scratch.\n",
    "\n",
    "<img src=\"assets/decoder_only.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scaled Dot-Product Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $Q \\in \\mathbb{R}^{m \\times d_k}$, $K \\in \\mathbb{R}^{n \\times d_k}$ y $V \\in \\mathbb{R}^{n \\times d_v}$ be the query, key and value matrices, respectively. The scaled dot-product attention is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\in \\mathbb{R}^{m \\times d_v}\n",
    "$$\n",
    "\n",
    "<img src=\"assets/self_attention.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddingSize: int, headSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.query = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.value = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(blockSize, blockSize)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        weights = q @ k.transpose(-2, -1) / (C ** 0.5)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        output = weights @ v\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $h$ be the number of heads. The multi-head attention is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\in \\mathbb{R}^{m \\times d_v}\n",
    "$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$ and $W_i^Q \\in \\mathbb{R}^{d_k \\times d_k}$, $W_i^K \\in \\mathbb{R}^{d_k \\times d_k}$, $W_i^V \\in \\mathbb{R}^{d_v \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_v}$ are the learnable parameters.\n",
    "\n",
    "<img src=\"assets/multi_head_attention.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, nHeads: int, embeddingSize: int, headSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([ScaledDotProductAttention(embeddingSize, headSize, blockSize, dropout) for _ in range(nHeads)])\n",
    "        self.projection = nn.Linear(embeddingSize, embeddingSize)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        output = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        output = self.projection(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Position-wise Feed-Forward Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position-wise feed-forward networks are defined as:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "where $W_1 \\in \\mathbb{R}^{d_{ff} \\times d_{model}}$, $b_1 \\in \\mathbb{R}^{d_{ff}}$, $W_2 \\in \\mathbb{R}^{d_{model} \\times d_{ff}}$ and $b_2 \\in \\mathbb{R}^{d_{model}}$ are the learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddingSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embeddingSize, 4 * embeddingSize),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embeddingSize, embeddingSize),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Layer Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layer normalization is defined as:\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sigma} + \\beta\n",
    "$$\n",
    "\n",
    "where $\\gamma \\in \\mathbb{R}^{d_{model}}$ and $\\beta \\in \\mathbb{R}^{d_{model}}$ are the learnable parameters, and $\\mu$ and $\\sigma$ are the mean and standard deviation of $x$, respectively.\n",
    "\n",
    "In the original paper, the layer normalization is applied after the residual connections, whereas in more modern implementations, it is applied before in what is known as **pre-norm formulation**.\n",
    "\n",
    "```python\n",
    "# Original paper\n",
    "x = LayerNorm(x + MultiHeadAttention(x))\n",
    "x = LayerNorm(x + FeedForward(x))\n",
    "\n",
    "# Pre-norm formulation\n",
    "x = x + MultiHeadAttention(LayerNorm(x))\n",
    "x = x + FeedForward(LayerNorm(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "\n",
    "    def __init__(self, nHeads: int, embeddingSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        headSize = embeddingSize // nHeads\n",
    "        self.multiHeadAttention = MultiHeadAttention(nHeads, embeddingSize, headSize, blockSize, dropout)\n",
    "        self.feedForward = FeedForward(embeddingSize, dropout)\n",
    "        self.layerNorm1 = nn.LayerNorm(embeddingSize)\n",
    "        self.layerNorm2 = nn.LayerNorm(embeddingSize)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.multiHeadAttention(self.layerNorm1(x))\n",
    "        x = x + self.feedForward(self.layerNorm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__ (self, vocabularySize: int, nLayers: int, nHeads: int, embeddingSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenEmbeddingTable = nn.Embedding(vocabularySize, embeddingSize)\n",
    "        self.positionEmbeddingTable = nn.Embedding(blockSize, embeddingSize)\n",
    "        self.layers = nn.Sequential(*[Layer(nHeads, embeddingSize, blockSize, dropout) for _ in range(nLayers)])\n",
    "        self.layerNorm = nn.LayerNorm(embeddingSize)\n",
    "        self.linearModelHead = nn.Linear(embeddingSize, vocabularySize)\n",
    "        self.blockSize = blockSize\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor, targets: torch.Tensor = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        B, T = idx.shape\n",
    "        tokenEmbeddings = self.tokenEmbeddingTable(idx)\n",
    "        positionEmbeddings = self.positionEmbeddingTable(torch.arange(T, device=device))\n",
    "        x = tokenEmbeddings + positionEmbeddings\n",
    "        x = self.layers(x)\n",
    "        x = self.layerNorm(x)\n",
    "        logits = self.linearModelHead(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.Tensor, nTokens: int) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(nTokens):\n",
    "                logits, loss = self(idx[:, -self.blockSize:])\n",
    "                logits = logits[:, -1, :]\n",
    "                probabilities = F.softmax(logits, dim=-1)\n",
    "                nextIdx = torch.multinomial(probabilities, num_samples=1)\n",
    "                idx = torch.cat([idx, nextIdx], dim=1)\n",
    "        self.train()\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 187096\n",
      "I\n",
      "\n",
      "Aquí me pongo a cantar\n",
      "al compás de la vigüela,\n",
      "que el hombre que lo desvela\n",
      "una pena estrordinaria,\n",
      "como la ave solitaria\n",
      "con el cantar se consuela.\n",
      "\n",
      "Pido a los santos del cielo\n",
      "que ayuden mi pensamiento:\n",
      "les pido en este momento\n",
      "que voy a cantar\n"
     ]
    }
   ],
   "source": [
    "dataDir = \"data\"\n",
    "file = os.path.join(dataDir, \"martin_fierro.txt\")\n",
    "with open(file, \"r\") as f:\n",
    "    text = f.read()\n",
    "print(f\"Text length: {len(text)}\")\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 72\n",
      "\n",
      " !\"'(),-.:;?ABCDEFGHIJLMNOPQRSTUVXYZabcdefghijklmnopqrstuvxyz¡¿Ñáéíñóúü\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sorted(set(text))\n",
    "vocabularySize = len(vocabulary)\n",
    "print(f\"Vocabulary size: {vocabularySize}\")\n",
    "print(\"\".join(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 51, 55, 1, 44, 41, 54, 49, 37, 50, 51, 55, 1, 55, 41, 37, 50, 1, 57, 50, 45, 40, 51, 55]\n",
      "Los hermanos sean unidos\n"
     ]
    }
   ],
   "source": [
    "stringToToken = {ch: i for i, ch in enumerate(vocabulary)}\n",
    "tokenToString = {i: ch for i, ch in enumerate(vocabulary)}\n",
    "encode = lambda string: [stringToToken[char] for char in string]\n",
    "decode = lambda tokens: \"\".join([tokenToString[token] for token in tokens])\n",
    "print(encode(\"Los hermanos sean unidos\"))\n",
    "print(decode(encode(\"Los hermanos sean unidos\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([187096])\n",
      "Data type: torch.int64\n",
      "tensor([21,  0,  0, 13, 53, 57, 67,  1, 49, 41,  1, 52, 51, 50, 43, 51,  1, 37,\n",
      "         1, 39, 37, 50, 56, 37, 54,  0, 37, 48,  1, 39, 51, 49, 52, 65, 55,  1,\n",
      "        40, 41,  1, 48, 37,  1, 58, 45, 43, 71, 41, 48, 37,  7,  0, 53, 57, 41,\n",
      "         1, 41, 48,  1, 44, 51, 49, 38, 54, 41,  1, 53, 57, 41,  1, 48, 51,  1,\n",
      "        40, 41, 55, 58, 41, 48, 37,  0, 57, 50, 37,  1, 52, 41, 50, 37,  1, 41,\n",
      "        55, 56, 54, 51, 54, 40, 45, 50, 37, 54])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Data type: {data.dtype}\")\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([168386])\n",
      "Validation data shape: torch.Size([18710])\n"
     ]
    }
   ],
   "source": [
    "trainValSplit = 0.9\n",
    "trainSize = int(len(data) * trainValSplit)\n",
    "trainData = data[:trainSize]\n",
    "valData = data[trainSize:]\n",
    "print(f\"Train data shape: {trainData.shape}\")\n",
    "print(f\"Validation data shape: {valData.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.79M parameters\n"
     ]
    }
   ],
   "source": [
    "blockSize = 256\n",
    "embeddingSize = 384\n",
    "nHeads = 6\n",
    "nLayers = 6\n",
    "dropout = 0.2\n",
    "\n",
    "model = GPTLanguageModel(vocabularySize, nLayers, nHeads, embeddingSize, blockSize, dropout).to(device)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(data: torch.Tensor,\n",
    "             batchSize: int,\n",
    "             blockSize: int\n",
    "             ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    ix = torch.randint(0, data.size(0) - blockSize, (batchSize,))\n",
    "    x = torch.stack([data[i:i+blockSize] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+blockSize+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimateLoss(model: nn.Module,\n",
    "                 data: torch.Tensor,\n",
    "                 evalIter: int,\n",
    "                 batchSize: int,\n",
    "                 blockSize: int\n",
    "                ) -> float:\n",
    "    model.eval()\n",
    "    losses = torch.zeros(evalIter)\n",
    "    for i in range(evalIter):\n",
    "        xBatch, yBatch = getBatch(data, batchSize, blockSize)\n",
    "        logits, loss = model(xBatch, yBatch)\n",
    "        losses[i] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Train loss: 4.37283182144165, Val loss: 4.376593112945557\n",
      "Iter: 500, Train loss: 1.8056716918945312, Val loss: 1.8216660022735596\n",
      "Iter: 999, Train loss: 0.9794025421142578, Val loss: 1.5681744813919067\n"
     ]
    }
   ],
   "source": [
    "batchSize = 64\n",
    "learningRate = 3e-4\n",
    "maxIter = 1000\n",
    "evalInterval = 500\n",
    "evalIter = 200\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "for iter in range(maxIter):\n",
    "    if iter % evalInterval == 0 or iter == maxIter - 1:\n",
    "        trainLoss = estimateLoss(model, trainData, evalIter, batchSize, blockSize)\n",
    "        valLoss = estimateLoss(model, valData, evalIter, batchSize, blockSize)\n",
    "        print(f\"Iter: {iter}, Train loss: {trainLoss}, Val loss: {valLoss}\")\n",
    "    xBatch, yBatch = getBatch(trainData, batchSize, blockSize)\n",
    "    logits, loss = model(xBatch, yBatch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "modelsDir = \"models\"\n",
    "torch.save(model.state_dict(), os.path.join(modelsDir, \"martin_fierro.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los hermanos sean unidos\n",
      "que nos más pobres que una últifición,\n",
      "para más mucho de un rerecho\n",
      "como una ajuel más...\n",
      "\n",
      "Yo no gato no queré\n",
      "cuando el canto se atenció;\n",
      "lo vista en la ocasión\n",
      "para andar dás.\n",
      "Me gente mi solza\n",
      "y lo ponga sus manejos;\n",
      "pero Dios haron de vistos\n",
      "y perdonó el chiquito es manes.\n",
      "úspren los que azon pierdo\n",
      "que los es pesaren el malento,\n",
      "Y que es el indio los berral\n",
      "dende que es amorenan.\n",
      "\n",
      "Y ahi con esta el mior\n",
      "fue, altante con la rodel;\n",
      "no he punto el encordidan,\n",
      "empeñéan que allí tratapao\n",
      "por vengar nuestracia.\n",
      "\n",
      "\n",
      "En esa ima flortera\n",
      "con mi gana pao andar,\n",
      "y un salvaje medio flordar\n",
      "y aunque me me ata amagocía;\n",
      "por no me pude no arracha\n",
      "a saber algún memoro.\n",
      "\n",
      "Una hay veces me atropel\n",
      "gritando me he de eseguida;\n",
      "aunque ya veo me afroja\n",
      "de enseñarle a hude noche\n",
      "iba a dista corrarer\n",
      "de grina, me me enseñura,\n",
      "cuando no hablar al calma,\n",
      "es poneces al corraje.\n",
      "\n",
      "\n",
      "Y en aquel puerte negro\n",
      "le echaron corrorror,\n",
      "mas como más corroro;\n",
      "soy en es rogro ganaos;\n",
      "tan gamas si el corrél;\n",
      "y los ojos o que penan contos\n",
      "dieron con antes carnes.\n",
      "\n",
      "Juntos reciendas metas\n",
      "como no prete el Juez;\n",
      "lo pasaren los mevos\n",
      "y jergatiando y esplico,\n",
      "peroneciéndomé a ablaviendo\n",
      "esperane y que yablarse yo.\n",
      "\n",
      "Y quedó corgulenta el maldito\n",
      "y por estuve a de hacición\n",
      "pero nos dejamos años\n",
      "estuve andarr mi parado.\n",
      "\n",
      "No me contesigó mi Ros\n",
      "en me atar a orriar;\n",
      "en al paversemo seguira:\n",
      "el campia me frento\n",
      "si me halló a más anmigo;\n",
      "cada que les arreciman\n",
      "el bolazo de un modo.\n",
      "\n",
      "\n",
      "Nide cuando gaucho oroso\n",
      "pero el asunto me peliar),\n",
      "y aunque no había y tengar\n",
      "aunque cuando de mío\n",
      "me todo el mía el aponcho,\n",
      "y me han decir el errie.\n",
      "\n",
      "\n",
      "Es a cuadro a podecía\n",
      "el noche de mayor;\n",
      "es tuve a escaprarle rudaba\n",
      "y que había dejao un boca\n",
      "diciéndomé la la enche..\n",
      "\n",
      "La vacia a seguir\n",
      "el mescononecío:\n",
      "el aqueo es gasta algún\n",
      "con el yuego crezía\n",
      "y aguardanzado venía a duro.\n",
      "\n",
      "Ahí mío me conocíí\n",
      "y al todo grandita,\n",
      "me hizo muy mi hijo a padera;\n",
      "nunca he venir lo a hermana,\n",
      "aunque ande la bruma y para\n",
      "\n",
      "hay enlla mesma nos envenan.\n",
      "\n",
      "XII\n",
      "\n",
      "Yo no pensé el perho\n",
      "en ninguna mayor,\n",
      "nadaba olvidao la dota.\n",
      "Me larghé la veda a tapro\n",
      "cuando el indio y me arrecebra;\n",
      "y, la gente al hina de china\n",
      "en la cusa a de verir su palo\n",
      "por bajo el cuara.\n",
      "\n",
      "Tomo el que con le esimperan\n",
      "jamás los dos dientos,\n",
      "y tal vez acual salva\n",
      "hasta que con sus eolices;\n",
      "vamos anda del camparto\n",
      "lo verme, por suplicado.\n",
      "\n",
      "Dende esa cosasión vender\n",
      "ade esta pechos asonado,\n",
      "en dos loros y lletaban\n",
      "y al fin de sacararlo al primerno,\n",
      "pero yo en su velarion\n",
      "aunque de tarle lo juera entirra\n",
      "sinó en cuanto nueva parere.\n",
      "\n",
      "Con una hijeras no dongracia\n",
      "y críamendamos al pobre hastacao;\n",
      "que cada que al meto me asaprecida\n",
      "de la bolanción resuelta\n",
      "de campa y andecl por ajenas.\n",
      "\n",
      "a un mal oración,\n",
      "ningun arrigllar en bolras\n",
      "y campiara de vientos;\n",
      "yo es me hago el por Cruero\n",
      "que no acho alguna cruera;\n",
      "me tapa brazaos y aproviendos\n",
      "con pronta estación.\n",
      "\n",
      "\n",
      "Con el pobre no me dir\n",
      "el camino me aterro;\n",
      "bolian solo en la brita\n",
      "y me faltó una grita,\n",
      "y al punta mi aligina\n",
      "\n",
      "se puse se acan adenaria enterna.\n",
      "\n",
      "Tan cuando había amarna\n",
      "por esto pió el dín parado;\n",
      "en este las gónos dían\n",
      "pa más he de vasallonar;\n",
      "pero hasta al no atropencia\n",
      "las que ha ovidao dentrar;\n",
      "mas darme como ratar\n",
      "y se ha de conmendarme adrar.\n",
      "\n",
      "\n",
      "A recorrer entrio los vieras\n",
      "con alguna doy crisliana,\n",
      "la projer me el asunto perro:\n",
      "no podía el facer grito\n",
      "el por lo cieta a en le siente.\n",
      "\n",
      "Con el cluca ya verive ningreta,\n",
      "y ya a quía no ni mandar\n",
      "ese quedaba cantar,\n",
      "no había ningún perdida\n",
      "espué de atentes nuicias.\n",
      "\n",
      "\n",
      "Pero allí hombre consueles\n",
      "andabí el escordador;\n",
      "y allí había arre boliao,\n",
      "¡coniendas le alma cenchar\n",
      "o puede, él matura escapado.\n",
      "\n",
      "Tenía imi pedirme\n",
      "a manece a Son,\n",
      "se huce el deser,Dnde pues hermanao;\n",
      "pues al veces no favor\n",
      "lo hallla de un lado uera pUerz\n",
      "que con la Juez a dolos.\n",
      "\n",
      "Indije bando all cuerpo\n",
      "le pasaba el hormido,\n",
      "la prenderme entre mi ceso\n",
      "tal vez en una fiera\n",
      "que seata la amorención.\n",
      "\n",
      "\n",
      "Adaba atrota es su arde\n",
      "y algún lengua ye pasara;\n",
      "como el tierra cualquiera,\n",
      "sobre, allí en su deberate,\n",
      "de pusierando yo vete,\n",
      "no hay regrecias notes\n",
      "de todo amansejarle bllavan.\n",
      "\n",
      "\n",
      "Yo atropelleman se encuentra\n",
      "con adeuelo que había aguagentao\n",
      "y andando me larguén\n",
      "con el austo me colgó;\n",
      "y salía se atrever\n",
      "y las hijos peliandas y añunas.\n",
      "\n",
      "X\n",
      "\n",
      "\"Siempre allí cantar\n",
      "como la manejetaba:\n",
      "con estuve a oficiaos,\n",
      "algó pelició a el sadoto\n",
      "de aquel fin se diera,\n",
      "y aunque es mi afetel\n",
      "en arreglar de la dejusca,\n",
      "hiciéndoé muerto cuanto\n",
      "y lo alzaban cuando aventa.\n",
      "\n",
      "\n",
      "Pero en aquella projudara\n",
      "cuando estaba venir;\n",
      "se había conocido,\n",
      "y me hacía de andarme,\n",
      "y al hindo me dije,\n",
      "pues no que se sé alcarde,\n",
      "sin aquel cuerpo y mi cuerde,\n",
      "no tenía de cuando gran Piede\n",
      "pero naides salenaria,\n",
      "\n",
      "el pasan hay tan herejías.\n",
      "\n",
      "\n",
      "Y nadaba servaciao\n",
      "como una erancho,\n",
      "y es boarditá el toro\n",
      "la despotía el resenquer;\n",
      "el su casidáa ni ambla\n",
      "porque no tiene crerr.\n",
      "\n",
      "\n",
      "El ande de soloriario esperanza,\n",
      "buscan la frontera\n",
      "y cruerso sus adecir,\n",
      "todas la mirada\n",
      "el efación la la istancia;\n",
      "en mi pasa tantaba,\n",
      "aunque casana se venía\n",
      "ato con uno se venía.\n",
      "\n",
      "\n",
      "Empecé su hicen la vastencia\n",
      "de salengrao a orrer;\n",
      "y son poco ojo el berrro,\n",
      "a muy flormarme tanta grano,\n",
      "sin concluar verdanio el mandando\n",
      "digo la guana en partida.\n",
      "\n",
      "\n",
      "(icMento el indio al sentino\n",
      "en terriban el cantón:\n",
      "al alma gana mangana\n",
      "las hee sona enterraos\n",
      "y en sus más inoras,\n",
      "y algunos de unones penas\n",
      "haciendos de empeñaran.\n",
      "\n",
      "Ahi madre mostrarse esistancia\n",
      "cuando imi Oficial se altrona;\n",
      "el rodito con la pagra\n",
      "hasta que esperamos no agarra,\n",
      "tristas, y los correrías\n",
      "me colgarró una pidicia.\n",
      "\n",
      "\n",
      "Y dejé adurar escaponer\n",
      "que andaba en la punta tanida,\n",
      "alcíamocíamos en el seño\n",
      "que aprende aquel indio se bendita;\n",
      "son falta como lamente\n",
      "el miraron en el dos campos.\n",
      "\n",
      "Hacen proma el promero\n",
      "en aquella alime al cuchuzlo;\n",
      "me aquéllo era maldito\n",
      "que el miso encuena pedrida;\n",
      "siempre es mi corraje\n",
      "no había aides amiginarme\n",
      "cuando se acanda ay quejan\n",
      "el frendaba estabar.\n",
      "\n",
      "\n",
      "Y en trance al nacia\n",
      "por el indio se escufriz;\n",
      "y, con tompo sufrir\n",
      "cuando me peliarme,\n",
      "y esa daba aprendite.\n",
      "\n",
      "De lo que acontes me tues\n",
      "vino mata muerte;\n",
      "vené a mi histos carmesias\n",
      "me tenía a su mandar,\n",
      "y yansí y ya enconmiénes\n",
      "me había cordao perrsir.\n",
      "\n",
      "\n",
      "IIntra avedes los brantos\n",
      "a ganas juimos me atró;\n",
      "cuando pa el alma muy leta\n",
      "yo he de otro y grostos;\n",
      "me espué cuando medio muerto\n",
      "se tenía el pobre el tirate.\n",
      "\n",
      "\n",
      "Yo tenía cuando ros;\n",
      "el así mi gatarremos,\n",
      "bramaras al están apuraos\n",
      "es tres azón pa partidaos\n",
      "\"cuando estaba el presente.\"\n",
      "\n",
      "Todita es junto brato\n",
      "el pelique venga en razar;\n",
      "yo se andía y es algía\n",
      "la algún heche bolita y en el veces,\n",
      "de naides entre todos\n",
      "por está temirno brando.\n",
      "\n",
      "Y no solidos con pa muerto\n",
      "el juezamos a la guete;\n",
      "si hay su dejamos día\n",
      "con la penamiente en el conte\n",
      "\n",
      "a ver ande a los pastes\n",
      "con pacen desconaos.\n",
      "\n",
      "Pero aunque es precimero\n",
      "tuve salvacao por el sucero,\n",
      "lo no apreté desnués\n",
      "me resóó el campasión al rente,\n",
      "y dentra con la hombre mustente\n",
      "y ya pa el lao la encemanza.\n",
      "\n",
      "Al ver más perriban apuersa\n",
      "al correrle en el sentar;\n",
      "yo por no hay en esto elerdar\n",
      "aunque amarrarse en algúncarro.\n",
      "y el gringo cruerda esgracia\n",
      "de punto a un asobre se aler\n",
      "conocía a el primero suplico.\n",
      "\n",
      "Hay muchos carrer el serveno\n",
      "cuando hay una descuchaciénda!\n",
      "que es junta en peliado\n",
      "sin poderme no hace anima;\n",
      "soy se dechan le has amfrión\n",
      "hace palosas de dechés.\n",
      "\n",
      "\n",
      "Tiempo con el potranción\n",
      "a por el facón aquéllo brotao;\n",
      "como el único de un bato\n",
      "y el todo el tía guapeta uerra,\n",
      "en lugúra al hombre el juer\n",
      "y era jugadar el canto.\n",
      "\n",
      "Una enl comendaza pelo\n",
      "y me atropellló a gusta,\n",
      "y con una repunta a mí\n",
      "y hacerme la cuada arre,\n",
      "\n",
      "y allí estabiao a aquel protace\n",
      "encerrajado parte un nace.\n",
      "\n",
      "Déban calle a todo a encuero\n",
      "porque trota los quejeros;\n",
      "és bárbaro callos\n",
      "que al yo uno le peludante;\n",
      "conocían paues, seguido patede\n",
      "vino contemidá, sueles asiento\n",
      "\n",
      "a cabarr el hacen aquel al versiento,\n",
      "y de adurarle el juego\n",
      "lo haga como adentro el día.\n",
      "\n",
      "¡Y con lo en contores\n",
      "aunta fante se ablanda\n",
      "en sus viera cualidár,\n",
      "que no salta con gimina,\n",
      "con páamos a hacerme pronta\n",
      "cuando lo plana en con el prima\n",
      "el hombre sin malicia encima.\n",
      "\n",
      "\n",
      "Con los miros se me almó\n",
      "y son ser algreta más;\n",
      "los ciego que es dispues ado\n",
      "si que sabe hacer con ellices;\n",
      "nada anda allías de esgún,\n",
      "le amor con desespués\n",
      "pecen aquel quere no parase.\n",
      "\n",
      "Allí se alguir correr\n",
      "sin dejando ganampecera,\n",
      "nada vágaca andar\n",
      "tuve olvidaos a hechao.\n",
      "\n",
      "Ora juntos los años\n",
      "le como pán que acer;\n",
      "sólo es gata es alta primera,\n",
      "y aprendí ande, las bascas,\n",
      "dende que diera ciertas.\n",
      "\n",
      "\n",
      "Es clució que cantiar,\n",
      "un abulito pinga el indio,\n",
      "y podían a esa poncia\n",
      "y en el tiempo grito,\n",
      "pero en mi punta tan cruela,\n",
      "un triste que dejaba\n",
      "cuando acabó mi arrerse.\n",
      "\n",
      "\n",
      "Vamó andaba criao\n",
      "pues arme encuentro la facha,\n",
      "y no plega se tenía\n",
      "con un ingua escapata el esperano,\n",
      "y el día no hay ponerme vano\n",
      "como cuando el bandita,\n",
      "con todo el pina hervenia,\n",
      "mis hecho sin por aquellas.\n",
      "\n",
      "\n",
      "Sin darme buscar tomas\n",
      "las veces, como cuguer.\n",
      "ádamos le dije:\n",
      "nunca mías ibuen lado,\n",
      "lo tiene todo el Que te errdido\n",
      "debe negrar ande este cabeza:\n",
      "y que allí todo envierano\n",
      "cuando cuando ve bolazo.\n",
      "\n",
      "\n",
      "Se se intirita la os manses\n",
      "como a juerza al santo;\n",
      "lo están carbarlo y llenas\n",
      "se tal que en desesparacia,\n",
      "y ver si ha podido prando,\n",
      "éreciéndomeló estaución.\n",
      "\n",
      "\n",
      "No me escapré barigoso\n",
      "aquél mirar maneciar\n",
      "y la hingo el tante projun;\n",
      "si nada olvides dando\n",
      "y siempre que aprende ser,\n",
      "y vi en cuando regrarse,\n",
      "con plaican a trota\n",
      "de terrse o encuentra más.\n",
      "\n",
      "Sin decir cual campo supara\n",
      "con el aimado de ponchas;\n",
      "matas le toce a marrejan,\n",
      "y no salvarir esan granquera\n",
      "en los que enseguir\n",
      "en cansando algún trago.\n",
      "\n",
      "\n",
      "Ya menandaba hociendos\n",
      "me lueve agarrar;\n",
      "y eso a mostrar milar,\n",
      "naides fue y era vecir\n",
      "aunque me falta en el aimenso.\n",
      "Ya esa mi no le darna,\n",
      "de fondonar con la espana:\n",
      "sacara aquella los cielan\n",
      "sin pones de cener.\n",
      "\n",
      "\n",
      "Para campa en los doren\n",
      "cuando el blando lo engros\n",
      "y es todo en mis anos;\n",
      "es cho amilo de ponersarso\n",
      "y le asonté vergaciao.\n",
      "\n",
      "En los hijos a padre\n",
      "con el primero el frente,\n",
      "pero fXin yo\n",
      "formhé serpermo esperancia;\n",
      "y nun\n"
     ]
    }
   ],
   "source": [
    "context = \"Los hermanos sean unidos\"\n",
    "context = torch.tensor(encode(context), dtype=torch.long).unsqueeze(0).to(device)\n",
    "generatedText = decode(model.generate(context, nTokens=10000)[0].tolist())\n",
    "print(generatedText)\n",
    "outputDir = \"outputs\"\n",
    "with open(os.path.join(outputDir, \"martin_fierro.txt\"), \"w\") as f:\n",
    "    f.write(generatedText)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
