{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transformer Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoder-Decoder Architecture**\n",
    "\n",
    "It is used for **sequence-to-sequence** tasks like machine translation.\n",
    "\n",
    "This is the architecture proposed by the original transformer paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. in 2017.\n",
    "\n",
    "<img src=\"assets/encoder_decoder.png\" alt=\"Encoder-Decoder Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decoder-Only Architecture**\n",
    "\n",
    "It is used for **sequence generation** tasks like language modeling.\n",
    "\n",
    "This simpler architecture will be implemented in this notebook from scratch.\n",
    "\n",
    "<img src=\"assets/decoder_only.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scaled Dot-Product Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $Q \\in \\mathbb{R}^{m \\times d_k}$, $K \\in \\mathbb{R}^{n \\times d_k}$ y $V \\in \\mathbb{R}^{n \\times d_v}$ be the query, key and value matrices, respectively. The scaled dot-product attention is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\in \\mathbb{R}^{m \\times d_v}\n",
    "$$\n",
    "\n",
    "<img src=\"assets/self_attention.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddingSize: int, headSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.query = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.value = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(blockSize, blockSize)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        weights = q @ k.transpose(-2, -1) / (C ** 0.5)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        output = weights @ v\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $h$ be the number of heads. The multi-head attention is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\in \\mathbb{R}^{m \\times d_v}\n",
    "$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$ and $W_i^Q \\in \\mathbb{R}^{d_k \\times d_k}$, $W_i^K \\in \\mathbb{R}^{d_k \\times d_k}$, $W_i^V \\in \\mathbb{R}^{d_v \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_v}$ are the learnable parameters.\n",
    "\n",
    "<img src=\"assets/multi_head_attention.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, nHeads: int, embeddingSize: int, headSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([ScaledDotProductAttention(embeddingSize, headSize, blockSize, dropout) for _ in range(nHeads)])\n",
    "        self.projection = nn.Linear(embeddingSize, embeddingSize)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        output = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        output = self.projection(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Position-wise Feed-Forward Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position-wise feed-forward networks are defined as:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "where $W_1 \\in \\mathbb{R}^{d_{ff} \\times d_{model}}$, $b_1 \\in \\mathbb{R}^{d_{ff}}$, $W_2 \\in \\mathbb{R}^{d_{model} \\times d_{ff}}$ and $b_2 \\in \\mathbb{R}^{d_{model}}$ are the learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddingSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embeddingSize, 4 * embeddingSize),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embeddingSize, embeddingSize),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Layer Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layer normalization is defined as:\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sigma} + \\beta\n",
    "$$\n",
    "\n",
    "where $\\gamma \\in \\mathbb{R}^{d_{model}}$ and $\\beta \\in \\mathbb{R}^{d_{model}}$ are the learnable parameters, and $\\mu$ and $\\sigma$ are the mean and standard deviation of $x$, respectively.\n",
    "\n",
    "In the original paper, the layer normalization is applied after the residual connections, whereas in more modern implementations, it is applied before in what is known as **pre-norm formulation**.\n",
    "\n",
    "```python\n",
    "# Original paper\n",
    "x = LayerNorm(x + MultiHeadAttention(x))\n",
    "x = LayerNorm(x + FeedForward(x))\n",
    "\n",
    "# Pre-norm formulation\n",
    "x = x + MultiHeadAttention(LayerNorm(x))\n",
    "x = x + FeedForward(LayerNorm(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "\n",
    "    def __init__(self, nHeads: int, embeddingSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        headSize = embeddingSize // nHeads\n",
    "        self.multiHeadAttention = MultiHeadAttention(nHeads, embeddingSize, headSize, blockSize, dropout)\n",
    "        self.feedForward = FeedForward(embeddingSize, dropout)\n",
    "        self.layerNorm1 = nn.LayerNorm(embeddingSize)\n",
    "        self.layerNorm2 = nn.LayerNorm(embeddingSize)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.multiHeadAttention(self.layerNorm1(x))\n",
    "        x = x + self.feedForward(self.layerNorm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__ (self, vocabularySize: int, nLayers: int, nHeads: int, embeddingSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenEmbeddingTable = nn.Embedding(vocabularySize, embeddingSize)\n",
    "        self.positionEmbeddingTable = nn.Embedding(blockSize, embeddingSize)\n",
    "        self.layers = nn.Sequential(*[Layer(nHeads, embeddingSize, blockSize, dropout) for _ in range(nLayers)])\n",
    "        self.layerNorm = nn.LayerNorm(embeddingSize)\n",
    "        self.linearModelHead = nn.Linear(embeddingSize, vocabularySize)\n",
    "        self.blockSize = blockSize\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor, targets: torch.Tensor = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        B, T = idx.shape\n",
    "        tokenEmbeddings = self.tokenEmbeddingTable(idx)\n",
    "        positionEmbeddings = self.positionEmbeddingTable(torch.arange(T, device=device))\n",
    "        x = tokenEmbeddings + positionEmbeddings\n",
    "        x = self.layers(x)\n",
    "        x = self.layerNorm(x)\n",
    "        logits = self.linearModelHead(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.Tensor, nTokens: int) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(nTokens):\n",
    "                logits, loss = self(idx[:, -self.blockSize:])\n",
    "                logits = logits[:, -1, :]\n",
    "                probabilities = F.softmax(logits, dim=-1)\n",
    "                nextIdx = torch.multinomial(probabilities, num_samples=1)\n",
    "                idx = torch.cat([idx, nextIdx], dim=1)\n",
    "        self.train()\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 187095\n",
      "I\n",
      "\n",
      "Aquí me pongo a cantar\n",
      "al compás de la vigüela,\n",
      "que el hombre que lo desvela\n",
      "una pena estrordinaria,\n",
      "como la ave solitaria\n",
      "con el cantar se consuela.\n",
      "\n",
      "Pido a los santos del cielo\n",
      "que ayuden mi pensamiento:\n",
      "les pido en este momento\n",
      "que voy a cantar\n"
     ]
    }
   ],
   "source": [
    "dataDir = \"data\"\n",
    "file = os.path.join(dataDir, \"martin_fierro.txt\")\n",
    "with open(file, \"r\") as f:\n",
    "    text = f.read()\n",
    "print(f\"Text length: {len(text)}\")\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 72\n",
      "\n",
      " !\"'(),-.:;?ABCDEFGHIJLMNOPQRSTUVXYZabcdefghijklmnopqrstuvxyz¡¿Ñáéíñóúü\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sorted(set(text))\n",
    "vocabularySize = len(vocabulary)\n",
    "print(f\"Vocabulary size: {vocabularySize}\")\n",
    "print(\"\".join(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 51, 55, 1, 44, 41, 54, 49, 37, 50, 51, 55, 1, 55, 41, 37, 50, 1, 57, 50, 45, 40, 51, 55]\n",
      "Los hermanos sean unidos\n"
     ]
    }
   ],
   "source": [
    "stringToToken = {ch: i for i, ch in enumerate(vocabulary)}\n",
    "tokenToString = {i: ch for i, ch in enumerate(vocabulary)}\n",
    "encode = lambda string: [stringToToken[char] for char in string]\n",
    "decode = lambda tokens: \"\".join([tokenToString[token] for token in tokens])\n",
    "print(encode(\"Los hermanos sean unidos\"))\n",
    "print(decode(encode(\"Los hermanos sean unidos\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([187095])\n",
      "Data type: torch.int64\n",
      "tensor([21,  0,  0, 13, 53, 57, 67,  1, 49, 41,  1, 52, 51, 50, 43, 51,  1, 37,\n",
      "         1, 39, 37, 50, 56, 37, 54,  0, 37, 48,  1, 39, 51, 49, 52, 65, 55,  1,\n",
      "        40, 41,  1, 48, 37,  1, 58, 45, 43, 71, 41, 48, 37,  7,  0, 53, 57, 41,\n",
      "         1, 41, 48,  1, 44, 51, 49, 38, 54, 41,  1, 53, 57, 41,  1, 48, 51,  1,\n",
      "        40, 41, 55, 58, 41, 48, 37,  0, 57, 50, 37,  1, 52, 41, 50, 37,  1, 41,\n",
      "        55, 56, 54, 51, 54, 40, 45, 50, 37, 54])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Data type: {data.dtype}\")\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([168385])\n",
      "Validation data shape: torch.Size([18710])\n"
     ]
    }
   ],
   "source": [
    "trainValSplit = 0.9\n",
    "trainSize = int(len(data) * trainValSplit)\n",
    "trainData = data[:trainSize]\n",
    "valData = data[trainSize:]\n",
    "print(f\"Train data shape: {trainData.shape}\")\n",
    "print(f\"Validation data shape: {valData.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context batch shape: torch.Size([4, 8])\n",
      "tensor([[ 3, 60,  1, 55, 41,  1, 48, 51],\n",
      "        [48, 51, 11,  0,  0, 49, 41, 56],\n",
      "        [ 1, 52, 37, 55, 37, 54,  9,  0],\n",
      "        [45, 46, 51,  1, 41, 55, 37,  1]], device='cuda:0')\n",
      "Target batch shape: torch.Size([4, 8])\n",
      "tensor([[60,  1, 55, 41,  1, 48, 51,  1],\n",
      "        [51, 11,  0,  0, 49, 41, 56, 45],\n",
      "        [52, 37, 55, 37, 54,  9,  0,  0],\n",
      "        [46, 51,  1, 41, 55, 37,  1, 51]], device='cuda:0')\n",
      "Context: [3] -> Target: 60\n",
      "Context: [3, 60] -> Target: 1\n",
      "Context: [3, 60, 1] -> Target: 55\n",
      "Context: [3, 60, 1, 55] -> Target: 41\n",
      "Context: [3, 60, 1, 55, 41] -> Target: 1\n",
      "Context: [3, 60, 1, 55, 41, 1] -> Target: 48\n",
      "Context: [3, 60, 1, 55, 41, 1, 48] -> Target: 51\n",
      "Context: [3, 60, 1, 55, 41, 1, 48, 51] -> Target: 1\n",
      "Context: [48] -> Target: 51\n",
      "Context: [48, 51] -> Target: 11\n",
      "Context: [48, 51, 11] -> Target: 0\n",
      "Context: [48, 51, 11, 0] -> Target: 0\n",
      "Context: [48, 51, 11, 0, 0] -> Target: 49\n",
      "Context: [48, 51, 11, 0, 0, 49] -> Target: 41\n",
      "Context: [48, 51, 11, 0, 0, 49, 41] -> Target: 56\n",
      "Context: [48, 51, 11, 0, 0, 49, 41, 56] -> Target: 45\n",
      "Context: [1] -> Target: 52\n",
      "Context: [1, 52] -> Target: 37\n",
      "Context: [1, 52, 37] -> Target: 55\n",
      "Context: [1, 52, 37, 55] -> Target: 37\n",
      "Context: [1, 52, 37, 55, 37] -> Target: 54\n",
      "Context: [1, 52, 37, 55, 37, 54] -> Target: 9\n",
      "Context: [1, 52, 37, 55, 37, 54, 9] -> Target: 0\n",
      "Context: [1, 52, 37, 55, 37, 54, 9, 0] -> Target: 0\n",
      "Context: [45] -> Target: 46\n",
      "Context: [45, 46] -> Target: 51\n",
      "Context: [45, 46, 51] -> Target: 1\n",
      "Context: [45, 46, 51, 1] -> Target: 41\n",
      "Context: [45, 46, 51, 1, 41] -> Target: 55\n",
      "Context: [45, 46, 51, 1, 41, 55] -> Target: 37\n",
      "Context: [45, 46, 51, 1, 41, 55, 37] -> Target: 1\n",
      "Context: [45, 46, 51, 1, 41, 55, 37, 1] -> Target: 51\n"
     ]
    }
   ],
   "source": [
    "def getBatch(data: torch.Tensor,\n",
    "             batchSize: int,\n",
    "             blockSize: int\n",
    "             ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    ix = torch.randint(0, data.size(0) - blockSize, (batchSize,))\n",
    "    x = torch.stack([data[i:i+blockSize] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+blockSize+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "batchSize = 4 # Sequences in parallel\n",
    "blockSize = 8 # Sequence length\n",
    "\n",
    "xBatch, yBatch = getBatch(trainData, batchSize, blockSize)\n",
    "\n",
    "print(f\"Context batch shape: {xBatch.shape}\")\n",
    "print(xBatch)\n",
    "print(\"Target batch shape:\", yBatch.shape)\n",
    "print(yBatch)\n",
    "for b in range(batchSize):\n",
    "    for t in range(blockSize):\n",
    "        context = xBatch[b, :t+1].tolist()\n",
    "        target = yBatch[b, t].item()\n",
    "        print(f\"Context: {context} -> Target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.79M parameters\n"
     ]
    }
   ],
   "source": [
    "blockSize = 256\n",
    "embeddingSize = 384\n",
    "nHeads = 6\n",
    "nLayers = 6\n",
    "dropout = 0.2\n",
    "\n",
    "model = GPTLanguageModel(vocabularySize, nLayers, nHeads, embeddingSize, blockSize, dropout).to(device)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Train loss: 4.33938455581665, Val loss: 4.339482307434082\n",
      "Iter: 500, Train loss: 1.8174996376037598, Val loss: 1.829372525215149\n",
      "Iter: 999, Train loss: 0.9472612142562866, Val loss: 1.5820138454437256\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimateLoss(model: nn.Module,\n",
    "                 data: torch.Tensor,\n",
    "                 evalIter: int,\n",
    "                 batchSize: int,\n",
    "                 blockSize: int\n",
    "                ) -> float:\n",
    "    model.eval()\n",
    "    losses = torch.zeros(evalIter)\n",
    "    for i in range(evalIter):\n",
    "        xBatch, yBatch = getBatch(data, batchSize, blockSize)\n",
    "        logits, loss = model(xBatch, yBatch)\n",
    "        losses[i] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean().item()\n",
    "\n",
    "\n",
    "batchSize = 64\n",
    "learningRate = 3e-4\n",
    "maxIter = 1000\n",
    "evalInterval = 500\n",
    "evalIter = 200\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "for iter in range(maxIter):\n",
    "    if iter % evalInterval == 0 or iter == maxIter - 1:\n",
    "        trainLoss = estimateLoss(model, trainData, evalIter, batchSize, blockSize)\n",
    "        valLoss = estimateLoss(model, valData, evalIter, batchSize, blockSize)\n",
    "        print(f\"Iter: {iter}, Train loss: {trainLoss}, Val loss: {valLoss}\")\n",
    "    xBatch, yBatch = getBatch(trainData, batchSize, blockSize)\n",
    "    logits, loss = model(xBatch, yBatch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "modelsDir = \"models\"\n",
    "torch.save(model.state_dict(), os.path.join(modelsDir, \"martin_fierro.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los hermanos sean unidos\n",
      "en los dios muy queridos.\n",
      "\n",
      "Sólo nos se alvan moros\n",
      "viniendo nos saben manejar;\n",
      "no sé nos lágrimos quejas\n",
      "cuál árbaro cruños del ejos,\n",
      "sinó siempre quiere soldo,\n",
      "sin dejarlo dolorarlon.\n",
      "\n",
      "El nacio ninguna viejo\n",
      "ni la cosas no negroso:\n",
      "le habían ejeús, empaño un dor\n",
      "\"hasta que allí el vez projundo,\n",
      "\n",
      "\"a la llevamos a a vuelta\n",
      "\"vuno a contestro tan años\n",
      "\"con el haciendo una comasión.\"\n",
      "\n",
      "\"Esto bien los brazos y blanco\n",
      "que a mándará el del carrón:\n",
      "\"se más me dijo y un redo\n",
      "\"en los haces de has De tor calto;\n",
      "\"es el vez por más mejor que el jo\n",
      "a todo como enfantro y.\"\n",
      "\n",
      "\"y esa milos de corró gritos\n",
      "buscando el vierto fasaoños\n",
      "que allí entre de vez baer\n",
      "tomar y se polvidá:\n",
      "todo eja cuando en todo\n",
      "\"luego na llevaban con fiera.\"\n",
      "\n",
      "\"Ansé me dar pasar\n",
      "cordar el indio fablate,\n",
      "y le dio tanto varéta\n",
      "\"la da ganas una carrera.\"\n",
      "\n",
      "\"RecoDeno el que bien los cantos\n",
      "\"nos decidito ausentos!\"\n",
      "\"nunca media fierronal,\n",
      "\"porque todos todos bescapante\n",
      "\"vano el gaucho.\"\n",
      "\n",
      "\"En tanto deste ha dar\n",
      "en estirpendido,\n",
      "\"que andápelNan un doldito,\n",
      "\"a ser lo que echan contidos\n",
      "\"pero es maya con el lanzas.\"\n",
      "\n",
      "Las mujeres te hasta apladas,\n",
      "remosé echan la barrata\n",
      "y al cristián, cuando y hacía,\n",
      "en al prende se animos\n",
      "y me satiro en el tirón.\n",
      "\n",
      "Se lo mesmo digo amigo\n",
      "que un sinficia en vago;\n",
      "con el fanzo el goro,\n",
      "mientras como singuiero;\n",
      "mas siempre mal el calomo,\n",
      "es milos angunancia y golpia\n",
      "y el mirarlo de ese perro\n",
      "un polpes me a las lanas.\n",
      "\n",
      "En las cuarpes, compos del más\n",
      "el mal nombre hoy se atirdo;\n",
      "en el indio he se encuadro,\n",
      "y ninguno se negro,\n",
      "porque el ampa ejarguera.\n",
      "\n",
      "Yo qué sin diar muchos\n",
      "y precuerden con alz!\n",
      "¡La vista al pobre que no querío,\n",
      "sin darme atardía\n",
      "porque a tomar y olvido,\n",
      "ir a tropindo la infierro\n",
      "que no taban lechó quería,\n",
      "\n",
      "porque si trar ser comendar\n",
      "vere a la última con frasca.\n",
      "¡La criolla en el indio gaviente,\n",
      "en criciao que es el tiral\n",
      "en el cardo del ogén\n",
      "en la pampes pa de salvado;\n",
      "al ver de mi ser\n",
      "cuando el desierio comisa\n",
      "la hizo no más tasa vida.\n",
      "\n",
      "Cuando los animales\n",
      "emplé el día, hasta la mana:\n",
      "la engro gamón allí\n",
      "saltando ni trapo al carde;\n",
      "y hasta que la ma)as para no tan rado\n",
      "toditos las dos informas.\n",
      "\n",
      "(el grito, es una orade\n",
      "le corazón de las omas;\n",
      "en adarrimento en su plata,\n",
      "sin refoloja de mi gusta,\n",
      "me recordé con mi padre\n",
      "pero y naides me sentá.\n",
      "\n",
      "En miserias máiro\n",
      "no tanto buen hacienda,\n",
      "ni le dijo y el corazón,\n",
      "le emperontro que en el restor,\n",
      "los declastidos hambre\n",
      "el porque me atraca a mi froya.\n",
      "\n",
      "Y al vez! mi mi carrón,\n",
      "y dende con la aguardión:\n",
      "era vi la manos ranchosas,\n",
      "que en un rodio de mano\n",
      "el como que el ramido.\n",
      "\n",
      "El gaucho sabido\n",
      "en en mil cuchillos:\n",
      "en encosa tiba, en en medio,\n",
      "no tenía cuando ni potro;\n",
      "y era esto no encomendo\n",
      "lo hace encuentrar a venién.\n",
      "\n",
      "Se caroz, verizos, como muro,\n",
      "en sombrar en Canto,\n",
      "con mis mesmo das diclas;\n",
      "el día vez que de esta pasa\n",
      "dentro conoce que estaba.\n",
      "\n",
      "Si no\n",
      "se ha de comonteciós\n",
      "y pidarle componete;\n",
      "si áa madre encuentras,\n",
      "debían comos, desesdición,\n",
      "enderao vi y un tropincho\n",
      "con la carba y bichocuta.\n",
      "\n",
      "Vamos, relosososos,\n",
      "y un voiejos langres;\n",
      "dende los han razones\n",
      "ada malena los sentones;\n",
      "al verven como andavianones\n",
      "como una miradficianza,\n",
      "y a otro les dieran ya.\n",
      "\n",
      "Y se cuando en el bienso\n",
      "como cuanto el momento,\n",
      "porque el mundo un horroro\n",
      "no hay sinoria que abardar;\n",
      "ni sie los ojos que ne atarda\n",
      "empeñao y perdo, y amparo.\n",
      "\n",
      "\"Yo tenío arrerodilar,\n",
      "hace hemos y reregusoso,\n",
      "de un pobre me daba haciendo,\n",
      "con perseguidiaba santimo.\n",
      "Sacá, al sin funo,\n",
      "esos meso se duro;\n",
      "a ver a con hasto escojo\n",
      "uno de amarrarido,\n",
      "y si le aprendiden de ese frobo\n",
      "\n",
      "también pisa él a porriado.\n",
      "Todo ese moro con faor\n",
      "y si corté ande ganadicia:\n",
      "si ha enser más mesmo van vil,\n",
      "\n",
      "a todo no cautiva el cielo\n",
      "la fiebre que con miedo.\n",
      "\n",
      "Y cristiano viste\n",
      "con tiranza fuera,\n",
      "todito al vez manorar;\n",
      "todo estaba calar\n",
      "a gristro luanquiera.\n",
      "\n",
      "¡Amos, si su meriaguando\n",
      "con tener denao,\n",
      "¡qué tanto dar la fansa\n",
      "con toro que el fin diarlo,\n",
      "porque yo no dejé amosar\n",
      "ni una cosa seaca.\n",
      "\n",
      "Y al fin se desea caja\n",
      "en aquella infiel día mio;\n",
      "¡us no ha de antinllo,\n",
      "no tenga que el puebre\n",
      "que a fin de la auspnra,\n",
      "recogía el desdicha,\n",
      "ni cuanto a tempo pa sana;\n",
      "tenga o pinuna de aflojina\n",
      "al ver echar ningún chiro.\n",
      "\n",
      "Gras,amos cantos botalíos,\n",
      "provo, porque puede ha mandao,\n",
      "marcho, y todo, era el día:\n",
      "en el laz es sobre agena\n",
      "in la hizo más de favor.\n",
      "\n",
      "Aunque jure por un frando\n",
      "hombre de lo que va y;\n",
      "pero calienza otro mi roda\n",
      "y atirándolé pa el juego.\n",
      "Nunca si me vir\n",
      "hasta que el claro la overda,\n",
      "hasta que es para mi trata:\n",
      "todo como cantor \n",
      "vusa siempre el cielo.\n",
      "\n",
      "Esa mi curaz muy señor\n",
      "de faltan no sé barriguro;\n",
      "y por no dejé sabar\n",
      "aunque lo blanco bicho cerro;\n",
      "aunque como esa encuentraba\n",
      "con la razón los perros.\n",
      "\n",
      "Y camiar supe frontoso,\n",
      "no le dan perdido si sagú;\n",
      "en señar o el viejo\n",
      "de la furigüela la mujer;\n",
      "y en más la págaro que al suerto\n",
      "muchas canto hacienda;\n",
      "era mí la inocenda\n",
      "cai causa del procura.\n",
      "\n",
      "Racuando nos, los dos\n",
      "acampasionados,\n",
      "y por el cortaner\n",
      "el hombre por qué la va,\n",
      "tirigún sisto maga,\n",
      "con el cuero la sano,\n",
      "y era muy Cadrón.\n",
      "\n",
      ", aunque cardao no ha dentra\n",
      "en ese entregao y canto:\n",
      "en cuanto es es opierron,\n",
      "todo se amor y es suplicio,\n",
      "y me hizo una de madre\n",
      "\"pero he de ver de animasaria\".\n",
      "\n",
      "¡Ah visto si voce\n",
      "mina............ ni perro quierra..\n",
      "no hay cempata\n",
      "a tonce por la bara.\n",
      "de llantan jera.\n",
      "\n",
      "Ferú sabe a agenar\n",
      "a haciendo el mono un siego,\n",
      "en anima cosa ande en uglo,\n",
      "y dentrás el señor\n",
      "a algún de un gringo al ran entro.\n",
      "\n",
      "Y al vez me darlo en mis hizo\n",
      "que en un debo barrigo tiende;\n",
      "lo hamos dariguila yo\n",
      "andaba dejan el toqe estaba;\n",
      "el día el prefesia no labior\n",
      "vino cometigimo.\n",
      "\n",
      "\n",
      "En mi rempo debe servir,\n",
      "yo siempre no mesmo sa\n",
      "y puedo y siempre hacer;\n",
      "para me declamar\n",
      "me coloriar la dormida,\n",
      "hastasta que remirario\n",
      "y aunque no este se anima.\n",
      "\n",
      "Me asisia no se enconterra\n",
      "en cuentra de mis días,\n",
      "y coRiados, nos, hambraos,\n",
      "y los pitoran llorar\n",
      "que primero a la pobre del.\n",
      "\n",
      "La hambre de entre la mama ortida\n",
      "proy, cái el día entre mi corar;\n",
      "ningún más mesmaco\n",
      "al preto ni aguitarraza.\n",
      "\n",
      "Si habían de lo que darmía\n",
      "cuando el precermero tanto,\n",
      "y que no es busca el infolmo,\n",
      "me dijo a entre un caristigo;\n",
      "se refrecido el cautivo\n",
      "a gristanar el mulito.\n",
      "\n",
      "No larga a over de infermo,\n",
      "en la santina della,\n",
      "regando a dola al cielo\n",
      "porque es se me encontende;\n",
      "como el que es, no se debe,\n",
      "porque naides se de animino,\n",
      "hacemos dos confiano,\n",
      "\n",
      "le dan aquellas muchos\n",
      "es esas preciones el menos.\n",
      "Marejamos si no,\n",
      "porque que el gaucho dudo,\n",
      "\n",
      "sinó no es enconojeda en lanza\n",
      "y, llegaban los diros agarraos\n",
      "empre a los dos presejas;\n",
      "no hay a plan del rianzollo\n",
      "los dos ajo el respolar.\n",
      "\n",
      "En llega mil horriblas\n",
      "de enemencar con granios;\n",
      "nos santicia madronias\n",
      "sin pronto que yo camuarda;\n",
      "ni siempre que mandaraon\n",
      "sabe quiieren lo sepprendencia.\n",
      "\n",
      "Y güeyos, pocos, cruzao,\n",
      "sociendo un catón,\n",
      "con la manias, poblas\n",
      "y no mesmanan a mía:\n",
      "por el cavillo gaucho.\n",
      "\n",
      "Me diré, si un depenimal\n",
      "era boliz de cargón\n",
      "en mi carriñPico,\n",
      "y no hay fajo supranzo\n",
      "y ha de salvado al bolazo\n",
      "a sufrido el cerpo de totó?\n",
      "\n",
      "Aunque ya no he de entrado\n",
      "a como rigor tan desdichas;\n",
      "si empre una gente sin tarda,\n",
      "y el indio del pesanza\n",
      "en mil invierno a el primero;\n",
      "\n",
      "y siempre el salvaje no encuentro\n",
      "ni pecho mis macrrimos.\n",
      "en moditos sus engraciaos,\n",
      "y siempre tanto a cuento,\n",
      "sólo en otro más al cuero\n",
      "y el indio a los animales.\n",
      "\n",
      "Sin darlo como el salvaje,\n",
      "y a ey la mala infierno:\n",
      "a tiriste que no iba lanza\n",
      "a es ciel de soposara\n",
      "y el que se aftra que me lardido.\n",
      "\n",
      "Dentro que aflijido como lno\n",
      "habré que mandó en esta el soldo\n",
      "y ni oy se tiro de un trago,\n",
      "dentrado el tiro labio suerte;\n",
      "y he va sino siembre\n",
      "no debe aquel mesmo rame.\n",
      "\n",
      "Y yo se diera parar eso\n",
      "a golpes de aflor\n",
      "como si ha ver de nombre\n",
      "tenía de calar;\n",
      "como que recebí a el piel\n",
      "lo dice al verse domar.\n",
      "\n",
      "Me refuregana al día\n",
      "y en la agarguada del ser,\n",
      "siempre que el mesmo perro\n",
      "se hace al decierto;\n",
      "hace quiero llega a bornte\n",
      "dieron en otros encuentro?\n",
      "\n",
      "Yo se me hace riegido\n",
      "áhi no más pedido;\n",
      "esos moro mi casiano,\n",
      "porque, si uno se se remedio,\n",
      "como el cantor\n",
      "y allí nace al mamar.\n",
      "\n",
      "Esosa cosa que rendina;\n",
      "no que llegra al piegarse,\n",
      "no hay cosa no gala en mano,\n",
      "sino siquieren los quebres,\n",
      "y al que tabajuntos viejos!\n",
      "\n",
      "\n",
      "La ruzamos de nos altaron,\n",
      "aunque es amori la márromala,\n",
      "y no sé qué el brato en el suelo\n",
      "nos ha de nombre aquel nevo\n",
      "en de barazo a su en ni suerte\n",
      "y nunca se más el agarido.\n",
      "\n",
      "A si amigo el primero\n",
      "veramos al jefe en señor;\n",
      "pues a su rumbo yalcio,\n",
      "con aquíl mesmitarro el rigor\n",
      "he dejan de enferme el regüél.\n",
      "\n",
      "y veré cuántos de más\n",
      "canto al desierto mi va.\n",
      "\n",
      "Todo que como la vida\n",
      "cimo pavecimo sin perro;\n",
      "ha visto en al chiquito\n",
      "a tarrera en segretiano;\n",
      "sinó un maqina que ramino,\n",
      "menta o la parara al frontero.\n",
      "\n",
      "Y vámos a un gringo tiema\n",
      "y aunque descuida desel mor\n",
      "con el desierto me tanto,\n",
      "y le dijo, cuando el bolazo,\n",
      "los manos como un Far.\n",
      "\n",
      "Vierecion no motro\n",
      "ninguno más señor;\n",
      "ni un busco que yo en amanija;\n",
      "el carizo diera no perdida,\n",
      "ha visido en la emperanza\n",
      "ni la benemitera de mi va.\n",
      "\n",
      "Y en el juezgo no dentra\n",
      "el deser de tal ragor;\n",
      "es preso que no sabe ni lerda;\n",
      "ni va a infelicio ni riancia\n",
      "el más se animal\n",
      "ibán el de tenerido.\n",
      "\n",
      "Ramos como a razón,\n",
      "ni tenía desnufido en ello,\n",
      "¡que manda siempre me perso\n",
      "y sé a que trabajo remona!\n",
      "Callo ya remedumor\n",
      "y más fieras de que esa hizo!\n",
      "\n",
      "ni sufrin tenvo encuentro\n",
      "que a regadar y tan bromar\n",
      "de amor que de la escupa\n",
      "quien a que a del desemor.\n",
      "\n",
      "Las puntos dichas, sangres\n",
      "cansamosonievones;\n",
      "en todas el carazón,\n",
      "los aguardanzann más daño,\n",
      "dende ha uno mátral\n",
      "que sin dentrar la chisana\n",
      "la madre la indio y vada.\n",
      "\n",
      "Pida el jefeo de cielo\n",
      "la miradá del horme\n",
      "se vino al corazón,\n",
      "y era misilescas del jor,\n",
      "ni facia y que se iba\n",
      "orreglaban mi carreto.\"\n",
      "\n",
      "Y iba con la orazón\n",
      "de ha de de mis cuidas,\n",
      "que ante creyendo en el fierro:\n",
      "cuando como el hombre serigo,\n",
      "dende ser la manos\n",
      "y que a su rumba a solva,\n",
      "es un por la mestra\n",
      "si cumplir más los agordidos.\n",
      "\n",
      "Y la miserimas ha dichas\n",
      "hasta en la razón del erro;\n",
      "ha e seguido en servierto,\n",
      "y tanto dos un corco\n",
      "aver a dar güelos,\n",
      "para ver o que allí.\n",
      "\n",
      "Me anda la \n"
     ]
    }
   ],
   "source": [
    "context = \"Los hermanos sean unidos\"\n",
    "context = torch.tensor(encode(context), dtype=torch.long).unsqueeze(0).to(device)\n",
    "generatedText = decode(model.generate(context, nTokens=10000)[0].tolist())\n",
    "print(generatedText)\n",
    "outputDir = \"outputs\"\n",
    "with open(os.path.join(outputDir, \"martin_fierro.txt\"), \"w\") as f:\n",
    "    f.write(generatedText)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
