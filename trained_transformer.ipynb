{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transformer Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoder-Decoder Architecture**\n",
    "\n",
    "It is used for **sequence-to-sequence** tasks like machine translation.\n",
    "\n",
    "This is the architecture proposed by the original transformer paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. in 2017.\n",
    "\n",
    "<img src=\"assets/encoder_decoder.png\" alt=\"Encoder-Decoder Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decoder-Only Architecture**\n",
    "\n",
    "It is used for **sequence generation** tasks like language modeling.\n",
    "\n",
    "This simpler architecture will be implemented in this notebook from scratch.\n",
    "\n",
    "<img src=\"assets/decoder_only.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scaled Dot-Product Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $Q \\in \\mathbb{R}^{m \\times d_k}$, $K \\in \\mathbb{R}^{n \\times d_k}$ y $V \\in \\mathbb{R}^{n \\times d_v}$ be the query, key and value matrices, respectively. The scaled dot-product attention is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\in \\mathbb{R}^{m \\times d_v}\n",
    "$$\n",
    "\n",
    "<img src=\"assets/self_attention.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddingSize: int, headSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.query = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.value = nn.Linear(embeddingSize, headSize, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(blockSize, blockSize)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        weights = q @ k.transpose(-2, -1) / (C ** 0.5)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        output = weights @ v\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $h$ be the number of heads. The multi-head attention is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\in \\mathbb{R}^{m \\times d_v}\n",
    "$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$ and $W_i^Q \\in \\mathbb{R}^{d_k \\times d_k}$, $W_i^K \\in \\mathbb{R}^{d_k \\times d_k}$, $W_i^V \\in \\mathbb{R}^{d_v \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_v}$ are the learnable parameters.\n",
    "\n",
    "<img src=\"assets/multi_head_attention.png\" alt=\"Decoder-Only Architecture\" style=\"background-color:white;\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, nHeads: int, embeddingSize: int, headSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([ScaledDotProductAttention(embeddingSize, headSize, blockSize, dropout) for _ in range(nHeads)])\n",
    "        self.projection = nn.Linear(embeddingSize, embeddingSize)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        output = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        output = self.projection(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Position-wise Feed-Forward Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position-wise feed-forward networks are defined as:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "where $W_1 \\in \\mathbb{R}^{d_{ff} \\times d_{model}}$, $b_1 \\in \\mathbb{R}^{d_{ff}}$, $W_2 \\in \\mathbb{R}^{d_{model} \\times d_{ff}}$ and $b_2 \\in \\mathbb{R}^{d_{model}}$ are the learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddingSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embeddingSize, 4 * embeddingSize),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embeddingSize, embeddingSize),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Layer Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layer normalization is defined as:\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sigma} + \\beta\n",
    "$$\n",
    "\n",
    "where $\\gamma \\in \\mathbb{R}^{d_{model}}$ and $\\beta \\in \\mathbb{R}^{d_{model}}$ are the learnable parameters, and $\\mu$ and $\\sigma$ are the mean and standard deviation of $x$, respectively.\n",
    "\n",
    "In the original paper, the layer normalization is applied after the residual connections, whereas in more modern implementations, it is applied before in what is known as **pre-norm formulation**.\n",
    "\n",
    "```python\n",
    "# Original paper\n",
    "x = LayerNorm(x + MultiHeadAttention(x))\n",
    "x = LayerNorm(x + FeedForward(x))\n",
    "\n",
    "# Pre-norm formulation\n",
    "x = x + MultiHeadAttention(LayerNorm(x))\n",
    "x = x + FeedForward(LayerNorm(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "\n",
    "    def __init__(self, nHeads: int, embeddingSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        headSize = embeddingSize // nHeads\n",
    "        self.multiHeadAttention = MultiHeadAttention(nHeads, embeddingSize, headSize, blockSize, dropout)\n",
    "        self.feedForward = FeedForward(embeddingSize, dropout)\n",
    "        self.layerNorm1 = nn.LayerNorm(embeddingSize)\n",
    "        self.layerNorm2 = nn.LayerNorm(embeddingSize)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.multiHeadAttention(self.layerNorm1(x))\n",
    "        x = x + self.feedForward(self.layerNorm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__ (self, vocabularySize: int, nLayers: int, nHeads: int, embeddingSize: int, blockSize: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenEmbeddingTable = nn.Embedding(vocabularySize, embeddingSize)\n",
    "        self.positionEmbeddingTable = nn.Embedding(blockSize, embeddingSize)\n",
    "        self.layers = nn.Sequential(*[Layer(nHeads, embeddingSize, blockSize, dropout) for _ in range(nLayers)])\n",
    "        self.layerNorm = nn.LayerNorm(embeddingSize)\n",
    "        self.linearModelHead = nn.Linear(embeddingSize, vocabularySize)\n",
    "        self.blockSize = blockSize\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor, targets: torch.Tensor = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        B, T = idx.shape\n",
    "        tokenEmbeddings = self.tokenEmbeddingTable(idx)\n",
    "        positionEmbeddings = self.positionEmbeddingTable(torch.arange(T, device=device))\n",
    "        x = tokenEmbeddings + positionEmbeddings\n",
    "        x = self.layers(x)\n",
    "        x = self.layerNorm(x)\n",
    "        logits = self.linearModelHead(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.Tensor, nTokens: int) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(nTokens):\n",
    "                logits, loss = self(idx[:, -self.blockSize:])\n",
    "                logits = logits[:, -1, :]\n",
    "                probabilities = F.softmax(logits, dim=-1)\n",
    "                nextIdx = torch.multinomial(probabilities, num_samples=1)\n",
    "                idx = torch.cat([idx, nextIdx], dim=1)\n",
    "        self.train()\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text Generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDir = \"data\"\n",
    "file = os.path.join(dataDir, \"shakespeare.txt\")\n",
    "with open(file, \"r\") as f:\n",
    "    text = f.read()\n",
    "print(f\"Text length: {len(text)}\")\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sorted(set(text))\n",
    "vocabularySize = len(vocabulary)\n",
    "print(f\"Vocabulary size: {vocabularySize}\")\n",
    "print(\"\".join(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 53, 1, 40, 43, 1, 53, 56, 1, 52, 53, 58, 1, 58, 53, 1, 40, 43]\n",
      "To be or not to be\n"
     ]
    }
   ],
   "source": [
    "stringToToken = {ch: i for i, ch in enumerate(vocabulary)}\n",
    "tokenToString = {i: ch for i, ch in enumerate(vocabulary)}\n",
    "encode = lambda string: [stringToToken[char] for char in string]\n",
    "decode = lambda tokens: \"\".join([tokenToString[token] for token in tokens])\n",
    "print(encode(\"To be or not to be\"))\n",
    "print(decode(encode(\"To be or not to be\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.79M parameters\n"
     ]
    }
   ],
   "source": [
    "blockSize = 256\n",
    "embeddingSize = 384\n",
    "nHeads = 6\n",
    "nLayers = 6\n",
    "dropout = 0.2\n",
    "\n",
    "model = GPTLanguageModel(vocabularySize, nLayers, nHeads, embeddingSize, blockSize, dropout).to(device)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsDir = \"models\"\n",
    "model.load_state_dict(torch.load(os.path.join(modelsDir, \"shakespeare.pt\"), weights_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to bed,\n",
      "His affairs plucks and men it and heart:\n",
      "Therefore, my lords, King Edward we speak,\n",
      "Speak, must I near, be not die:\n",
      "The contriuments that I saw thy request;\n",
      "For that, not shall hear their wull hich shows them,\n",
      "And she may show, methinks me here in her;\n",
      "And with their senest upon my heart came\n",
      "From God tears me with disholopeful held.\n",
      "As so the lungly correts and wilt the resemble\n",
      "And would not what may do such a lost.\n",
      "There's Tybalt Angelo, but was traitor!\n",
      "I must ake the lands, and thinking these rogues,\n",
      "Ny Ebeseemino; mark, I cannot have done:\n",
      "I, this say; long on tarry, for thou art,\n",
      "Thou hast she pure beholder'd; kill me was butt,\n",
      "So set let me up, and I burn for this.\n",
      "\n",
      "JULIET:\n",
      "Marry, look you, my lord. I'll had you tell you of Weep\n",
      "What news?' what flesh is true than o'er company?\n",
      "\n",
      "ROMEO:\n",
      "Dear belike? friar, ere think you not: not show you\n",
      "Say have made. Good mars, what a tympassion?\n",
      "Pack, go the man for a lamb. For a lame, and fife;\n",
      "not to look me o' the woe, leathers in mine,\n",
      "How grow is not cure to Forth do kill me speed\n",
      "In purchases, and earth him beful suppose:\n",
      "I must be after to Kouchmond in:\n",
      "And I, though she, as he that bad, he, she bear'd to my head,\n",
      "As thou hast served, as better as die.\n",
      "\n",
      "PRINCE:\n",
      "Go hence; ha! what ha? can madne upon Him then?\n",
      "For stay have lieve: here, what will not amends.\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "PRINCE EDWARD:\n",
      "Say you me?\n",
      "\n",
      "KING HENRY VI\n",
      "\n",
      "KING RICHARD III:\n",
      "Be your grace! my crown friends, go you all;\n",
      "You must indeed. Be no longer Tower,\n",
      "Hear me, but we comes a fair life,\n",
      "Your honour of exedies she do doubt.\n",
      "\n",
      "MOPSA:\n",
      "Yes, know your lords, he protector have been\n",
      "That yielded with you hear the dread.\n",
      "Therefore you shall meet of night: upon your suitors\n",
      "Dare you most of the country's pardon bones?\n",
      "Now now Richard, poor Capulius, and colours you but call\n",
      "This pity foul Earl queen, hich to Burgulet,\n",
      "Dost thou aundone wrong such affaithful these voices,\n",
      "Thou have before her mine prodigy nurse.\n",
      "\n",
      "KING RICHARD II:\n",
      "I know nothing here on Raposle, when he bites,\n",
      "In true of us e'er special of his:\n",
      "Your apply is show'd with his sworn.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Look thee shall give you in the enemy;\n",
      "For I could groce your highness came to hear;\n",
      "Then have ever done my even are to hide.\n",
      "\n",
      "KING RICHARD II:\n",
      "Why lie, when the people that I seem myself.\n",
      "\n",
      "KING HENRY VI\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "What! but say, call'd I speak out so:\n",
      "I hear me, bid me, I have heard wrong too,\n",
      "Be rather should talk short to him good.\n",
      "\n",
      "BUCKINGHAM:\n",
      "A cousin deir cousin, tullus be luck'd,\n",
      "And lough me for thou repeal and bound tears\n",
      "To be endure that would can be frankly to him.\n",
      "\n",
      "RIVERS:\n",
      "'Twas not grave this sceptrary to my love.\n",
      "\n",
      "KING RICHARD III:\n",
      "Know but what I might are too cross'd to me?\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Mark, I thou have such a moan of pride,\n",
      "My flesh, so of Henry's proclaims,\n",
      "And wrather your seals! I have, your pretty,\n",
      "To that your passage hath return'd your moary,\n",
      "Nothing to the fearful plots of stoce\n",
      "That have joyful no, and your curgate day\n",
      "You, sir, and love me: high Duke of York, man,\n",
      "It is such a uborn-trow. With, to this way,\n",
      "That thou unrive, may must forgive: 'tis all.\n",
      "In my fetty mouse is afterwith fiend,\n",
      "Whore a man. As Peter he should be sawn?\n",
      "\n",
      "LUCIO:\n",
      "What! my father? how must he love until this?\n",
      "\n",
      "Provost:\n",
      "For I were your contractions to live him to him.\n",
      "\n",
      "PARIS:\n",
      "Let him, and let's go's ill.\n",
      "I pray you, nor peruse.\n",
      "\n",
      "JULIET:\n",
      "Embach him, not I be gone.\n",
      "Though Gauntio! is your love, when my lords?\n",
      "\n",
      "CAPULET:\n",
      "So lie: then he has a sais. What, ho! let me thinks!\n",
      "\n",
      "Nurse:\n",
      "No, no, sir, no more; but if is true peace,\n",
      "Let my heart\n",
      "A goodly's slymper: what is't again; and, but though\n",
      "The letter is it, and my kinsman,\n",
      "Morely was an impieral with a man;\n",
      "So doubt that good upon my country's pale\n",
      "With, if thou be depart thee, good country's stay\n",
      "With bloody-ward: our eyes bud's glassy,\n",
      "Most miniged thought he doth in justice the bub;\n",
      "For I playfe it.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "O, keep me I love, born, but I by whalf I\n",
      "\n",
      "JULIET:\n",
      "What, my lord, thou didst not change our marts,\n",
      "With suck mine a yield.\n",
      "Draw children, let us lose the house\n",
      "Is resent put a secur of thing way of thee.\n",
      "\n",
      "LADY CAPULET:\n",
      "I fear me, my lord.\n",
      "\n",
      "CLARENCE:\n",
      "O, my lord, I know forship thee was.\n",
      "\n",
      "JULIET:\n",
      "Shall I be raghter, old Clarence, for thou lovest,\n",
      "I do come no lamentabour to married?\n",
      "\n",
      "JULIET:\n",
      "How canst thou knew'st, thy day and by them?\n",
      "Our rich in my scountly ripen's loss!\n",
      "\n",
      "BRAKNBURY:\n",
      "Ay, I'll account me, fear me with me well,\n",
      "How say your venom; which I do bid.\n",
      "\n",
      "LORD WILLOUGHBY:\n",
      "No, no, my good master, that you are come to his,\n",
      "Or else you are it, sure of the atterator,\n",
      "Heaving your chronour to choose shall not be full of a thousand.\n",
      "\n",
      "KING RICHARD II:\n",
      "I'll tell you, madam, and he would have been so faults:\n",
      "His heafth, do bud from me patient of you.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "The sun of the last summer's oath. I do resolve ill.\n",
      "Great Signior and Saint Surrey, if he do do retem,\n",
      "As this preIntly, All-Sons doth you, and I so find\n",
      "The head to Benry, which became you him speak;\n",
      "And he is intempt of Warwick countrymen:\n",
      "Was not in Suffolk hath have cause you behnce\n",
      "Of mine in my life-beats death; or the laness I\n",
      "Am the duke, that my life appeal in your alve;\n",
      "RIgharn Or you came. If I never let him no meet\n",
      "Mine effect the king night's reason, who, I return body,\n",
      "That yield a grant-parle, I will find his faults,\n",
      "And feast unjust avoid\n",
      "And now no boar thus. Not his head;\n",
      "His country on suffer us his age\n",
      "Slu winter'd and make the grief its to fall;\n",
      "This imposition, my might when he could live,\n",
      "And yields I not read our woe. What you sir,\n",
      "Leal that I am, that the ragge of your cheeks?\n",
      "\n",
      "PRINCE EDWARD:\n",
      "Please you take of Antigonus fortne of God!\n",
      "\n",
      "SOMERSET:\n",
      "I was a word heart the gates.\n",
      "\n",
      "HORTENSIO:\n",
      "I pray in me.\n",
      "\n",
      "HORTENSIO:\n",
      "Place, marry.\n",
      "\n",
      "QUEEN:\n",
      "With married to thy suppose, muth call act.\n",
      "But fear bemy of the county's-giving-shame,\n",
      "Their ancient run were I forgot be Apollo's;\n",
      "Much in the your commings of much discreets\n",
      "Which his power in prompty. Now, I have brought your brother\n",
      "Upon your person.\n",
      "\n",
      "SICINIUS:\n",
      "This arms:\n",
      "Lyou know but one senators; and though I\n",
      "have pilted with her than break.\n",
      "Thinkest me the heavens seeming in a breath,\n",
      "and in safe3 Kather, Clarence?--\n",
      "\n",
      "CAMILLO:\n",
      "My valiant Duke? will you not beat you?\n",
      "\n",
      "SICINIUS:\n",
      "He shall know the give accuse for it our into\n",
      "much the capes: but which being leave of him\n",
      "which By you this Capulet: or your righth, might this apple,\n",
      "and prince well. Shall I other themselve not  speak.\n",
      "\n",
      "VIRGILIA:\n",
      "O, I salve hers you tarry abonded. And go to him;\n",
      "pause, since me you, and you mean to have; when mannely\n",
      "is a noble services, more, and your affevours\n",
      "of my city, than you power it again, to regreat them in\n",
      "counterprove it, we trouble.\n",
      "\n",
      "MENENIUS:\n",
      "You have officed him,\n",
      "We before you the partve, he will received the eaven\n",
      "Notagle that a gifter cur\n",
      "A threapulate-oath oit. Edward which he does,\n",
      "Whom is his anchor, and so poor bones in Tite,\n",
      "Than Elymas, and after their slaughters:\n",
      "Methinks came your holy most heir,\n",
      "I did speak, peach much bring lowers at lions\n",
      "Shall lay in those greety lands our nows she glory.\n",
      "Earl must uponder the villain of the claudm;\n",
      "And red who hear our perial new-man's two,\n",
      "Or in this fanhood is a hid on't.\n",
      "\n",
      "JULIET:\n",
      "Why is Merculio? thou art for our sen?\n",
      "\n",
      "Nurse:\n",
      "She's a kind a nettle-given prospere to-night,\n",
      "Whose is a kind of reval day with love;\n",
      "Saveping before him.\n",
      "\n",
      "JULIET:\n",
      "Live a tape of his death, sprokes of his bloodied,\n",
      "Do not unless less his born toward me.\n",
      "I'll warrant of her she liar.\n",
      "\n",
      "NORFOLK:\n",
      "I do so: it is not you o'er of the people,\n",
      "Young men that I have, I have rue no other\n",
      "Tremble stand; he hath he have sold on say.\n",
      "\n",
      "KOthle, I heard my true love and my heart's heavy:\n",
      "O, drawn, lords, proof it that should be his\n",
      "Affrown'd to fixs me, and my mourner's heart,\n",
      "A pleasure, provoke, if his night shall redeems to-night\n",
      "In blood lightly; I cannot shope again.\n",
      "O, please you no country's foot, never brother,\n",
      "Than stout, a preplais many deal spight.\n",
      "Displeasing faith, let him in your tyrannous,\n",
      "With all require false, for an entreation:\n",
      "To Romeo his heart, that you shall look upon him;\n",
      "But like she comes at England's hand;\n",
      "Confess so would please your pillot our welcomes,\n",
      "Which say murders have shuns you for me than I\n",
      "But as unhis like country's death. Where methinks,\n",
      "Afther as this hour smoothsom to cure it:\n",
      "Come, cull hath a cold in soft? My lord,\n",
      "Have say home to him all the duke of my soul?\n",
      "\n",
      "ABHORSON:\n",
      "If our own countrys will ne'er keep the boil.\n",
      "\n",
      "POMPSA:\n",
      "He was thee, I know no more and your place:\n",
      "And mine is victory first a galland o' the way,\n",
      "'Tis song mine eye; I am think you, being my soul,\n",
      "And there to your remost as I tend myself.\n",
      "Well, I hoppe it a dream of my phyches;\n",
      "Despition you there to make thee a chair foot.\n",
      "\n",
      "WARWICK:\n",
      "I tell him your son woundled them have\n",
      "A such a loff truial will I should ride.--\n",
      "Though any time, I bear--gone. Hie will my tongue;\n",
      "Say, with our gracious chaity blood,\n",
      "Thou darest to be puull'd unknown in Behmia;\n",
      "Thou wouldst have no more she day it last to fly,\n",
      "And his own blood will each it.\n",
      "\n",
      "MENENIUS:\n",
      "Not I!\n",
      "\n",
      "CORIOLANUS:\n",
      "Thy father was thought their tears:\n",
      "And the runk'st persuade upon thy good and\n",
      "Bleed in ears, if they have greet powers spoil'd\n",
      "The bench'd, not not stand Romeo last up,\n",
      "Lest that lie should shall make out off.\n",
      "\n",
      "MENENIUS:\n",
      "Come.\n",
      "\n",
      "VOLUMNIA:\n",
      "Now I have popunded,\n",
      "I, as I have lived by you, since heard me o'\n",
      "His nurse.\n",
      "\n",
      "SICINIUS:\n",
      "Menes he is a whore,\n",
      "A sir, bugge you hith to each the duke\n",
      "To home. Not but counterpest him, which pleaseth\n",
      "To-morrow dares upon me.\n",
      "\n",
      "VOLUMNIA:\n",
      "It give you did it.\n",
      "\n",
      "CORIOLANUS:\n",
      "On her, I comme to ourpose\n",
      "To hope no buze: but I charge him? What say'st accontion?\n",
      "My ears was his mind, and there's should speak:\n",
      "The cares of a longer in the war.\n",
      "\n",
      "COMINIUS:\n",
      "Officer,\n",
      "I pray the country.\n",
      "\n",
      "VOLUMNIA:\n",
      "You are your worship:\n",
      "And beguile me his judge's as daughter,\n",
      "And make me in his loyal daughters' mouth,\n",
      "But else ourward.\n",
      "You should come him now.\n",
      "\n",
      "MENENIUS:\n",
      "Why, I should hear him\n"
     ]
    }
   ],
   "source": [
    "context = \"To be or not to be\"\n",
    "context = torch.tensor(encode(context), dtype=torch.long).unsqueeze(0).to(device)\n",
    "generatedText = decode(model.generate(context, nTokens=10000)[0].tolist())\n",
    "print(generatedText)\n",
    "outputDir = \"outputs\"\n",
    "with open(os.path.join(outputDir, \"shakespeare.txt\"), \"w\") as f:\n",
    "    f.write(generatedText)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
